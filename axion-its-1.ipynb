{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13572679,"sourceType":"datasetVersion","datasetId":8622214}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, dan Library (LightGBM only)\n# =========================================================\nimport os, gc, math, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# Install LightGBM (jika belum tersedia)\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"lightgbm\"])\n    import lightgbm as lgb\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\n# ====== Paths (prioritas Kaggle sesuai permintaan) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback (opsional, jika ingin jalankan di lokal):\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# Blok 1 â€” Load Data, Sort Timestamp, dan Cek Kolom\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\n# Parse timestamp dan urutkan\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\nFEATURES_RAW = [\n    \"Average Water Speed\",\n    \"Average Water Direction\",\n    \"Chlorophyll\",\n    \"Temperature\",\n    \"Dissolved Oxygen\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Salinity\",\n    \"Specific Conductance\",\n]\n\n# Cek kolom wajib\nmissing_train = [c for c in FEATURES_RAW+[TARGET,\"Timestamp\",IDCOL] if c not in train.columns]\nmissing_test  = [c for c in FEATURES_RAW+[\"Timestamp\",IDCOL] if c not in test.columns]\nif missing_train: print(\"Peringatan, kolom hilang di train:\", missing_train)\nif missing_test:  print(\"Peringatan, kolom hilang di test :\", missing_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:13:45.809922Z","iopub.execute_input":"2025-11-01T01:13:45.810240Z","iopub.status.idle":"2025-11-01T01:13:46.099104Z","shell.execute_reply.started":"2025-11-01T01:13:45.810217Z","shell.execute_reply":"2025-11-01T01:13:46.098463Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =========================================================\n# Blok 2 â€” Fitur Waktu & Arah + Interaksi Ringan\n# =========================================================\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt) == 0:\n        return 1\n    median_sec = np.median(dt)\n    if not np.isfinite(median_sec) or median_sec <= 0:\n        return 1\n    return int(max(1, round(3600.0 / median_sec)))\n\ndef add_time_harmonics(df, ts_col=\"Timestamp\"):\n    # siklus harian, mingguan, dan pasang 12.42 jam\n    out = df.copy()\n    s = out[ts_col].astype(\"int64\") // 10**9\n    day_sec = 24*3600\n    week_sec = 7*24*3600\n    tide_sec = int(round(12.42*3600))\n    out[\"sin_day\"]  = np.sin(2*np.pi * (s % day_sec) / day_sec)\n    out[\"cos_day\"]  = np.cos(2*np.pi * (s % day_sec) / day_sec)\n    out[\"sin_week\"] = np.sin(2*np.pi * (s % week_sec) / week_sec)\n    out[\"cos_week\"] = np.cos(2*np.pi * (s % week_sec) / week_sec)\n    out[\"sin_tide\"] = np.sin(2*np.pi * (s % tide_sec) / tide_sec)\n    out[\"cos_tide\"] = np.cos(2*np.pi * (s % tide_sec) / tide_sec)\n    return out\n\ndef dir_to_sincos(df, dir_col=\"Average Water Direction\", spd_col=\"Average Water Speed\"):\n    out = df.copy()\n    rad = np.deg2rad(out[dir_col])\n    out[\"dir_sin\"] = np.sin(rad)\n    out[\"dir_cos\"] = np.cos(rad)\n    # proyeksi arus\n    out[\"u_comp\"] = out[spd_col] * out[\"dir_cos\"]\n    out[\"v_comp\"] = out[spd_col] * out[\"dir_sin\"]\n    return out\n\n# Gabung train+test untuk konsistensi fitur non-target\ntrain[\"__is_train__\"] = 1\ntest[\"__is_train__\"]  = 0\nfull = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\n# Sanitasi ringan\nfull[\"pH\"] = full[\"pH\"].clip(lower=0, upper=14)\n\n# Tambahkan fitur waktu & arah\nfull = add_time_harmonics(full, \"Timestamp\")\nfull = dir_to_sincos(full, \"Average Water Direction\", \"Average Water Speed\")\n\n# Interaksi domain ringan\nfull[\"temp_do_sat\"] = full[\"Temperature\"] * full[\"Dissolved Oxygen (%Saturation)\"]\nfull[\"sal_conduct_ratio\"] = full[\"Salinity\"] / full[\"Specific Conductance\"].replace(0, np.nan)\nfull[\"sal_conduct_ratio\"] = full[\"sal_conduct_ratio\"].replace([np.inf, -np.inf], np.nan)\n\n# Estimasi resolusi data â†’ langkah per jam (untuk menentukan lag/rolling)\nsteps_per_hour = estimate_steps_per_hour(full)\nsteps_1h  = max(1, steps_per_hour)\nsteps_3h  = max(1, 3*steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\n\nEXTRA_FEATS_BASE = [\n    \"sin_day\",\"cos_day\",\"sin_week\",\"cos_week\",\"sin_tide\",\"cos_tide\",\n    \"dir_sin\",\"dir_cos\",\"u_comp\",\"v_comp\",\"temp_do_sat\",\"sal_conduct_ratio\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:13:49.122073Z","iopub.execute_input":"2025-11-01T01:13:49.122365Z","iopub.status.idle":"2025-11-01T01:13:49.225432Z","shell.execute_reply.started":"2025-11-01T01:13:49.122327Z","shell.execute_reply":"2025-11-01T01:13:49.224776Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# =========================================================\n# Blok 3 â€” Imputasi Time-Aware + Lag & Rolling Tanpa Bocor\n# =========================================================\ndef timewise_impute(df, cols):\n    out = df.copy()\n    out[cols] = out[cols].ffill().bfill()\n    for c in cols:\n        # rolling median ke belakang (no leakage)\n        out[c] = out[c].fillna(out[c].rolling(window=steps_6h, min_periods=1).median())\n        out[c] = out[c].fillna(out[c].median())\n    return out\n\ndef add_lag_roll(df, cols, lags, rolls):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n        for w in rolls:\n            # gunakan shift(1) agar hanya masa lalu\n            out[f\"{c}_roll{w}_mean\"] = out[c].shift(1).rolling(window=w, min_periods=1).mean()\n            out[f\"{c}_roll{w}_std\"]  = out[c].shift(1).rolling(window=w, min_periods=1).std()\n            out[f\"{c}_roll{w}_min\"]  = out[c].shift(1).rolling(window=w, min_periods=1).min()\n            out[f\"{c}_roll{w}_max\"]  = out[c].shift(1).rolling(window=w, min_periods=1).max()\n    return out\n\n# Kolom yang diimputasi/diturunkan lag&rolling\nsensor_cols = FEATURES_RAW + [\"u_comp\",\"v_comp\",\"temp_do_sat\",\"sal_conduct_ratio\",\"dir_sin\",\"dir_cos\"]\nfull = timewise_impute(full, sensor_cols)\n\n# Tambahkan lag & rolling\nLAGS  = [1, steps_1h, steps_3h]                  # 1-step, ~1 jam, ~3 jam\nROLLS = [steps_1h, steps_6h, steps_24h]          # ~1h, ~6h, ~24h\nfull = add_lag_roll(full, sensor_cols, LAGS, ROLLS)\n\n# Daftar fitur final\nlagroll_feats = [c for c in full.columns if any(s in c for s in [\"_lag\", \"_roll\"])]\nFEATURES = FEATURES_RAW + EXTRA_FEATS_BASE + lagroll_feats\n\n# Split kembali ke train/test (target tetap dari frame train awal)\ntrain_fe = full[full[\"__is_train__\"]==1].copy()\ntest_fe  = full[full[\"__is_train__\"]==0].copy()\ntrain_fe[TARGET] = train[TARGET].values  # align target ke urutan baru\n\n# Drop baris train yang masih NaN setelah lag/rolling\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\n# Isi sisa NaN di test dengan median train (aman untuk inference)\ntest_X = test_fe[FEATURES].copy()\ntrain_X = train_fe[FEATURES].copy()\ntest_X = test_X.fillna(train_X.median())\ny = train_fe[TARGET].values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:13:52.685556Z","iopub.execute_input":"2025-11-01T01:13:52.685905Z","iopub.status.idle":"2025-11-01T01:13:54.608365Z","shell.execute_reply.started":"2025-11-01T01:13:52.685878Z","shell.execute_reply":"2025-11-01T01:13:54.607457Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =========================================================\n# Blok 4 â€” TimeSeries CV (LightGBM, Early Stopping, OOF)\n# =========================================================\nn_splits = 5\ngap = max(1, steps_1h)  # buffer ~1 jam\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\n# Parameter LGBM\nlgb_params = dict(\n    objective=\"l2\",               # MSE\n    learning_rate=0.05,\n    num_leaves=64,\n    feature_fraction=0.9,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    min_data_in_leaf=50,\n    n_estimators=5000,            # besar, akan dipangkas early_stopping\n    random_state=SEED,\n    verbose=-1\n)\n\noof_pred = np.zeros(len(train_fe))\nbest_iters = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)]\n    )\n\n    pred_va = model.predict(X_va, num_iteration=model.best_iteration_)\n    oof_pred[va_idx] = np.clip(pred_va, 0, None)  # NTU tidak negatif\n\n    fold_mse = mean_squared_error(y[va_idx], oof_pred[va_idx])\n    best_iters.append(model.best_iteration_)\n    print(f\"Fold {fold} â€” best_iter={model.best_iteration_}, MSE={fold_mse:.6f}\")\n\ncv_mse = mean_squared_error(y, oof_pred)\nbest_iter_final = int(np.median(best_iters)) if len(best_iters)>0 else lgb_params[\"n_estimators\"]\nprint(f\"\\nCV MSE (OOF) = {cv_mse:.6f} | median best_iter = {best_iter_final}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:13:58.130236Z","iopub.execute_input":"2025-11-01T01:13:58.130544Z","iopub.status.idle":"2025-11-01T01:16:36.246196Z","shell.execute_reply.started":"2025-11-01T01:13:58.130523Z","shell.execute_reply":"2025-11-01T01:16:36.245367Z"}},"outputs":[{"name":"stdout","text":"Fold 1 â€” best_iter=3153, MSE=4.507777\nFold 2 â€” best_iter=347, MSE=39.994899\nFold 3 â€” best_iter=2, MSE=13.195891\nFold 4 â€” best_iter=6, MSE=8.059490\nFold 5 â€” best_iter=204, MSE=45.425432\n\nCV MSE (OOF) = 19.392469 | median best_iter = 204\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================================================\n# Blok 5 â€” Train Full Model dengan best_iter & Prediksi Test\n# =========================================================\n# Latih ulang full model memakai jumlah tree = median best_iter dari CV\nlgb_full = lgb.LGBMRegressor(**{**lgb_params, \"n_estimators\": best_iter_final})\nlgb_full.fit(train_X, y)\n\npred_test = lgb_full.predict(test_X)\npred_test = np.clip(pred_test, 0, None)  # jaga non-negatif\n\n# Buat submission.csv (urut sesuai sample_submission)\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": pred_test\n})\n\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:16:36.247323Z","iopub.execute_input":"2025-11-01T01:16:36.247551Z","iopub.status.idle":"2025-11-01T01:16:47.381532Z","shell.execute_reply.started":"2025-11-01T01:16:36.247534Z","shell.execute_reply":"2025-11-01T01:16:47.380871Z"}},"outputs":[{"name":"stdout","text":"âœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916   3.230723\n1          54917   3.343420\n2          54918   3.766943\n3          54919   4.195236\n4          54920   4.720352\n5          54921   4.866751\n6          54922   4.671263\n7          54923   5.292683\n8          54924   5.623497\n9          54925   6.091133\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 6 â€” (Opsional) Penting: Cek Fitur Penting LightGBM\n# =========================================================\ntry:\n    importances = pd.DataFrame({\n        \"feature\": train_X.columns,\n        \"importance\": lgb_full.booster_.feature_importance(importance_type=\"gain\")\n    }).sort_values(\"importance\", ascending=False)\n    print(\"Top 30 feature importance (gain):\")\n    print(importances.head(30))\nexcept Exception as e:\n    print(\"Feature importance tidak tersedia:\", e)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# Blok 7 â€” Simpan Model & Artefak (model, fitur, OOF, FI, submission)\n# =========================================================\nimport json, os\nfrom pathlib import Path\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\nOUTPUT_DIR = Path(\"./outputs\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# 1) Simpan submission (di root & di outputs/)\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.to_csv(OUTPUT_DIR / \"submission.csv\", index=False)\n\n# 2) Simpan model LightGBM\n#    a) Booster (format text .txt, bisa dibuka/edit, kompatibel LightGBM)\nlgb_full.booster_.save_model(str(OUTPUT_DIR / \"lgbm_turbidity.txt\"))\n#    b) Sklearn wrapper (pickle .pkl via joblib, langsung .predict)\njoblib.dump(lgb_full, OUTPUT_DIR / \"lgbm_sklearn.pkl\")\n\n# 3) Simpan daftar fitur (urutan kolom penting untuk inference)\npd.Series(train_X.columns).to_csv(OUTPUT_DIR / \"features.txt\", index=False, header=False)\n\n# 4) Simpan feature importance (gain & split)\nfi = pd.DataFrame({\n    \"feature\": train_X.columns,\n    \"gain\":   lgb_full.booster_.feature_importance(importance_type=\"gain\"),\n    \"split\":  lgb_full.booster_.feature_importance(importance_type=\"split\"),\n}).sort_values(\"gain\", ascending=False)\nfi.to_csv(OUTPUT_DIR / \"feature_importance.csv\", index=False)\n\n# 5) Simpan OOF (untuk audit CV)\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred[:len(oof_df)]\noof_df.to_csv(OUTPUT_DIR / \"oof_predictions.csv\", index=False)\n\n# 6) Simpan metadata ringkas\nmeta = {\n    \"cv_mse\": float(cv_mse),\n    \"best_iter_final\": int(best_iter_final),\n    \"n_features\": int(len(train_X.columns)),\n    \"seed\": int(SEED),\n    \"steps_per_hour\": int(steps_per_hour)\n}\nwith open(OUTPUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\nprint(\"âœ… Artefak disimpan di:\", OUTPUT_DIR.resolve())\nprint(\"ðŸ“„ Daftar file:\", [p.name for p in OUTPUT_DIR.iterdir()])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# Blok 9 â€” LGBM-only CV-Ensemble (TimeSeriesSplit)\n# =========================================================\nimport joblib, json, gc\nfrom pathlib import Path\n\nENS_DIR = Path(\"./outputs_cv_ensemble\")\nENS_DIR.mkdir(parents=True, exist_ok=True)\n\ncv_models = []\ncv_best_iters = []\ntest_preds_folds = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)]\n    )\n    best_it = model.best_iteration_\n    cv_models.append(model)\n    cv_best_iters.append(int(best_it) if best_it is not None else int(lgb_params[\"n_estimators\"]))\n\n    # Prediksi test dengan model fold ini\n    pred_te = model.predict(test_X, num_iteration=best_it)\n    pred_te = np.clip(pred_te, 0, None)\n    test_preds_folds.append(pred_te)\n\n    # Simpan model per fold (booster dan joblib)\n    model.booster_.save_model(str(ENS_DIR / f\"lgbm_fold{fold}.txt\"))\n    joblib.dump(model, ENS_DIR / f\"lgbm_fold{fold}.pkl\")\n\n    # FI per fold\n    fi_fold = pd.DataFrame({\n        \"feature\": train_X.columns,\n        \"gain\":   model.booster_.feature_importance(importance_type=\"gain\"),\n        \"split\":  model.booster_.feature_importance(importance_type=\"split\"),\n        \"fold\":   fold\n    }).sort_values(\"gain\", ascending=False)\n    fi_fold.to_csv(ENS_DIR / f\"feature_importance_fold{fold}.csv\", index=False)\n\n    # OOF fold (untuk audit)\n    oof_fold = pd.DataFrame({\n        \"Record number\": train_fe.iloc[va_idx][IDCOL].values,\n        \"Timestamp\":     train_fe.iloc[va_idx][\"Timestamp\"].values,\n        TARGET:          y[va_idx],\n        \"oof_pred\":      np.clip(model.predict(train_X.iloc[va_idx], num_iteration=best_it), 0, None)\n    })\n    oof_fold.to_csv(ENS_DIR / f\"oof_fold{fold}.csv\", index=False)\n\n    print(f\"Ensemble Fold {fold} â€” best_iter={best_it}, test_pred_mean={pred_te.mean():.4f}\")\n\n# Rata-rata prediksi test dari semua fold (CV-ensemble)\npred_test_cv_ens = np.mean(np.vstack(test_preds_folds), axis=0)\npred_test_cv_ens = np.clip(pred_test_cv_ens, 0, None)\n\n# Simpan ringkasan ensemble\nwith open(ENS_DIR / \"cv_ensemble_meta.json\", \"w\") as f:\n    json.dump({\n        \"n_models\": len(cv_models),\n        \"best_iters\": cv_best_iters,\n        \"avg_best_iter\": float(np.mean(cv_best_iters)) if len(cv_best_iters)>0 else None\n    }, f, indent=2)\n\n# FI agregat (gain rata-rata across folds)\nfi_all = []\nfor fold in range(1, len(cv_models)+1):\n    fip = pd.read_csv(ENS_DIR / f\"feature_importance_fold{fold}.csv\")\n    fi_all.append(fip[[\"feature\", \"gain\"]].set_index(\"feature\"))\nfi_avg = pd.concat(fi_all, axis=1)\nfi_avg.columns = [f\"fold{idx+1}\" for idx in range(len(cv_models))]\nfi_avg[\"avg_gain\"] = fi_avg.mean(axis=1)\nfi_avg = fi_avg.sort_values(\"avg_gain\", ascending=False).reset_index()\nfi_avg.to_csv(ENS_DIR / \"feature_importance_avg.csv\", index=False)\n\ngc.collect()\nprint(\"âœ… CV-ensemble selesai. Model & FI per-fold tersimpan di\", ENS_DIR.resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:22:02.812490Z","iopub.execute_input":"2025-11-01T01:22:02.812807Z","iopub.status.idle":"2025-11-01T01:24:42.165004Z","shell.execute_reply.started":"2025-11-01T01:22:02.812784Z","shell.execute_reply":"2025-11-01T01:24:42.164091Z"}},"outputs":[{"name":"stdout","text":"Ensemble Fold 1 â€” best_iter=3153, test_pred_mean=3.2714\nEnsemble Fold 2 â€” best_iter=347, test_pred_mean=9.2749\nEnsemble Fold 3 â€” best_iter=2, test_pred_mean=4.1535\nEnsemble Fold 4 â€” best_iter=6, test_pred_mean=4.9754\nEnsemble Fold 5 â€” best_iter=204, test_pred_mean=7.8459\nâœ… CV-ensemble selesai. Model & FI per-fold tersimpan di /kaggle/working/outputs_cv_ensemble\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# =========================================================\n# Blok 10 â€” Buat submission dari CV-Ensemble & Simpan\n# =========================================================\nsubmission_cv = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": pred_test_cv_ens\n})\n\n# jaga urutan sesuai sample_submission\nif \"Record number\" in sub.columns:\n    submission_cv = sub[[\"Record number\"]].merge(submission_cv, on=\"Record number\", how=\"left\")\n\nsubmission_cv.to_csv(\"submission_cv_ensemble.csv\", index=False)\nsubmission_cv.to_csv(ENS_DIR / \"submission_cv_ensemble.csv\", index=False)\n\nprint(\"âœ… submission_cv_ensemble.csv dibuat!\")\nprint(submission_cv.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:24:52.314530Z","iopub.execute_input":"2025-11-01T01:24:52.314759Z","iopub.status.idle":"2025-11-01T01:24:52.382882Z","shell.execute_reply.started":"2025-11-01T01:24:52.314744Z","shell.execute_reply":"2025-11-01T01:24:52.382344Z"}},"outputs":[{"name":"stdout","text":"âœ… submission_cv_ensemble.csv dibuat!\n   Record number  Turbidity\n0          54916   5.512695\n1          54917   5.568242\n2          54918   6.136874\n3          54919   5.659361\n4          54920   6.280953\n5          54921   5.993691\n6          54922   6.280746\n7          54923   6.345777\n8          54924   6.995998\n9          54925   6.289674\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# random forest","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, dan Library (RandomForest only)\n# =========================================================\nimport os, gc, math, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils.validation import check_is_fitted\n\n# Simpan/Load model\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\n# ====== Paths (prioritas Kaggle sesuai permintaan) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback (opsional, jika dijalankan lokal)\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nrng = np.random.RandomState(SEED)\nnp.random.seed(SEED)\n\n\n\n# =========================================================\n# Blok 1 â€” Load Data, Sort Timestamp, dan Cek Kolom\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\n# Parse timestamp dan urutkan\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Daftar fitur top korelasi (sesuai list yang kamu kirim)\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n# Cek kolom wajib\nmust_train = TOP_CORR_RAW + [TARGET, \"Timestamp\", IDCOL]\nmust_test  = TOP_CORR_RAW + [\"Timestamp\", IDCOL]\nmissing_train = [c for c in must_train if c not in train.columns]\nmissing_test  = [c for c in must_test  if c not in test.columns]\nif missing_train: print(\"Peringatan, kolom hilang di train:\", missing_train)\nif missing_test:  print(\"Peringatan, kolom hilang di test :\", missing_test)\n\n\n\n# =========================================================\n# Blok 2 â€” Gabung Train+Test & Estimasi Resolusi Waktu\n# =========================================================\ntrain[\"__is_train__\"] = 1\ntest[\"__is_train__\"]  = 0\nfull = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt) == 0:\n        return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med <= 0:\n        return 1\n    return int(max(1, round(3600.0 / med)))\n\nsteps_per_hour = estimate_steps_per_hour(full)\nsteps_1h  = max(1, steps_per_hour)\nsteps_3h  = max(1, 3*steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\n\nprint(f\"Estimasi steps per hour: {steps_per_hour} | windows: 1h={steps_1h}, 3h={steps_3h}, 6h={steps_6h}, 24h={steps_24h}\")\n\n\n\n# =========================================================\n# Blok 3 â€” Imputasi Time-Aware + Fitur Lag/Rolling (Top Korelasi)\n# =========================================================\ndef timewise_impute(df, cols, steps_6h=12):\n    out = df.copy()\n    out[cols] = out[cols].ffill().bfill()             # ffill & bfill\n    for c in cols:\n        out[c] = out[c].fillna(out[c].rolling(window=steps_6h, min_periods=1).median())  # rolling median (ke belakang)\n        out[c] = out[c].fillna(out[c].median())       # fallback median global\n    return out\n\ndef add_lag_roll(df, cols, lags, rolls):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n        for w in rolls:\n            out[f\"{c}_roll{w}_mean\"] = out[c].shift(1).rolling(window=w, min_periods=1).mean()\n            out[f\"{c}_roll{w}_std\"]  = out[c].shift(1).rolling(window=w, min_periods=1).std()\n            out[f\"{c}_roll{w}_min\"]  = out[c].shift(1).rolling(window=w, min_periods=1).min()\n            out[f\"{c}_roll{w}_max\"]  = out[c].shift(1).rolling(window=w, min_periods=1).max()\n    return out\n\n# Sanitasi sederhana\nfull[\"pH\"] = full[\"pH\"].clip(lower=0, upper=14)\n\n# Imputasi hanya fitur top-korelasi\nfull = timewise_impute(full, TOP_CORR_RAW, steps_6h=steps_6h)\n\n# Tambah lag/rolling (anti-leakage pakai shift(1) untuk rolling)\nLAGS  = [1, steps_1h, steps_3h]                 # 1-step, ~1 jam, ~3 jam\nROLLS = [steps_1h, steps_6h, steps_24h]         # ~1h, ~6h, ~24h\nfull = add_lag_roll(full, TOP_CORR_RAW, LAGS, ROLLS)\n\n# Susun daftar fitur akhir\nlagroll_feats = [c for c in full.columns if any(s in c for s in [\"_lag\", \"_roll\"])]\nFEATURES = TOP_CORR_RAW + lagroll_feats\nprint(f\"Jumlah fitur dipakai: {len(FEATURES)}\")\n\n\n\n\n# =========================================================\n# Blok 4 â€” Split Train/Test (drop NaN awal), Matrix X/y\n# =========================================================\ntrain_fe = full[full[\"__is_train__\"]==1].copy()\ntest_fe  = full[full[\"__is_train__\"]==0].copy()\ntrain_fe[TARGET] = train[TARGET].values  # align target\n\n# Drop baris train yang masih NaN (efek lag/rolling di awal)\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\ntrain_X = train_fe[FEATURES].copy()\ny       = train_fe[TARGET].values\ntest_X  = test_fe[FEATURES].copy()\n\n# Isi NaN sisa di test dengan median train (aman untuk inference)\ntest_X = test_X.fillna(train_X.median())\n\nprint(\"train_X shape:\", train_X.shape, \"| test_X shape:\", test_X.shape)\n\n\n\n\n# =========================================================\n# Blok 5 â€” TimeSeriesSplit CV (OOF) dengan RandomForest\n# =========================================================\n# Catatan: RF tidak punya early stopping.\n# Kita pakai CV OOF untuk estimasi MSE & juga simpan prediksi test per-fold (CV-ensemble).\n\nn_splits = 5\n# Buffer 'gap' untuk memutus ketergantungan rolling besar\ngap = max(1, steps_1h)  # bisa ditingkatkan, misal steps_6h atau steps_24h jika leakage terasa\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\nrf_params = dict(\n    n_estimators=800,\n    max_depth=None,             # bisa dikunci (mis. 18-28) jika overfit\n    max_features=\"sqrt\",\n    min_samples_split=4,\n    min_samples_leaf=3,\n    bootstrap=True,\n    n_jobs=-1,\n    random_state=SEED\n)\n\noof_pred = np.zeros(len(train_fe))\ntest_fold_preds = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    rf = RandomForestRegressor(**rf_params)\n    rf.fit(X_tr, y_tr)\n\n    pred_va = rf.predict(X_va)\n    oof_pred[va_idx] = np.clip(pred_va, 0, None)  # NTU tidak negatif\n\n    # Simpan prediksi test per fold (untuk ensemble average)\n    pred_te = rf.predict(test_X)\n    pred_te = np.clip(pred_te, 0, None)\n    test_fold_preds.append(pred_te)\n\n    mse_fold = mean_squared_error(y[va_idx], oof_pred[va_idx])\n    print(f\"Fold {fold} â€” MSE={mse_fold:.6f}\")\n\ncv_mse = mean_squared_error(y, oof_pred)\nprint(f\"\\nCV MSE (OOF, RF) = {cv_mse:.6f}\")\n\n\n\n# =========================================================\n# Blok 6 â€” Train Full RF & Prediksi Test (+ CV-Ensemble)\n# =========================================================\n# 1) Full-model: latih ulang di seluruh data\nrf_full = RandomForestRegressor(**rf_params)\nrf_full.fit(train_X, y)\n\npred_test_full = rf_full.predict(test_X)\npred_test_full = np.clip(pred_test_full, 0, None)\n\n# 2) CV-ensemble: rata-rata prediksi test dari semua model fold\nif len(test_fold_preds) > 0:\n    pred_test_cv_ens = np.mean(np.vstack(test_fold_preds), axis=0)\n    pred_test_cv_ens = np.clip(pred_test_cv_ens, 0, None)\nelse:\n    pred_test_cv_ens = pred_test_full.copy()\n\nprint(\"Pred test (full)  mean/std:\", float(np.mean(pred_test_full)), float(np.std(pred_test_full)))\nprint(\"Pred test (cvens) mean/std:\", float(np.mean(pred_test_cv_ens)), float(np.std(pred_test_cv_ens)))\n\n\n\n\n\n# =========================================================\n# Blok 7 â€” Buat submission.csv (urut sesuai sample)\n# =========================================================\n# Default: pakai CV-ensemble (lebih stabil). Kalau mau pakai full model, ganti variabel di bawah.\nFINAL_PRED = pred_test_cv_ens  # atau: pred_test_full\n\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n\n# Jaga urutan sesuai sample_submission\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n\n\n\n\n# =========================================================\n# Blok 8 â€” Simpan Artefak (model, fitur, OOF, FI, metadata)\n# =========================================================\nOUT_DIR = Path(\"./outputs_rf\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Simpan kedua versi submission (full & ensemble)\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": pred_test_full\n}).to_csv(OUT_DIR / \"submission_full_model.csv\", index=False)\n\nif len(test_fold_preds) > 0:\n    pd.DataFrame({\n        \"Record number\": test_fe[IDCOL].values,\n        \"Turbidity\": pred_test_cv_ens\n    }).to_csv(OUT_DIR / \"submission_cv_ensemble.csv\", index=False)\n\n# Simpan model full\njoblib.dump(rf_full, OUT_DIR / \"rf_full.pkl\")\n\n# Simpan fitur (urutan penting untuk inference)\npd.Series(train_X.columns).to_csv(OUT_DIR / \"features.txt\", index=False, header=False)\n\n# Simpan OOF pred\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# Feature importance (RF Gini importance)\ntry:\n    fi = pd.DataFrame({\n        \"feature\": train_X.columns,\n        \"importance\": rf_full.feature_importances_\n    }).sort_values(\"importance\", ascending=False)\n    fi.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\n    print(\"Top 20 FI:\")\n    print(fi.head(20))\nexcept Exception as e:\n    print(\"Gagal mengambil feature_importances_:\", e)\n\n# Metadata\nmeta = {\n    \"cv_mse\": float(cv_mse),\n    \"n_splits\": int(n_splits),\n    \"gap\": int(gap),\n    \"rf_params\": rf_params,\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(train_X.columns))\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\nprint(\"âœ… Artefak disimpan di:\", OUT_DIR.resolve())\nprint(\"ðŸ“„ Files:\", [p.name for p in OUT_DIR.iterdir()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:30:31.219366Z","iopub.execute_input":"2025-11-01T01:30:31.220006Z","iopub.status.idle":"2025-11-01T01:38:04.023088Z","shell.execute_reply.started":"2025-11-01T01:30:31.219983Z","shell.execute_reply":"2025-11-01T01:38:04.022314Z"}},"outputs":[{"name":"stdout","text":"Estimasi steps per hour: 6 | windows: 1h=6, 3h=18, 6h=36, 24h=144\nJumlah fitur dipakai: 128\ntrain_X shape: (47364, 128) | test_X shape: (14610, 128)\nFold 1 â€” MSE=5.247021\nFold 2 â€” MSE=38.993243\nFold 3 â€” MSE=118.627333\nFold 4 â€” MSE=19.076475\nFold 5 â€” MSE=51.116512\n\nCV MSE (OOF, RF) = 39.705318\nPred test (full)  mean/std: 6.666789935990432 1.3255411832094708\nPred test (cvens) mean/std: 6.555483457049812 1.882542586905388\nâœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916   5.330515\n1          54917   5.568005\n2          54918   6.102316\n3          54919   6.196658\n4          54920   5.883959\n5          54921   5.911880\n6          54922   6.084784\n7          54923   6.447823\n8          54924   7.104583\n9          54925   7.195050\nTop 20 FI:\n                                         feature  importance\n34              Average Water Speed_roll144_mean    0.038460\n126                      Chlorophyll_roll144_min    0.025130\n82                                pH_roll144_max    0.024130\n64   Dissolved Oxygen (%Saturation)_roll144_mean    0.022163\n124                     Chlorophyll_roll144_mean    0.020787\n66    Dissolved Oxygen (%Saturation)_roll144_min    0.019023\n109                     Temperature_roll144_mean    0.018356\n52                          Salinity_roll144_max    0.017597\n79                               pH_roll144_mean    0.017103\n37               Average Water Speed_roll144_max    0.017082\n22              Specific Conductance_roll144_max    0.017060\n111                      Temperature_roll144_min    0.016588\n49                         Salinity_roll144_mean    0.015651\n19             Specific Conductance_roll144_mean    0.015622\n122                       Chlorophyll_roll36_min    0.014676\n67    Dissolved Oxygen (%Saturation)_roll144_max    0.014511\n110                      Temperature_roll144_std    0.013757\n112                      Temperature_roll144_max    0.013600\n96                  Dissolved Oxygen_roll144_min    0.013448\n107                       Temperature_roll36_min    0.013348\nâœ… Artefak disimpan di: /kaggle/working/outputs_rf\nðŸ“„ Files: ['features.txt', 'submission_full_model.csv', 'feature_importance.csv', 'oof_predictions.csv', 'metadata.json', 'rf_full.pkl', 'submission.csv', 'submission_cv_ensemble.csv']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# XGBOST","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, dan Library (XGBoost only)\n# =========================================================\nimport os, gc, math, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# Install XGBoost jika belum tersedia\ntry:\n    import xgboost as xgb\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xgboost\"])\n    import xgboost as xgb\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\n# Simpan/Load model\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\n# ====== Paths (prioritas Kaggle sesuai permintaan) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback (opsional, jika ingin jalankan lokal)\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n\n# =========================================================\n# Blok 1 â€” Load Data, Sort Timestamp, dan Cek Kolom\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\n# Parse timestamp dan urutkan\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Daftar fitur top korelasi (sesuai list yang kamu kirim)\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n# Cek kolom wajib\nmust_train = TOP_CORR_RAW + [TARGET, \"Timestamp\", IDCOL]\nmust_test  = TOP_CORR_RAW + [\"Timestamp\", IDCOL]\nmissing_train = [c for c in must_train if c not in train.columns]\nmissing_test  = [c for c in must_test  if c not in test.columns]\nif missing_train: print(\"Peringatan, kolom hilang di train:\", missing_train)\nif missing_test:  print(\"Peringatan, kolom hilang di test :\", missing_test)\n\n\n# =========================================================\n# Blok 2 â€” Gabung Train+Test & Estimasi Resolusi Waktu\n# =========================================================\ntrain[\"__is_train__\"] = 1\ntest[\"__is_train__\"]  = 0\nfull = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt) == 0:\n        return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med <= 0:\n        return 1\n    return int(max(1, round(3600.0 / med)))\n\nsteps_per_hour = estimate_steps_per_hour(full)\nsteps_1h  = max(1, steps_per_hour)\nsteps_3h  = max(1, 3*steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\n\nprint(f\"Estimasi steps per hour: {steps_per_hour} | windows: 1h={steps_1h}, 3h={steps_3h}, 6h={steps_6h}, 24h={steps_24h}\")\n\n\n# =========================================================\n# Blok 3 â€” Imputasi Time-Aware + Fitur Lag/Rolling (Top Korelasi)\n# =========================================================\ndef timewise_impute(df, cols, steps_6h=12):\n    out = df.copy()\n    out[cols] = out[cols].ffill().bfill()  # ffill & bfill\n    for c in cols:\n        out[c] = out[c].fillna(out[c].rolling(window=steps_6h, min_periods=1).median())  # rolling median (ke belakang)\n        out[c] = out[c].fillna(out[c].median())  # fallback median global\n    return out\n\ndef add_lag_roll(df, cols, lags, rolls):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n        for w in rolls:\n            out[f\"{c}_roll{w}_mean\"] = out[c].shift(1).rolling(window=w, min_periods=1).mean()\n            out[f\"{c}_roll{w}_std\"]  = out[c].shift(1).rolling(window=w, min_periods=1).std()\n            out[f\"{c}_roll{w}_min\"]  = out[c].shift(1).rolling(window=w, min_periods=1).min()\n            out[f\"{c}_roll{w}_max\"]  = out[c].shift(1).rolling(window=w, min_periods=1).max()\n    return out\n\n# Sanitasi sederhana\nfull[\"pH\"] = full[\"pH\"].clip(lower=0, upper=14)\n\n# Imputasi hanya fitur top-korelasi\nfull = timewise_impute(full, TOP_CORR_RAW, steps_6h=steps_6h)\n\n# Tambah lag/rolling (anti-leakage pakai shift(1) untuk rolling)\nLAGS  = [1, steps_1h, steps_3h]                 # 1-step, ~1 jam, ~3 jam\nROLLS = [steps_1h, steps_6h, steps_24h]         # ~1h, ~6h, ~24h\nfull = add_lag_roll(full, TOP_CORR_RAW, LAGS, ROLLS)\n\n# Susun daftar fitur akhir\nlagroll_feats = [c for c in full.columns if any(s in c for s in [\"_lag\", \"_roll\"])]\nFEATURES = TOP_CORR_RAW + lagroll_feats\nprint(f\"Jumlah fitur dipakai: {len(FEATURES)}\")\n\n# =========================================================\n# Blok 4 â€” Split Train/Test (drop NaN awal), Matrix X/y\n# =========================================================\ntrain_fe = full[full[\"__is_train__\"]==1].copy()\ntest_fe  = full[full[\"__is_train__\"]==0].copy()\ntrain_fe[TARGET] = train[TARGET].values  # align target\n\n# Drop baris train yang masih NaN (efek lag/rolling di awal)\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\ntrain_X = train_fe[FEATURES].copy()\ny       = train_fe[TARGET].values\ntest_X  = test_fe[FEATURES].copy()\n\n# Isi NaN sisa di test dengan median train (aman untuk inference)\ntest_X = test_X.fillna(train_X.median())\n\nprint(\"train_X shape:\", train_X.shape, \"| test_X shape:\", test_X.shape)\n\n\n\n# =========================================================\n# Blok 5 â€” TimeSeriesSplit CV (XGBoost, Early Stopping, OOF)\n# =========================================================\nfrom xgboost import XGBRegressor\n\nn_splits = 5\n# Buffer 'gap' untuk memutus ketergantungan rolling besar\ngap = max(1, steps_1h)  # bisa ditingkatkan ke steps_6h/steps_24h jika leakage terasa\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\nxgb_params = dict(\n    objective=\"reg:squarederror\",  # MSE objective\n    learning_rate=0.05,\n    max_depth=8,\n    min_child_weight=4,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    n_estimators=5000,             # besar, dipangkas oleh early_stopping\n    random_state=SEED,\n    tree_method=\"hist\",            # aman & cepat di CPU; ganti \"gpu_hist\" jika yakin GPU tersedia\n)\n\noof_pred = np.zeros(len(train_fe))\ntest_fold_preds = []\nbest_iters = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        verbose=False,\n        early_stopping_rounds=300\n    )\n\n    best_it = model.best_iteration if hasattr(model, \"best_iteration\") else model.n_estimators\n    best_iters.append(int(best_it))\n\n    pred_va = model.predict(X_va, iteration_range=(0, best_it))\n    oof_pred[va_idx] = np.clip(pred_va, 0, None)  # NTU tidak negatif\n\n    # Simpan prediksi test per fold (untuk ensemble)\n    pred_te = model.predict(test_X, iteration_range=(0, best_it))\n    pred_te = np.clip(pred_te, 0, None)\n    test_fold_preds.append(pred_te)\n\n    fold_mse = mean_squared_error(y[va_idx], oof_pred[va_idx])\n    print(f\"Fold {fold} â€” best_iter={best_it}, MSE={fold_mse:.6f}\")\n\ncv_mse = mean_squared_error(y, oof_pred)\nbest_iter_final = int(np.median(best_iters)) if len(best_iters)>0 else xgb_params[\"n_estimators\"]\nprint(f\"\\nCV MSE (OOF, XGB) = {cv_mse:.6f} | median best_iter = {best_iter_final}\")\n\n\n# =========================================================\n# Blok 6 â€” Train Full Model & Prediksi Test (+ CV-Ensemble)\n# =========================================================\n# 1) Full-model: latih ulang di seluruh data dengan n_estimators = median best_iter\nxgb_full = XGBRegressor(**{**xgb_params, \"n_estimators\": best_iter_final})\nxgb_full.fit(train_X, y, verbose=False)\n\npred_test_full = xgb_full.predict(test_X)\npred_test_full = np.clip(pred_test_full, 0, None)\n\n# 2) CV-ensemble: rata-rata prediksi test dari semua model fold\nif len(test_fold_preds) > 0:\n    pred_test_cv_ens = np.mean(np.vstack(test_fold_preds), axis=0)\n    pred_test_cv_ens = np.clip(pred_test_cv_ens, 0, None)\nelse:\n    pred_test_cv_ens = pred_test_full.copy()\n\nprint(\"Pred test (full)  mean/std:\", float(np.mean(pred_test_full)), float(np.std(pred_test_full)))\nprint(\"Pred test (cvens) mean/std:\", float(np.mean(pred_test_cv_ens)), float(np.std(pred_test_cv_ens)))\n\n\n# =========================================================\n# Blok 7 â€” Buat submission.csv (urut sesuai sample)\n# =========================================================\n# Default: pakai CV-ensemble (umumnya lebih stabil). Kalau mau pakai full model, ganti variabel di bawah.\nFINAL_PRED = pred_test_cv_ens  # atau: pred_test_full\n\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n\n# Jaga urutan sesuai sample_submission\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n# =========================================================\n# Blok 8 â€” Simpan Artefak (model, fitur, OOF, FI, metadata)\n# =========================================================\nOUT_DIR = Path(\"./outputs_xgb\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Simpan submission (utama & alternatif)\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": pred_test_full\n}).to_csv(OUT_DIR / \"submission_full_model.csv\", index=False)\n\nif len(test_fold_preds) > 0:\n    pd.DataFrame({\n        \"Record number\": test_fe[IDCOL].values,\n        \"Turbidity\": pred_test_cv_ens\n    }).to_csv(OUT_DIR / \"submission_cv_ensemble.csv\", index=False)\n\n# Simpan model full\njoblib.dump(xgb_full, OUT_DIR / \"xgb_full.pkl\")\n# Simpan booster model JSON (portable ke XGBoost native)\nxgb_full.get_booster().save_model(str(OUT_DIR / \"xgb_full.json\"))\n\n# Simpan daftar fitur (urutan penting untuk inference)\npd.Series(train_X.columns).to_csv(OUT_DIR / \"features.txt\", index=False, header=False)\n\n# Simpan OOF pred\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# Feature importance (gain)\ntry:\n    booster = xgb_full.get_booster()\n    gain_map = booster.get_score(importance_type=\"gain\")  # dict: {feature_name: gain}\n    # Pastikan nama fitur konsisten dengan kolom: XGBoost memetakan secara otomatis ke f0,f1... jika tidak pakai DMatrix\n    # Untuk kestabilan, gunakan feature_importances_ dari wrapper sebagai fallback\n    fi_gain = pd.DataFrame({\n        \"feature\": list(train_X.columns),\n        \"importance\": xgb_full.feature_importances_\n    }).sort_values(\"importance\", ascending=False)\n    fi_gain.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\n    print(\"Top 20 FI (wrapper-based):\")\n    print(fi_gain.head(20))\nexcept Exception as e:\n    print(\"Gagal mengambil feature importance:\", e)\n\n# Metadata\nmeta = {\n    \"cv_mse\": float(cv_mse),\n    \"n_splits\": int(n_splits),\n    \"gap\": int(gap),\n    \"xgb_params\": {**xgb_params, \"n_estimators\": int(best_iter_final)},\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(train_X.columns))\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\nprint(\"âœ… Artefak disimpan di:\", OUT_DIR.resolve())\nprint(\"ðŸ“„ Files:\", [p.name for p in OUT_DIR.iterdir()])\n\n\n# =========================================================\n# Blok 9 â€” (Opsional) Reload Model & Cek Prediksi Cepat\n# =========================================================\nreloaded = joblib.load(OUT_DIR / \"xgb_full.pkl\")\ntest_pred_reload = np.clip(reloaded.predict(test_X), 0, None)\nassert len(test_pred_reload) == len(test_fe), \"Mismatch panjang prediksi vs test!\"\nprint(\"Reload OK. Contoh 5 pred:\", np.round(test_pred_reload[:5], 4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:38:05.800632Z","iopub.execute_input":"2025-11-01T01:38:05.801158Z","iopub.status.idle":"2025-11-01T01:40:34.285271Z","shell.execute_reply.started":"2025-11-01T01:38:05.801135Z","shell.execute_reply":"2025-11-01T01:40:34.283844Z"}},"outputs":[{"name":"stdout","text":"Estimasi steps per hour: 6 | windows: 1h=6, 3h=18, 6h=36, 24h=144\nJumlah fitur dipakai: 128\ntrain_X shape: (47364, 128) | test_X shape: (14610, 128)\nFold 1 â€” best_iter=1133, MSE=4.529689\nFold 2 â€” best_iter=1273, MSE=37.132218\nFold 3 â€” best_iter=0, MSE=13.490963\nFold 4 â€” best_iter=0, MSE=8.400354\nFold 5 â€” best_iter=15, MSE=47.766616\n\nCV MSE (OOF, XGB) = 19.415194 | median best_iter = 15\nPred test (full)  mean/std: 5.488552570343018 1.47579026222229\nPred test (cvens) mean/std: 5.610215663909912 1.0423319339752197\nâœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916   5.688222\n1          54917   6.173228\n2          54918   7.083472\n3          54919   6.831615\n4          54920   6.278066\n5          54921   6.215439\n6          54922   6.873591\n7          54923   7.334956\n8          54924   7.366265\n9          54925   7.181188\nTop 20 FI (wrapper-based):\n                                         feature  importance\n111                      Temperature_roll144_min    0.070469\n66    Dissolved Oxygen (%Saturation)_roll144_min    0.053866\n97                  Dissolved Oxygen_roll144_max    0.052948\n126                      Chlorophyll_roll144_min    0.049339\n82                                pH_roll144_max    0.048645\n64   Dissolved Oxygen (%Saturation)_roll144_mean    0.047234\n34              Average Water Speed_roll144_mean    0.043545\n94                 Dissolved Oxygen_roll144_mean    0.039669\n52                          Salinity_roll144_max    0.030180\n79                               pH_roll144_mean    0.028351\n83                         Dissolved Oxygen_lag1    0.021201\n37               Average Water Speed_roll144_max    0.020015\n105                      Temperature_roll36_mean    0.017183\n21              Specific Conductance_roll144_min    0.016122\n38                                 Salinity_lag1    0.016067\n93                   Dissolved Oxygen_roll36_max    0.015942\n109                     Temperature_roll144_mean    0.015709\n17               Specific Conductance_roll36_min    0.013757\n96                  Dissolved Oxygen_roll144_min    0.013485\n45                          Salinity_roll36_mean    0.013201\nâœ… Artefak disimpan di: /kaggle/working/outputs_xgb\nðŸ“„ Files: ['xgb_full.pkl', 'features.txt', 'submission_full_model.csv', 'xgb_full.json', 'feature_importance.csv', 'oof_predictions.csv', 'metadata.json', 'submission.csv', 'submission_cv_ensemble.csv']\nReload OK. Contoh 5 pred: [4.4841 4.4935 4.6465 5.1295 5.3899]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"train_fe.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGB Update","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, Library\n# =========================================================\nimport os, gc, math, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# XGBoost\ntry:\n    import xgboost as xgb\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xgboost\"])\n    import xgboost as xgb\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\n# Paths Kaggle\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback lokal\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Top-korelasi (dari daftar kamu)\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n\n\n# =========================================================\n# Blok 1 â€” Load & Sort\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\n# Cek kolom wajib\nneed_train = TOP_CORR_RAW + [TARGET, \"Timestamp\", IDCOL]\nneed_test  = TOP_CORR_RAW + [\"Timestamp\", IDCOL]\nmt = [c for c in need_train if c not in train.columns]\nms = [c for c in need_test  if c not in test.columns]\nif mt: print(\"âš ï¸ Missing train cols:\", mt)\nif ms: print(\"âš ï¸ Missing test cols :\", ms)\n\n\n\n# =========================================================\n# Blok 2 â€” Gabung & Estimasi Resolusi Waktu\n# =========================================================\ntrain[\"__is_train__\"] = 1\ntest[\"__is_train__\"]  = 0\nfull = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt)==0: return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med<=0: return 1\n    return int(max(1, round(3600.0/med)))\n\nsteps_per_hour = estimate_steps_per_hour(full)\nsteps_1h  = max(1, steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_12h = max(1, 12*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\nsteps_48h = max(1, 48*steps_per_hour)\n\nprint(f\"steps/hour={steps_per_hour} | 1h={steps_1h} 6h={steps_6h} 12h={steps_12h} 24h={steps_24h} 48h={steps_48h}\")\n\n\n\n\n\n# =========================================================\n# Blok 3 â€” Flag Missingness & Imputasi Time-aware (no ffill lintas gap)\n# =========================================================\n# Flag missingness sebelum imputasi\nfor c in TOP_CORR_RAW:\n    full[c+\"_was_na\"] = full[c].isna().astype(int)\n\n# Batas \"gap besar\" (contoh: >6 jam) â€” optional; di sini kita tidak pakai ffill,\n# jadi tak perlu memutus secara eksplisit. Dibiarkan sebagai referensi:\nfull[\"gap_sec\"] = full[\"Timestamp\"].diff().dt.total_seconds().fillna(0)\n\ndef impute_timeaware_past_rolling(df, cols, win=12):\n    # Imputasi pakai rolling median dari MASA LALU (shift(1)) â†’ tidak menyeberangkan informasi\n    out = df.copy()\n    for c in cols:\n        past_med = out[c].shift(1).rolling(window=win, min_periods=1).median()\n        out[c] = out[c].fillna(past_med)\n        out[c] = out[c].fillna(out[c].median())  # fallback global\n    return out\n\n# Sanitasi ringan\nfull[\"pH\"] = full[\"pH\"].clip(0, 14)\n\n# Imputasi: gunakan window ~6 jam\nfull = impute_timeaware_past_rolling(full, TOP_CORR_RAW, win=steps_6h)\n\n\n\n# =========================================================\n# Blok 4 â€” Lag & Rolling Features (tanpa std)\n# =========================================================\ndef add_lag_features(df, cols, lags):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n    return out\n\ndef add_roll_features(df, cols, windows):\n    out = df.copy()\n    for c in cols:\n        for w in windows:\n            base = out[c].shift(1).rolling(window=w, min_periods=1)  # masa lalu saja\n            out[f\"{c}_roll{w}_mean\"] = base.mean()\n            out[f\"{c}_roll{w}_min\"]  = base.min()\n            out[f\"{c}_roll{w}_max\"]  = base.max()\n    return out\n\nLAGS  = [1, steps_1h, steps_6h]  # 1-step, ~1h, ~6h\nROLLS = [steps_1h, steps_6h, steps_12h, steps_24h, steps_48h]  # 1h,6h,12h,24h,48h\n\nfull = add_lag_features(full, TOP_CORR_RAW, LAGS)\nfull = add_roll_features(full, TOP_CORR_RAW, ROLLS)\n\n# Kumpulkan fitur final\nlag_feats   = [c for c in full.columns if any(s in c for s in [\"_lag\"])]\nroll_feats  = [c for c in full.columns if any(s in c for s in [\"_roll\"]) and c.endswith((\"mean\",\"min\",\"max\"))]\nmiss_flags  = [c+\"_was_na\" for c in TOP_CORR_RAW]\nFEATURES    = TOP_CORR_RAW + miss_flags + lag_feats + roll_feats\n\nprint(f\"n_features={len(FEATURES)}\")\n\n\n\n# =========================================================\n# Blok 5 â€” Split Train/Test, Log-Target, Matrix X/y\n# =========================================================\ntrain_fe = full[full[\"__is_train__\"]==1].copy()\ntest_fe  = full[full[\"__is_train__\"]==0].copy()\n\n# Align target ke urutan train_fe\ntrain_fe[TARGET] = train[TARGET].values\n\n# Buang baris awal yang masih NaN (efek lag/rolling)\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\ntrain_X = train_fe[FEATURES].copy()\ntest_X  = test_fe[FEATURES].copy()\n\n# Isi NaN sisa pada test dengan median train\ntest_X = test_X.fillna(train_X.median())\n\n# Log1p target (stabilisasi)\nUSE_LOG_TARGET = True\nif USE_LOG_TARGET:\n    y_raw = train_fe[TARGET].values\n    y = np.log1p(y_raw)\nelse:\n    y = train_fe[TARGET].values\n\nprint(\"train_X:\", train_X.shape, \"test_X:\", test_X.shape, \"| USE_LOG_TARGET:\", USE_LOG_TARGET)\n\n\n\n\n# =========================================================\n# Blok 6 â€” TimeSeriesSplit CV (gap = 24h) & Training XGB\n# =========================================================\n# Gap minimal = window terpanjang (24h). Bisa dinaikkan ke 2*24h untuk ekstra aman.\nGAP_MULTIPLIER = 1\ngap = steps_24h * GAP_MULTIPLIER\n\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\n# Param XGB yang lebih stabil\nxgb_params = dict(\n    objective=\"reg:squarederror\",\n    learning_rate=0.03,\n    max_depth=8,\n    min_child_weight=8,\n    subsample=0.8,\n    colsample_bytree=0.7,\n    reg_lambda=3.0,\n    reg_alpha=0.2,\n    n_estimators=6000,\n    random_state=SEED,\n    tree_method=\"hist\",   # ganti ke 'gpu_hist' bila GPU XGB tersedia\n)\n\noof_pred_log = np.zeros(len(train_fe))\ntest_fold_preds = []\nbest_iters = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        verbose=False,\n        early_stopping_rounds=500\n    )\n\n    best_it = model.best_iteration if hasattr(model, \"best_iteration\") else None\n    best_iters.append(int(best_it) if best_it is not None else int(xgb_params[\"n_estimators\"]))\n\n    # Prediksi valid di ruang log (OOF tetap log), lalu inverse buat MSE report\n    if best_it is None:\n        pred_va_log = model.predict(X_va)\n    else:\n        pred_va_log = model.predict(X_va, iteration_range=(0, best_it))\n\n    oof_pred_log[va_idx] = pred_va_log\n\n    # Prediksi test (log), lalu simpan (nanti kita inverse & ensemble)\n    if best_it is None:\n        pred_te_log = model.predict(test_X)\n    else:\n        pred_te_log = model.predict(test_X, iteration_range=(0, best_it))\n    test_fold_preds.append(pred_te_log)\n\n    # MSE OOF dihitung pada ruang asli (expm1)\n    y_va_raw = np.expm1(y_va) if USE_LOG_TARGET else y_va\n    pred_va_raw = np.expm1(pred_va_log) if USE_LOG_TARGET else pred_va_log\n    pred_va_raw = np.clip(pred_va_raw, 0, None)\n    mse_fold = mean_squared_error(y_va_raw, pred_va_raw)\n    print(f\"Fold {fold} â€” best_iter={best_it}, MSE(raw)={mse_fold:.6f}\")\n\nbest_iter_final = int(np.median(best_iters)) if len(best_iters)>0 else int(xgb_params[\"n_estimators\"])\n\n# Hitung CV MSE (OOF) di ruang asli\nif USE_LOG_TARGET:\n    oof_pred_raw = np.expm1(oof_pred_log)\nelse:\n    oof_pred_raw = oof_pred_log\noof_pred_raw = np.clip(oof_pred_raw, 0, None)\n\ncv_mse = mean_squared_error(train_fe[TARGET].values, oof_pred_raw)\nprint(f\"\\nCV MSE (OOF, raw) = {cv_mse:.6f} | median best_iter = {best_iter_final}\")\n\n\n\n# =========================================================\n# Blok 7 â€” Train Full Model & Prediksi Test (Full + CV-Ensemble)\n# =========================================================\n# Full-model di ruang log (jika USE_LOG_TARGET)\nxgb_full = XGBRegressor(**{**xgb_params, \"n_estimators\": best_iter_final})\nxgb_full.fit(train_X, y, verbose=False)\n\npred_test_full_log = xgb_full.predict(test_X)\npred_test_cvens_log = np.mean(np.vstack(test_fold_preds), axis=0) if len(test_fold_preds)>0 else pred_test_full_log\n\n# Inverse ke ruang asli & clip non-negatif\npred_test_full  = np.expm1(pred_test_full_log)  if USE_LOG_TARGET else pred_test_full_log\npred_test_cvens = np.expm1(pred_test_cvens_log) if USE_LOG_TARGET else pred_test_cvens_log\npred_test_full  = np.clip(pred_test_full,  0, None)\npred_test_cvens = np.clip(pred_test_cvens, 0, None)\n\nprint(\"Pred test (full)  mean/std:\", float(pred_test_full.mean()), float(pred_test_full.std()))\nprint(\"Pred test (cvens) mean/std:\", float(pred_test_cvens.mean()), float(pred_test_cvens.std()))\n\n\n\n# =========================================================\n# Blok 8 â€” Submission (pakai CV-ensemble by default)\n# =========================================================\nFINAL_PRED = pred_test_cvens  # ganti ke pred_test_full jika ingin full-model\n\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n\n# Jaga urutan sesuai sample\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n\n\n\n\n# =========================================================\n# Blok 9 â€” Artefak (model, fitur, OOF, FI, metadata)\n# =========================================================\nOUT_DIR = Path(\"./outputs_xgb_fixed\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Simpan submission alternatif\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": pred_test_full}).to_csv(\n    OUT_DIR / \"submission_full_model.csv\", index=False\n)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": pred_test_cvens}).to_csv(\n    OUT_DIR / \"submission_cv_ensemble.csv\", index=False\n)\n\n# Simpan model & booster\njoblib.dump(xgb_full, OUT_DIR / \"xgb_full.pkl\")\nxgb_full.get_booster().save_model(str(OUT_DIR / \"xgb_full.json\"))\n\n# Simpan daftar fitur\npd.Series(train_X.columns).to_csv(OUT_DIR / \"features.txt\", index=False, header=False)\n\n# Simpan OOF (ruang asli)\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred_raw[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# FI (wrapper-based)\nfi_gain = pd.DataFrame({\n    \"feature\": train_X.columns,\n    \"importance\": xgb_full.feature_importances_\n}).sort_values(\"importance\", ascending=False)\nfi_gain.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\nprint(\"Top 20 FI:\")\nprint(fi_gain.head(20))\n\n# Metadata\nmeta = {\n    \"cv_mse_raw\": float(cv_mse),\n    \"n_splits\": int(n_splits),\n    \"gap_steps\": int(gap),\n    \"gap_hours\": float(gap / steps_per_hour),\n    \"xgb_params\": {**xgb_params, \"n_estimators\": int(best_iter_final)},\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(train_X.columns)),\n    \"use_log_target\": bool(USE_LOG_TARGET)\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\nprint(\"âœ… Artefak disimpan di:\", OUT_DIR.resolve())\nprint(\"ðŸ“„ Files:\", [p.name for p in OUT_DIR.iterdir()])\n\n\n\n\n\n# =========================================================\n# Blok 10 â€” (Opsional) Reload & Sanity Check\n# =========================================================\nreloaded = joblib.load(OUT_DIR / \"xgb_full.pkl\")\nreload_pred = reloaded.predict(test_X)\nreload_pred = np.expm1(reload_pred) if USE_LOG_TARGET else reload_pred\nreload_pred = np.clip(reload_pred, 0, None)\nprint(\"Reload OK. 5 preds:\", np.round(reload_pred[:5], 4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:46:31.700760Z","iopub.execute_input":"2025-11-01T01:46:31.701575Z","iopub.status.idle":"2025-11-01T01:51:28.636444Z","shell.execute_reply.started":"2025-11-01T01:46:31.701549Z","shell.execute_reply":"2025-11-01T01:51:28.635130Z"}},"outputs":[{"name":"stdout","text":"steps/hour=6 | 1h=6 6h=36 12h=72 24h=144 48h=288\nn_features=160\ntrain_X: (47346, 160) test_X: (14610, 160) | USE_LOG_TARGET: True\nFold 1 â€” best_iter=1670, MSE(raw)=5.605664\nFold 2 â€” best_iter=2042, MSE(raw)=38.242518\nFold 3 â€” best_iter=10, MSE(raw)=13.649890\nFold 4 â€” best_iter=9, MSE(raw)=9.500141\nFold 5 â€” best_iter=256, MSE(raw)=42.655219\n\nCV MSE (OOF, raw) = 19.136668 | median best_iter = 256\nPred test (full)  mean/std: 5.373887538909912 1.3802059888839722\nPred test (cvens) mean/std: 4.224207878112793 0.6610403656959534\nâœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916   3.834093\n1          54917   3.826866\n2          54918   3.824403\n3          54919   3.828991\n4          54920   3.852460\n5          54921   3.958128\n6          54922   3.822677\n7          54923   3.820855\n8          54924   3.880584\n9          54925   4.476418\nTop 20 FI:\n                                        feature  importance\n52            Specific Conductance_roll288_mean    0.125424\n54             Specific Conductance_roll288_max    0.118699\n128                Dissolved Oxygen_roll288_min    0.044549\n98   Dissolved Oxygen (%Saturation)_roll288_min    0.033657\n155                     Chlorophyll_roll144_min    0.030351\n143                     Temperature_roll288_min    0.029045\n64             Average Water Speed_roll144_mean    0.027226\n67             Average Water Speed_roll288_mean    0.026725\n142                    Temperature_roll288_mean    0.024942\n158                     Chlorophyll_roll288_min    0.022472\n53             Specific Conductance_roll288_min    0.022315\n116                  Dissolved Oxygen_roll6_min    0.017362\n10                              Salinity_was_na    0.015903\n51             Specific Conductance_roll144_max    0.014428\n113                              pH_roll288_min    0.014402\n114                              pH_roll288_max    0.013457\n83                         Salinity_roll288_min    0.013323\n140                     Temperature_roll144_min    0.012527\n159                     Chlorophyll_roll288_max    0.012251\n125                Dissolved Oxygen_roll144_min    0.011510\nâœ… Artefak disimpan di: /kaggle/working/outputs_xgb_fixed\nðŸ“„ Files: ['xgb_full.pkl', 'features.txt', 'submission_full_model.csv', 'xgb_full.json', 'feature_importance.csv', 'oof_predictions.csv', 'metadata.json', 'submission.csv', 'submission_cv_ensemble.csv']\nReload OK. 5 preds: [3.2768 3.6876 3.7519 3.7804 3.8686]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Catbost","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, dan Library (CatBoost only)\n# =========================================================\nimport os, gc, math, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# Install CatBoost jika belum ada\ntry:\n    from catboost import CatBoostRegressor, Pool\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"catboost\"])\n    from catboost import CatBoostRegressor, Pool\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\n# ====== Paths (prioritas Kaggle) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback lokal\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Top-korelasi dari kamu\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n\n# =========================================================\n# Blok 1 â€” Load, Parse Timestamp, Sort, Sanity Checks\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\nneed_train = TOP_CORR_RAW + [TARGET, \"Timestamp\", IDCOL]\nneed_test  = TOP_CORR_RAW + [\"Timestamp\", IDCOL]\nmt = [c for c in need_train if c not in train.columns]\nms = [c for c in need_test  if c not in test.columns]\nif mt: print(\"âš ï¸ Missing train cols:\", mt)\nif ms: print(\"âš ï¸ Missing test cols :\", ms)\n\n\n\n# =========================================================\n# Blok 2 â€” Gabung & Estimasi Resolusi Waktu (steps/hour)\n# =========================================================\ntrain[\"__is_train__\"] = 1\ntest[\"__is_train__\"]  = 0\nfull = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt)==0: return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med<=0: return 1\n    return int(max(1, round(3600.0/med)))\n\nsteps_per_hour = estimate_steps_per_hour(full)\nsteps_1h  = max(1, steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_12h = max(1, 12*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\nsteps_48h = max(1, 48*steps_per_hour)\n\nprint(f\"steps/hour={steps_per_hour} | 1h={steps_1h} 6h={steps_6h} 12h={steps_12h} 24h={steps_24h} 48h={steps_48h}\")\n\n\n\n# =========================================================\n# Blok 3 â€” Flag Missingness & Imputasi Time-Aware\n# =========================================================\n# Flag missingness sebelum imputasi\nfor c in TOP_CORR_RAW:\n    full[c+\"_was_na\"] = full[c].isna().astype(int)\n\n# Sanitasi ringan\nfull[\"pH\"] = full[\"pH\"].clip(0, 14)\n\ndef impute_timeaware_past_rolling(df, cols, win):\n    out = df.copy()\n    for c in cols:\n        past_med = out[c].shift(1).rolling(window=win, min_periods=1).median()\n        out[c] = out[c].fillna(past_med)\n        out[c] = out[c].fillna(out[c].median())  # fallback global\n    return out\n\n# Imputasi dengan window ~6 jam (masa lalu saja)\nfull = impute_timeaware_past_rolling(full, TOP_CORR_RAW, win=steps_6h)\n\n\n\n# =========================================================\n# Blok 4 â€” Lag & Rolling Features (tanpa std)\n# =========================================================\ndef add_lag_features(df, cols, lags):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n    return out\n\ndef add_roll_features(df, cols, windows):\n    out = df.copy()\n    for c in cols:\n        for w in windows:\n            past = out[c].shift(1).rolling(window=w, min_periods=1)  # masa lalu saja\n            out[f\"{c}_roll{w}_mean\"] = past.mean()\n            out[f\"{c}_roll{w}_min\"]  = past.min()\n            out[f\"{c}_roll{w}_max\"]  = past.max()\n    return out\n\nLAGS  = [1, steps_1h, steps_6h]                          # 1-step, 1h, 6h\nROLLS = [steps_1h, steps_6h, steps_12h, steps_24h, steps_48h]  # 1h,6h,12h,24h,48h\n\nfull = add_lag_features(full, TOP_CORR_RAW, LAGS)\nfull = add_roll_features(full, TOP_CORR_RAW, ROLLS)\n\nlag_feats   = [c for c in full.columns if c.endswith(tuple([f\"_lag{L}\" for L in LAGS]))]\nroll_feats  = [c for c in full.columns if any(s in c for s in [\"_roll\"]) and c.endswith((\"mean\",\"min\",\"max\"))]\nmiss_flags  = [c+\"_was_na\" for c in TOP_CORR_RAW]\nFEATURES    = TOP_CORR_RAW + miss_flags + lag_feats + roll_feats\n\nprint(f\"n_features={len(FEATURES)}\")\n\n\n\n# =========================================================\n# Blok 5 â€” Split Train/Test, Log-Target, Matrix X/y\n# =========================================================\ntrain_fe = full[full[\"__is_train__\"]==1].copy()\ntest_fe  = full[full[\"__is_train__\"]==0].copy()\ntrain_fe[TARGET] = train[TARGET].values  # align target\n\n# Drop baris awal (lag/roll NaN)\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\ntrain_X = train_fe[FEATURES].copy()\ntest_X  = test_fe[FEATURES].copy()\n# Isi NaN sisa test dengan median train\ntest_X = test_X.fillna(train_X.median())\n\nUSE_LOG_TARGET = True\ny_raw = train_fe[TARGET].values\ny = np.log1p(y_raw) if USE_LOG_TARGET else y_raw\n\nprint(\"train_X:\", train_X.shape, \"test_X:\", test_X.shape, \"| USE_LOG_TARGET:\", USE_LOG_TARGET)\n\n\n\n# =========================================================\n# Blok 6 â€” TimeSeriesSplit CV (gap = 24h) & Training CatBoost\n# =========================================================\n# Gap minimal = window terpanjang (24h)\nGAP_MULTIPLIER = 1\ngap = steps_24h * GAP_MULTIPLIER\n\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\n# Param CatBoost yang stabil\ncb_params = dict(\n    loss_function=\"RMSE\",        # bekerja di ruang log (jika USE_LOG_TARGET)\n    depth=8,\n    learning_rate=0.03,\n    l2_leaf_reg=6.0,\n    random_seed=SEED,\n    iterations=6000,\n    eval_metric=\"RMSE\",\n    od_type=\"Iter\",\n    od_wait=500,                 # early stopping patience\n    random_strength=1.5,\n    rsm=0.7,                     # feature sampling (analog colsample)\n    subsample=0.8,               # stochastic gradient boosting\n    verbose=False,\n    allow_writing_files=False,\n    # task_type=\"CPU\",           # uncomment untuk force CPU; bisa \"GPU\" jika tersedia\n)\n\noof_pred_log = np.zeros(len(train_fe))\ntest_fold_preds_log = []\nbest_iters = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    tr_pool = Pool(data=X_tr, label=y_tr)\n    va_pool = Pool(data=X_va, label=y_va)\n\n    model = CatBoostRegressor(**cb_params)\n    model.fit(tr_pool, eval_set=va_pool)\n\n    # CatBoost tidak expose best_iteration langsung; ambil dari model.get_best_iteration()\n    best_it = getattr(model, \"get_best_iteration\", lambda: None)()\n    best_iters.append(int(best_it) if best_it is not None else int(cb_params[\"iterations\"]))\n\n    pred_va_log = model.predict(va_pool)\n    oof_pred_log[va_idx] = pred_va_log\n\n    te_pool = Pool(data=test_X)\n    pred_te_log = model.predict(te_pool)\n    test_fold_preds_log.append(pred_te_log)\n\n    # OOF MSE pada ruang asli\n    y_va_raw = np.expm1(y_va) if USE_LOG_TARGET else y_va\n    pred_va_raw = np.expm1(pred_va_log) if USE_LOG_TARGET else pred_va_log\n    pred_va_raw = np.clip(pred_va_raw, 0, None)\n    mse_fold = mean_squared_error(y_va_raw, pred_va_raw)\n    print(f\"Fold {fold} â€” best_iter={best_it}, MSE(raw)={mse_fold:.6f}\")\n\nbest_iter_final = int(np.median(best_iters)) if len(best_iters)>0 else int(cb_params[\"iterations\"])\n\n# CV MSE (OOF) ruang asli\noof_pred_raw = np.expm1(oof_pred_log) if USE_LOG_TARGET else oof_pred_log\noof_pred_raw = np.clip(oof_pred_raw, 0, None)\ncv_mse = mean_squared_error(train_fe[TARGET].values, oof_pred_raw)\nprint(f\"\\nCV MSE (OOF, raw) = {cv_mse:.6f} | median best_iter = {best_iter_final}\")\n\n\n\n# =========================================================\n# Blok 7 â€” Train Full Model & Prediksi Test (Full + CV-Ensemble)\n# =========================================================\nfull_pool = Pool(data=train_X, label=y)\ncb_full = CatBoostRegressor(**{**cb_params, \"iterations\": best_iter_final})\ncb_full.fit(full_pool)\n\ntest_pool = Pool(data=test_X)\npred_test_full_log  = cb_full.predict(test_pool)\npred_test_cvens_log = np.mean(np.vstack(test_fold_preds_log), axis=0) if len(test_fold_preds_log)>0 else pred_test_full_log\n\n# Inverse ke ruang asli\npred_test_full  = np.expm1(pred_test_full_log)  if USE_LOG_TARGET else pred_test_full_log\npred_test_cvens = np.expm1(pred_test_cvens_log) if USE_LOG_TARGET else pred_test_cvens_log\npred_test_full  = np.clip(pred_test_full,  0, None)\npred_test_cvens = np.clip(pred_test_cvens, 0, None)\n\nprint(\"Pred test (full)  mean/std:\", float(pred_test_full.mean()), float(pred_test_full.std()))\nprint(\"Pred test (cvens) mean/std:\", float(pred_test_cvens.mean()), float(pred_test_cvens.std()))\n\n\n\n# =========================================================\n# Blok 8 â€” Buat submission.csv (default pakai CV-ensemble)\n# =========================================================\nFINAL_PRED = pred_test_cvens  # ganti ke pred_test_full jika ingin full-model\n\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n\n# Jaga urutan sesuai sample submission\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n\n\n\n# =========================================================\n# Blok 9 â€” Simpan Artefak (model, fitur, OOF, FI, metadata)\n# =========================================================\nOUT_DIR = Path(\"./outputs_catboost\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Submission alternatif\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": pred_test_full}).to_csv(\n    OUT_DIR / \"submission_full_model.csv\", index=False\n)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": pred_test_cvens}).to_csv(\n    OUT_DIR / \"submission_cv_ensemble.csv\", index=False\n)\n\n# Simpan full model (format .cbm) & joblib dump\ncb_full.save_model(str(OUT_DIR / \"catboost_full.cbm\"))\njoblib.dump(cb_full, OUT_DIR / \"catboost_full.pkl\")\n\n# Daftar fitur (urutan penting saat inference)\npd.Series(train_X.columns).to_csv(OUT_DIR / \"features.txt\", index=False, header=False)\n\n# Simpan OOF (ruang asli)\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred_raw[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# Feature importance (PredictionValuesChange)\nfi_vals = cb_full.get_feature_importance(full_pool, type=\"PredictionValuesChange\")\nfi = pd.DataFrame({\"feature\": train_X.columns, \"importance\": fi_vals}).sort_values(\"importance\", ascending=False)\nfi.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\nprint(\"Top 20 FI:\")\nprint(fi.head(20))\n\n# Metadata\nmeta = {\n    \"cv_mse_raw\": float(cv_mse),\n    \"n_splits\": int(n_splits),\n    \"gap_steps\": int(gap),\n    \"gap_hours\": float(gap / steps_per_hour),\n    \"cb_params\": {**cb_params, \"iterations\": int(best_iter_final)},\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(train_X.columns)),\n    \"use_log_target\": bool(USE_LOG_TARGET)\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\nprint(\"âœ… Artefak disimpan di:\", OUT_DIR.resolve())\nprint(\"ðŸ“„ Files:\", [p.name for p in OUT_DIR.iterdir()])\n\n\n\n# =========================================================\n# Blok 10 â€” (Opsional) Reload & Sanity Check\n# =========================================================\n# Load .cbm kembali (via CatBoostRegressor.load_model)\ncb_reload = CatBoostRegressor()\ncb_reload.load_model(str(OUT_DIR / \"catboost_full.cbm\"))\n\nreload_pred_log = cb_reload.predict(Pool(data=test_X))\nreload_pred = np.expm1(reload_pred_log) if USE_LOG_TARGET else reload_pred_log\nreload_pred = np.clip(reload_pred, 0, None)\n\nprint(\"Reload OK. 5 preds:\", np.round(reload_pred[:5], 4))\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T01:57:03.977808Z","iopub.execute_input":"2025-11-01T01:57:03.978549Z"}},"outputs":[{"name":"stdout","text":"steps/hour=6 | 1h=6 6h=36 12h=72 24h=144 48h=288\nn_features=160\ntrain_X: (47346, 160) test_X: (14610, 160) | USE_LOG_TARGET: True\nFold 1 â€” best_iter=2315, MSE(raw)=6.277053\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, Library\n# =========================================================\nimport os, gc, math, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# LightGBM\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"lightgbm\"])\n    import lightgbm as lgb\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\n# ====== Paths (Kaggle) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback lokal\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Fitur top-korelasi yang dipakai\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n\n# =========================================================\n# Blok 1 â€” Load, Parse Timestamp, Sort, Sanity Checks\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\nneed_train = TOP_CORR_RAW + [TARGET, \"Timestamp\", IDCOL]\nneed_test  = TOP_CORR_RAW + [\"Timestamp\", IDCOL]\nmt = [c for c in need_train if c not in train.columns]\nms = [c for c in need_test  if c not in test.columns]\nif mt: print(\"âš ï¸ Missing train cols:\", mt)\nif ms: print(\"âš ï¸ Missing test cols :\", ms)\n\n\n\n# =========================================================\n# Blok 2 â€” Estimasi Resolusi Waktu (steps/hour)\n# =========================================================\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt)==0: return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med<=0: return 1\n    return int(max(1, round(3600.0/med)))\n\n# Pakai TRAIN untuk estimasi resolusi utama\nsteps_per_hour = estimate_steps_per_hour(train)\nsteps_1h  = max(1, steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_12h = max(1, 12*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\nsteps_48h = max(1, 48*steps_per_hour)\n\nprint(f\"steps/hour={steps_per_hour} | 1h={steps_1h} 6h={steps_6h} 12h={steps_12h} 24h={steps_24h} 48h={steps_48h}\")\n\n\n\n# =========================================================\n# Blok 3 â€” Flag Missingness (FULL RAW, sebelum imputasi)\n# =========================================================\n# Kita hanya buat flag di sini. Imputasi akan dilakukan per-segmen (train & test) terpisah.\ntrain[\"_seg\"] = \"train\"\ntest[\"_seg\"]  = \"test\"\nfull_raw = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\nfor c in TOP_CORR_RAW:\n    full_raw[c+\"_was_na\"] = full_raw[c].isna().astype(int)\n\n# Simpan flags per segmen\ntrain_flags = full_raw.loc[full_raw[\"_seg\"]==\"train\", [c+\"_was_na\" for c in TOP_CORR_RAW]].reset_index(drop=True)\ntest_flags  = full_raw.loc[full_raw[\"_seg\"]==\"test\",  [c+\"_was_na\" for c in TOP_CORR_RAW]].reset_index(drop=True)\n\n\n# =========================================================\n# Blok 4 â€” Imputasi TRAIN (TRAIN-only)\n# =========================================================\n# pH sanitasi\ntrain[\"pH\"] = train[\"pH\"].clip(0, 14)\n\n# Fallback median TRAIN-ONLY\ntrain_medians = {c: train[c].median(skipna=True) for c in TOP_CORR_RAW}\n\ndef impute_train_timeaware(df, cols, roll_win, ffill_limit, med_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        # 1) Past-rolling median (no future)\n        past_med = s.shift(1).rolling(window=roll_win, min_periods=1).median()\n        s = s.where(~s.isna(), past_med)\n        # 2) Bounded forward-fill (â‰¤ 6 jam)\n        s = s.ffill(limit=ffill_limit)\n        # 3) Fallback median TRAIN-ONLY\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))\n        out[c] = s\n    return out\n\ntrain_imp = impute_train_timeaware(\n    df=train,\n    cols=TOP_CORR_RAW,\n    roll_win=steps_6h,\n    ffill_limit=steps_6h,\n    med_map=train_medians\n)\n\n# Last-known (hasil imputasi) per kolom pada AKHIR TRAIN â€” dipakai untuk â€œseedâ€ test\nlast_known_from_train = {}\nfor c in TOP_CORR_RAW:\n    last_val = train_imp[c].iloc[-1]\n    if pd.isna(last_val):\n        last_val = train_medians[c]\n    last_known_from_train[c] = last_val\n\n\n\n# =========================================================\n# Blok 5 â€” Imputasi TEST (tanpa statistik dari test)\n# =========================================================\n# Catatan:\n# - Tidak menghitung rolling median dari test.\n# - Jika awal test kosong, seed dengan last-known dari train (maks ffill 6 jam).\n# - Sisanya ffill (â‰¤ 6 jam) + fallback median TRAIN-ONLY.\n\ntest[\"pH\"] = test[\"pH\"].clip(0, 14)\n\ndef impute_test_seeded(df, cols, ffill_limit, med_map, seed_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n\n        # Seed awal dengan last-known dari train jika nilai awal NA\n        if len(s)>0 and pd.isna(s.iloc[0]) and pd.notna(seed_map.get(c, np.nan)):\n            s.iloc[0] = seed_map[c]\n\n        # Bounded forward-fill di dalam test (â‰¤ 6 jam)\n        s = s.ffill(limit=ffill_limit)\n\n        # Fallback median TRAIN-ONLY\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))\n\n        out[c] = s\n    return out\n\ntest_imp = impute_test_seeded(\n    df=test,\n    cols=TOP_CORR_RAW,\n    ffill_limit=steps_6h,\n    med_map=train_medians,\n    seed_map=last_known_from_train\n)\n\n\n\n# =========================================================\n# Blok 6 â€” Gabungkan hasil imputasi, lalu Buat Fitur Lag/Rolling (anti-leakage)\n# =========================================================\n# Rekatkan kembali hasil imputasi (untuk fitur turunan)\ntrain_imp[\"_seg\"] = \"train\"\ntest_imp[\"_seg\"]  = \"test\"\nfull_imp = pd.concat([train_imp, test_imp], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\n# Tambah flags yang sudah dibuat dari full_raw (agar baris/urutan cocok)\nflag_cols = [c+\"_was_na\" for c in TOP_CORR_RAW]\nfull_flags = pd.concat([train_flags, test_flags], axis=0, ignore_index=True)\nfor i, col in enumerate(flag_cols):\n    full_imp[col] = full_flags[col].values\n\ndef add_lag_features(df, cols, lags):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n    return out\n\ndef add_roll_features(df, cols, windows):\n    out = df.copy()\n    for c in cols:\n        for w in windows:\n            past = out[c].shift(1).rolling(window=w, min_periods=1)  # masa lalu saja\n            out[f\"{c}_roll{w}_mean\"] = past.mean()\n            out[f\"{c}_roll{w}_min\"]  = past.min()\n            out[f\"{c}_roll{w}_max\"]  = past.max()\n    return out\n\nLAGS  = [1, steps_1h, steps_6h]                          # 1-step, 1h, 6h\nROLLS = [steps_1h, steps_6h, steps_12h, steps_24h, steps_48h]  # 1h,6h,12h,24h,48h\n\nfull_imp = add_lag_features(full_imp, TOP_CORR_RAW, LAGS)\nfull_imp = add_roll_features(full_imp, TOP_CORR_RAW, ROLLS)\n\nlag_feats   = [c for c in full_imp.columns if \"_lag\" in c]\nroll_feats  = [c for c in full_imp.columns if \"_roll\" in c and c.endswith((\"mean\",\"min\",\"max\"))]\nmiss_flags  = [c+\"_was_na\" for c in TOP_CORR_RAW]\nFEATURES    = TOP_CORR_RAW + miss_flags + lag_feats + roll_feats\n\nprint(f\"n_features={len(FEATURES)}\")\n\n\n\n# =========================================================\n# Blok 7 â€” Split Train/Test Matrices (drop NaN awal lag/roll)\n# =========================================================\ntrain_fe = full_imp[full_imp[\"_seg\"]==\"train\"].copy()\ntest_fe  = full_imp[full_imp[\"_seg\"]==\"test\"].copy()\n\n# Align target ke urutan train_fe\ntrain_fe[TARGET] = train[TARGET].values\n\n# Drop baris awal train yang masih NaN (efek lag/rolling)\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\ntrain_X = train_fe[FEATURES].copy()\ny       = train_fe[TARGET].values\ntest_X  = test_fe[FEATURES].copy()\n\n# Isi NaN sisa test dengan median TRAIN (aman)\ntest_X = test_X.fillna(train_X.median())\n\nprint(\"train_X:\", train_X.shape, \"test_X:\", test_X.shape)\n\n\n\n\n# =========================================================\n# Blok 8 â€” TimeSeriesSplit CV (gap = 24h) & LGBM Training\n# =========================================================\n# Gap minimal = window terpanjang (24h)\nGAP_MULTIPLIER = 1\ngap = steps_24h * GAP_MULTIPLIER\n\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\nlgb_params = dict(\n    objective=\"l2\",            # MSE\n    learning_rate=0.05,\n    num_leaves=64,\n    feature_fraction=0.9,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    min_data_in_leaf=60,\n    lambda_l2=2.0,             # regulasi halus\n    n_estimators=6000,         # besar; early_stopping memangkas\n    random_state=SEED,\n    verbose=-1\n)\n\noof_pred = np.zeros(len(train_fe))\ntest_fold_preds = []\nbest_iters = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=500, verbose=False)]\n    )\n\n    best_it = model.best_iteration_\n    best_iters.append(int(best_it) if best_it is not None else int(lgb_params[\"n_estimators\"]))\n\n    pred_va = model.predict(X_va, num_iteration=best_it)\n    oof_pred[va_idx] = np.clip(pred_va, 0, None)\n\n    pred_te = model.predict(test_X, num_iteration=best_it)\n    pred_te = np.clip(pred_te, 0, None)\n    test_fold_preds.append(pred_te)\n\n    mse_fold = mean_squared_error(y[va_idx], oof_pred[va_idx])\n    print(f\"Fold {fold} â€” best_iter={best_it}, MSE={mse_fold:.6f}\")\n\nbest_iter_final = int(np.median(best_iters)) if len(best_iters)>0 else int(lgb_params[\"n_estimators\"])\ncv_mse = mean_squared_error(y, oof_pred)\nprint(f\"\\nCV MSE (OOF, raw) = {cv_mse:.6f} | median best_iter = {best_iter_final}\")\n\n\n\n# =========================================================\n# Blok 9 â€” Train Full Model & Prediksi Test (Full + CV-Ensemble)\n# =========================================================\nlgb_full = lgb.LGBMRegressor(**{**lgb_params, \"n_estimators\": best_iter_final})\nlgb_full.fit(train_X, y)\n\npred_test_full  = np.clip(lgb_full.predict(test_X), 0, None)\npred_test_cvens = np.clip(np.mean(np.vstack(test_fold_preds), axis=0), 0, None) if len(test_fold_preds)>0 else pred_test_full\n\nprint(\"Pred test (full)  mean/std:\", float(pred_test_full.mean()),  float(pred_test_full.std()))\nprint(\"Pred test (cvens) mean/std:\", float(pred_test_cvens.mean()), float(pred_test_cvens.std()))\n\n\n\n# =========================================================\n# Blok 10 â€” Submission (default: CV-ensemble)\n# =========================================================\nFINAL_PRED = pred_test_cvens  # ganti ke pred_test_full jika ingin full-model\n\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n\n# Jaga urutan sesuai sample\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n\n\n\n# =========================================================\n# Blok 11 â€” Artefak (model, fitur, OOF, FI, metadata)\n# =========================================================\nOUT_DIR = Path(\"./outputs_lgbm_trainonly_impute\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Submission alternatif\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": pred_test_full}).to_csv(\n    OUT_DIR / \"submission_full_model.csv\", index=False\n)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": FINAL_PRED}).to_csv(\n    OUT_DIR / \"submission_cv_ensemble.csv\", index=False\n)\n\n# Simpan model (booster + sklearn wrapper)\nlgb_full.booster_.save_model(str(OUT_DIR / \"lgbm_full.txt\"))\njoblib.dump(lgb_full, OUT_DIR / \"lgbm_full.pkl\")\n\n# Fitur (urutan penting untuk inference)\npd.Series(train_X.columns).to_csv(OUT_DIR / \"features.txt\", index=False, header=False)\n\n# OOF (ruang asli)\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# Feature importance (gain & split)\nfi = pd.DataFrame({\n    \"feature\": train_X.columns,\n    \"gain\":   lgb_full.booster_.feature_importance(importance_type=\"gain\"),\n    \"split\":  lgb_full.booster_.feature_importance(importance_type=\"split\"),\n}).sort_values(\"gain\", ascending=False)\nfi.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\nprint(\"Top 20 FI:\")\nprint(fi.head(20))\n\n# Metadata\nmeta = {\n    \"cv_mse_raw\": float(cv_mse),\n    \"n_splits\": int(n_splits),\n    \"gap_steps\": int(gap),\n    \"gap_hours\": float(gap / steps_per_hour),\n    \"lgb_params\": {**lgb_params, \"n_estimators\": int(best_iter_final)},\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(train_X.columns)),\n    \"impute_policy\": {\n        \"train\": \"past_rolling_median -> ffill<=6h -> fallback median TRAIN-only\",\n        \"test\":  \"seed last-known(train) -> ffill<=6h -> fallback median TRAIN-only\",\n    }\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\nprint(\"âœ… Artefak disimpan di:\", OUT_DIR.resolve())\nprint(\"ðŸ“„ Files:\", [p.name for p in OUT_DIR.iterdir()])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T02:58:59.992653Z","iopub.execute_input":"2025-11-01T02:58:59.993405Z","iopub.status.idle":"2025-11-01T03:02:02.967890Z","shell.execute_reply.started":"2025-11-01T02:58:59.993382Z","shell.execute_reply":"2025-11-01T03:02:02.966946Z"}},"outputs":[{"name":"stdout","text":"steps/hour=6 | 1h=6 6h=36 12h=72 24h=144 48h=288\nn_features=160\ntrain_X: (47346, 160) test_X: (14610, 160)\nFold 1 â€” best_iter=2368, MSE=4.384105\nFold 2 â€” best_iter=3492, MSE=34.831326\nFold 3 â€” best_iter=2, MSE=12.900845\nFold 4 â€” best_iter=2, MSE=8.374097\nFold 5 â€” best_iter=174, MSE=38.176619\n\nCV MSE (OOF, raw) = 17.305595 | median best_iter = 174\nPred test (full)  mean/std: 8.049893272593625 2.790662104118727\nPred test (cvens) mean/std: 5.83386998373311 0.7640241522932113\nâœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916   6.451931\n1          54917   6.805037\n2          54918   7.151798\n3          54919   6.930721\n4          54920   7.270678\n5          54921   7.220288\n6          54922   7.023513\n7          54923   7.027749\n8          54924   7.091690\n9          54925   7.596410\nTop 20 FI:\n                                         feature           gain  split\n158                      Chlorophyll_roll288_min  779707.415421    127\n97   Dissolved Oxygen (%Saturation)_roll288_mean  663458.393333    121\n114                               pH_roll288_max  428573.101191     71\n128                 Dissolved Oxygen_roll288_min  283435.237446    138\n67              Average Water Speed_roll288_mean  268990.346083    171\n98    Dissolved Oxygen (%Saturation)_roll288_min  233608.335258     85\n113                               pH_roll288_min  198745.439547     87\n143                      Temperature_roll288_min  178004.961355    116\n111                               pH_roll144_max  162553.885361     57\n155                      Chlorophyll_roll144_min  159049.613657    102\n95    Dissolved Oxygen (%Saturation)_roll144_min  131647.839102     79\n64              Average Water Speed_roll144_mean  125890.357191    126\n20                      Average Water Speed_lag6  105489.970131    211\n53              Specific Conductance_roll288_min  103649.054565     84\n48               Specific Conductance_roll72_max  100191.480999     42\n112                              pH_roll288_mean   91540.629465    106\n61               Average Water Speed_roll72_mean   73185.064804    117\n58               Average Water Speed_roll36_mean   72414.991777    202\n140                      Temperature_roll144_min   71454.195879     49\n157                     Chlorophyll_roll288_mean   70425.522118    128\nâœ… Artefak disimpan di: /kaggle/working/outputs_lgbm_trainonly_impute\nðŸ“„ Files: ['features.txt', 'submission_full_model.csv', 'feature_importance.csv', 'oof_predictions.csv', 'metadata.json', 'lgbm_full.txt', 'lgbm_full.pkl', 'submission.csv', 'submission_cv_ensemble.csv']\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# =========================================================\n# Blok 12 â€” Dokumentasi (Kontrol MSE & Catatan Rekayasa Fitur)\n# =========================================================\nimport io\n\n# --- 12.1 Tabel Kontrol MSE dari teks yang kamu berikan ---\ncontrols_csv = \"\"\"Area,Teknik Pengendalian,Keterangan\nPenanganan Outlier,\"Hapus, Transformasi Logaritma, atau Capping.\",\"Kunci penting untuk MSE, karena outlier menghasilkan kesalahan besar yang sangat memengaruhi nilai MSE.\"\nPenanganan Missing Values,\"Imputasi (dengan rata-rata/median), Imputasi Deret Waktu (Interpolasi).\",\"Data yang hilang harus ditangani agar tidak mengganggu pola dalam deret waktu.\"\nNormalisasi/Standardisasi,\"Min-Max Scaling atau Z-score Standardization.\",\"Memastikan semua fitur berkontribusi secara proporsional, mencegah fitur dengan skala besar mendominasi proses pelatihan.\"\n\"\"\"\ncontrols_df = pd.read_csv(io.StringIO(controls_csv))\ncontrols_path = OUT_DIR / \"mse_controls.csv\"\ncontrols_df.to_csv(controls_path, index=False)\n\n# --- 12.2 Catatan modeling (Markdown) dari poin-poin tambahan ---\nnotes_md = f\"\"\"# Catatan Modeling & MSE â€” LGBM Turbidity\n\n## Kontrol MSE (ringkas)\nLihat **mse_controls.csv** untuk tabel pengendalian (outlier, missing values, normalisasi/standardisasi).\n\n## Rekayasa Fitur (rangkuman)\n- **Pembuatan Fitur Lag**: Untuk deret waktu, membuat fitur dari nilai historis **variabel independen** (Suhu/Temperature, DO, pH, dll.) pada langkah waktu sebelumnya (t-1, t-6, dst.) sangat penting.  \n  **Catatan:** *Lag target* `Turbidity` **tidak** digunakan demi anti-leakage pada skenario one-shot inference (nilai target historis pada data uji tidak tersedia saat prediksi).\n- **Fitur Temporal**: Dapat mengekstrak sinyal waktu (hour-of-day, day-of-week, month/season) dari kolom `Timestamp` untuk menangkap pola musiman/siklus. (Belum diaktifkan di notebook ini â€” bisa ditambahkan sebagai opsi lanjutan.)\n- **Fitur Interaksi**: Opsional, misal kombinasi `Average Water Speed Ã— Salinity` atau `Temperature Ã— DO`, jika secara domain dianggap relevan.\n\n## Regularisasi\n- Model linear: gunakan **L1/L2**; Deep Learning: gunakan **Dropout**.\n- Pada **LightGBM**, regularisasi dan kontrol kompleksitas diterapkan via:\n  - `lambda_l2={lgb_params.get('lambda_l2', None)}`, \n  - `num_leaves={lgb_params.get('num_leaves', None)}`, \n  - `min_data_in_leaf={lgb_params.get('min_data_in_leaf', None)}`, \n  - `feature_fraction={lgb_params.get('feature_fraction', None)}`, \n  - `bagging_fraction={lgb_params.get('bagging_fraction', None)}`.\n\n## Mengapa MSE â€œkerasâ€ terhadap outlier?\n- MSE mengkuadratkan galat, sehingga **kesalahan besar (outlier)** mendapat penalti jauh lebih besar.  \n  Pengendalian dilakukan lewat: **capping/log-transform** (bila relevan), desain **imputasi median berbasis masa lalu** (meredam lonjakan sesaat), dan validasi time-series yang ketat (gap â‰¥ jendela terpanjang).\n\"\"\"\nnotes_path = OUT_DIR / \"modeling_notes.md\"\nwith open(notes_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(notes_md)\n\n# --- 12.3 Update metadata agar mereferensikan dokumen ini ---\ntry:\n    # Baca metadata yang sudah dibuat di Blok 11, lalu perbarui\n    meta_path = OUT_DIR / \"metadata.json\"\n    if meta_path.exists():\n        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n            meta_loaded = json.load(f)\n    else:\n        meta_loaded = {}\n    meta_loaded.setdefault(\"docs\", {})\n    meta_loaded[\"docs\"].update({\n        \"mse_controls_csv\": str(controls_path.name),\n        \"modeling_notes_md\": str(notes_path.name),\n    })\n    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(meta_loaded, f, indent=2, ensure_ascii=False)\nexcept Exception as e:\n    print(\"âš ï¸ Gagal mengupdate metadata dengan dokumen:\", e)\n\nprint(\"ðŸ“„ Dokumen tersimpan:\", controls_path.name, \"dan\", notes_path.name)\ndisplay_cols = list(controls_df.columns)\nprint(controls_df[display_cols])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:07:24.033220Z","iopub.execute_input":"2025-11-01T03:07:24.033855Z","iopub.status.idle":"2025-11-01T03:07:24.048514Z","shell.execute_reply.started":"2025-11-01T03:07:24.033834Z","shell.execute_reply":"2025-11-01T03:07:24.047630Z"}},"outputs":[{"name":"stdout","text":"ðŸ“„ Dokumen tersimpan: mse_controls.csv dan modeling_notes.md\n                        Area  \\\n0         Penanganan Outlier   \n1  Penanganan Missing Values   \n2  Normalisasi/Standardisasi   \n\n                                 Teknik Pengendalian  \\\n0       Hapus, Transformasi Logaritma, atau Capping.   \n1  Imputasi (dengan rata-rata/median), Imputasi D...   \n2      Min-Max Scaling atau Z-score Standardization.   \n\n                                          Keterangan  \n0  Kunci penting untuk MSE, karena outlier mengha...  \n1  Data yang hilang harus ditangani agar tidak me...  \n2  Memastikan semua fitur berkontribusi secara pr...  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# mmm","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, Library\n# =========================================================\nimport os, gc, math, json, warnings, io\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# LightGBM\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"lightgbm\"])\n    import lightgbm as lgb\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\n# ====== Paths (Kaggle) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback lokal\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Fitur top-korelasi yang dipakai\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n# === Opsi peningkatan ===\nUSE_LOG_TARGET   = True     # latih di log1p(Turbidity), inverse saat evaluasi & submit\nUSE_TARGET_LAGS  = False    # OFF (one-shot inference); ON butuh inferensi autoregresif\nGAP_MULTIPLIER   = 1        # 1 â†’ 24h; 2 â†’ 48h\nQ_LOW, Q_HIGH    = 0.005, 0.995  # capping quantiles (train-only)\n\n\n# =========================================================\n# Blok 1 â€” Load, Parse Timestamp, Sort, Sanity Checks\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\nneed_train = TOP_CORR_RAW + [TARGET, \"Timestamp\", IDCOL]\nneed_test  = TOP_CORR_RAW + [\"Timestamp\", IDCOL]\nmt = [c for c in need_train if c not in train.columns]\nms = [c for c in need_test  if c not in test.columns]\nif mt: print(\"âš ï¸ Missing train cols:\", mt)\nif ms: print(\"âš ï¸ Missing test cols :\", ms)\n\n\n# =========================================================\n# Blok 2b â€” Outlier Capping (train-only bounds)\n# =========================================================\ndef compute_caps(ref_df, cols, q_low, q_high):\n    bounds = {}\n    for c in cols:\n        lo, hi = ref_df[c].quantile([q_low, q_high])\n        if not np.isfinite(lo): lo = ref_df[c].min()\n        if not np.isfinite(hi): hi = ref_df[c].max()\n        bounds[c] = (lo, hi)\n    return bounds\n\ndef apply_caps(df, bounds):\n    out = df.copy()\n    for c, (lo, hi) in bounds.items():\n        if c in out.columns:\n            out[c] = out[c].clip(lo, hi)\n    return out\n\n# Hitung batas dari TRAIN, terapkan ke TRAIN & TEST\ncaps = compute_caps(train, TOP_CORR_RAW, Q_LOW, Q_HIGH)\ntrain = apply_caps(train, caps)\ntest  = apply_caps(test, caps)\n\n\n# =========================================================\n# Blok 3 â€” Flag Missingness (FULL RAW, sebelum imputasi)\n# =========================================================\ntrain[\"_seg\"] = \"train\"\ntest[\"_seg\"]  = \"test\"\nfull_raw = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\nfor c in TOP_CORR_RAW:\n    full_raw[c+\"_was_na\"] = full_raw[c].isna().astype(int)\n\ntrain_flags = full_raw.loc[full_raw[\"_seg\"]==\"train\", [c+\"_was_na\" for c in TOP_CORR_RAW]].reset_index(drop=True)\ntest_flags  = full_raw.loc[full_raw[\"_seg\"]==\"test\",  [c+\"_was_na\" for c in TOP_CORR_RAW]].reset_index(drop=True)\n\n\n\n# =========================================================\n# Blok 4 â€” Imputasi TRAIN (TRAIN-only)\n# =========================================================\ntrain[\"pH\"] = train[\"pH\"].clip(0, 14)\ntrain_medians = {c: train[c].median(skipna=True) for c in TOP_CORR_RAW}\n\ndef impute_train_timeaware(df, cols, roll_win, ffill_limit, med_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        past_med = s.shift(1).rolling(window=roll_win, min_periods=1).median()  # masa lalu\n        s = s.where(~s.isna(), past_med)\n        s = s.ffill(limit=ffill_limit)  # â‰¤ 6 jam\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))  # fallback train-only\n        out[c] = s\n    return out\n\ntrain_imp = impute_train_timeaware(\n    df=train, cols=TOP_CORR_RAW, roll_win=steps_6h, ffill_limit=steps_6h, med_map=train_medians\n)\n\nlast_known_from_train = {}\nfor c in TOP_CORR_RAW:\n    last_val = train_imp[c].iloc[-1]\n    if pd.isna(last_val):\n        last_val = train_medians[c]\n    last_known_from_train[c] = last_val\n\n\n# =========================================================\n# Blok 5 â€” Imputasi TEST (seed dari TRAIN, tanpa statistik test)\n# =========================================================\ntest[\"pH\"] = test[\"pH\"].clip(0, 14)\n\ndef impute_test_seeded(df, cols, ffill_limit, med_map, seed_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        if len(s)>0 and pd.isna(s.iloc[0]) and pd.notna(seed_map.get(c, np.nan)):\n            s.iloc[0] = seed_map[c]\n        s = s.ffill(limit=ffill_limit)\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))\n        out[c] = s\n    return out\n\ntest_imp = impute_test_seeded(\n    df=test, cols=TOP_CORR_RAW, ffill_limit=steps_6h, med_map=train_medians, seed_map=last_known_from_train\n)\n\n\n\n\n# =========================================================\n# Blok 6 â€” Gabungkan, Tambahkan Flags, Fitur Temporal & Interaksi\n# =========================================================\ntrain_imp[\"_seg\"] = \"train\"\ntest_imp[\"_seg\"]  = \"test\"\nfull_imp = pd.concat([train_imp, test_imp], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\n# flags\nflag_cols = [c+\"_was_na\" for c in TOP_CORR_RAW]\nfull_flags = pd.concat([train_flags, test_flags], axis=0, ignore_index=True)\nfor col in flag_cols:\n    full_imp[col] = full_flags[col].values\n\n# Fitur temporal\ndef add_time_features(df):\n    out = df.copy()\n    out[\"hour\"]  = out[\"Timestamp\"].dt.hour\n    out[\"dow\"]   = out[\"Timestamp\"].dt.dayofweek\n    out[\"month\"] = out[\"Timestamp\"].dt.month\n    # sin/cos\n    out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24.0)\n    out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24.0)\n    out[\"dow_sin\"]  = np.sin(2*np.pi*out[\"dow\"]/7.0)\n    out[\"dow_cos\"]  = np.cos(2*np.pi*out[\"dow\"]/7.0)\n    return out\n\nfull_imp = add_time_features(full_imp)\nTIME_FEATS = [\"hour\",\"dow\",\"month\",\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\"]\n\n# Fitur interaksi (domain)\ndef add_interactions(df):\n    out = df.copy()\n    out[\"speed_x_sal\"] = out[\"Average Water Speed\"] * out[\"Salinity\"]\n    out[\"temp_x_do\"]   = out[\"Temperature\"] * out[\"Dissolved Oxygen\"]\n    out[\"ph_x_cond\"]   = out[\"pH\"] * out[\"Specific Conductance\"]\n    out[\"cond_x_sal\"]  = out[\"Specific Conductance\"] * out[\"Salinity\"]\n    return out\n\nfull_imp = add_interactions(full_imp)\nINTER_FEATS = [\"speed_x_sal\",\"temp_x_do\",\"ph_x_cond\",\"cond_x_sal\"]\n\n\n\n# =========================================================\n# Blok 6b â€” Lag & Rolling (sensor), Z-Score features\n# =========================================================\ndef add_lag_features(df, cols, lags):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n    return out\n\ndef add_roll_features(df, cols, windows):\n    out = df.copy()\n    for c in cols:\n        for w in windows:\n            past = out[c].shift(1).rolling(window=w, min_periods=1)  # masa lalu\n            out[f\"{c}_roll{w}_mean\"] = past.mean()\n            out[f\"{c}_roll{w}_min\"]  = past.min()\n            out[f\"{c}_roll{w}_max\"]  = past.max()\n    return out\n\nLAGS  = [1, steps_1h, steps_6h]\nROLLS = [steps_1h, steps_6h, steps_12h, steps_24h, steps_48h]\n\nfull_imp = add_lag_features(full_imp, TOP_CORR_RAW, LAGS)\nfull_imp = add_roll_features(full_imp, TOP_CORR_RAW, ROLLS)\n\nlag_feats   = [c for c in full_imp.columns if \"_lag\" in c]\nroll_feats  = [c for c in full_imp.columns if \"_roll\" in c and c.endswith((\"mean\",\"min\",\"max\"))]\nmiss_flags  = [c+\"_was_na\" for c in TOP_CORR_RAW]\n\n# Z-score untuk 8 sensor (train-only mean/std)\ntrain_mask = full_imp[\"_seg\"]==\"train\"\nz_means = {c: full_imp.loc[train_mask, c].mean() for c in TOP_CORR_RAW}\nz_stds  = {c: full_imp.loc[train_mask, c].std(ddof=0) for c in TOP_CORR_RAW}\n\nfor c in TOP_CORR_RAW:\n    m, s = z_means[c], z_stds[c]\n    full_imp[c+\"_z\"] = (full_imp[c]-m) / (s if (s and s!=0) else 1.0)\n\nZ_FEATS = [c+\"_z\" for c in TOP_CORR_RAW]\n\n\n# =========================================================\n# Blok 6c â€” (Opsional) Lag Target (OFF by default, AR inference needed)\n# =========================================================\nTARGET_LAGS = []\nif USE_TARGET_LAGS:\n    # Buat lag target untuk TRAIN saja (nanti inference test perlu AR loop)\n    for L in [1, steps_1h, steps_6h]:\n        full_imp[f\"{TARGET}_lag{L}\"] = np.nan\n    # Isi untuk segmen train dengan Turbidity historis\n    train_idx = full_imp[\"_seg\"]==\"train\"\n    for L in [1, steps_1h, steps_6h]:\n        full_imp.loc[train_idx, f\"{TARGET}_lag{L}\"] = train[TARGET].shift(L).values\n    TARGET_LAGS = [f\"{TARGET}_lag{L}\" for L in [1, steps_1h, steps_6h]]\n\n\n\n\n# =========================================================\n# Blok 7 â€” Split Train/Test Matrices (drop NaN awal lag/roll)\n# =========================================================\ntrain_fe = full_imp[full_imp[\"_seg\"]==\"train\"].copy()\ntest_fe  = full_imp[full_imp[\"_seg\"]==\"test\"].copy()\n\n# Align target\ntrain_fe[TARGET] = train[TARGET].values\n\n# Buang baris awal train yang masih NaN (efek lag/rolling/target_lag)\nfeature_blocks = TOP_CORR_RAW + miss_flags + TIME_FEATS + INTER_FEATS + Z_FEATS + lag_feats + roll_feats + TARGET_LAGS\nFEATURES = feature_blocks.copy()\n\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\n# Log target (opsional)\ny_raw = train_fe[TARGET].values\ny = np.log1p(y_raw) if USE_LOG_TARGET else y_raw\n\ntrain_X = train_fe[FEATURES].copy()\ntest_X  = test_fe[FEATURES].copy()\n\n# Isi NaN sisa test dengan median TRAIN (aman)\ntest_X = test_X.fillna(train_X.median())\n\nprint(\"n_features:\", len(FEATURES))\nprint(\"train_X:\", train_X.shape, \"test_X:\", test_X.shape, \"| USE_LOG_TARGET:\", USE_LOG_TARGET, \"| USE_TARGET_LAGS:\", USE_TARGET_LAGS)\n\n\n\n# =========================================================\n# Blok 8 â€” TimeSeriesSplit CV (gap = 24h) & LGBM Training (regularized)\n# =========================================================\ngap = steps_24h * GAP_MULTIPLIER\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\nlgb_params = dict(\n    objective=\"l2\",            # MSE (di ruang log jika USE_LOG_TARGET=True)\n    learning_rate=0.045,\n    num_leaves=96,\n    max_depth=-1,\n    min_data_in_leaf=80,\n    feature_fraction=0.9,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    reg_alpha=0.2,\n    reg_lambda=4.0,\n    n_estimators=8000,         # besar; early_stopping memangkas\n    random_state=SEED,\n    verbose=-1\n)\n\noof_pred_log = np.zeros(len(train_fe))\ntest_fold_preds_log = []\nbest_iters = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=600, verbose=False)]\n    )\n\n    best_it = model.best_iteration_\n    best_iters.append(int(best_it) if best_it is not None else int(lgb_params[\"n_estimators\"]))\n\n    pred_va_log = model.predict(X_va, num_iteration=best_it)\n    oof_pred_log[va_idx] = pred_va_log\n\n    pred_te_log = model.predict(test_X, num_iteration=best_it)\n    test_fold_preds_log.append(pred_te_log)\n\n    # MSE OOF ruang asli\n    y_va_raw = np.expm1(y_va) if USE_LOG_TARGET else y_va\n    pred_va_raw = np.expm1(pred_va_log) if USE_LOG_TARGET else pred_va_log\n    pred_va_raw = np.clip(pred_va_raw, 0, None)\n    mse_fold = mean_squared_error(y_va_raw, pred_va_raw)\n    print(f\"Fold {fold} â€” best_iter={best_it}, MSE(raw)={mse_fold:.6f}\")\n\nbest_iter_final = int(np.median(best_iters)) if len(best_iters)>0 else int(lgb_params[\"n_estimators\"])\n\n# CV MSE (OOF) ruang asli\noof_pred_raw = np.expm1(oof_pred_log) if USE_LOG_TARGET else oof_pred_log\noof_pred_raw = np.clip(oof_pred_raw, 0, None)\ncv_mse = mean_squared_error(train_fe[TARGET].values, oof_pred_raw)\nprint(f\"\\nCV MSE (OOF, raw) = {cv_mse:.6f} | median best_iter = {best_iter_final}\")\n\n# =========================================================\n# Blok 9 â€” Train Full Model & Prediksi Test (Full + CV-Ensemble)\n# =========================================================\nlgb_full = lgb.LGBMRegressor(**{**lgb_params, \"n_estimators\": best_iter_final})\nlgb_full.fit(train_X, y)\n\npred_test_full_log  = lgb_full.predict(test_X)\npred_test_cvens_log = np.mean(np.vstack(test_fold_preds_log), axis=0) if len(test_fold_preds_log)>0 else pred_test_full_log\n\n# Inverse log & clip\npred_test_full  = np.expm1(pred_test_full_log)  if USE_LOG_TARGET else pred_test_full_log\npred_test_cvens = np.expm1(pred_test_cvens_log) if USE_LOG_TARGET else pred_test_cvens_log\npred_test_full  = np.clip(pred_test_full,  0, None)\npred_test_cvens = np.clip(pred_test_cvens, 0, None)\n\nprint(\"Pred test (full)  mean/std:\", float(pred_test_full.mean()),  float(pred_test_full.std()))\nprint(\"Pred test (cvens) mean/std:\", float(pred_test_cvens.mean()), float(pred_test_cvens.std()))\n\n\n\n# =========================================================\n# Blok 10 â€” Submission (default: CV-ensemble)\n# =========================================================\nFINAL_PRED = pred_test_cvens  # ganti ke pred_test_full bila ingin full-model\n\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n\n\n\n# =========================================================\n# Blok 11 â€” Artefak (model, fitur, OOF, FI, metadata)\n# =========================================================\nOUT_DIR = Path(\"./outputs_lgbm_maximized\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Submission alternatif\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": pred_test_full}).to_csv(\n    OUT_DIR / \"submission_full_model.csv\", index=False\n)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": FINAL_PRED}).to_csv(\n    OUT_DIR / \"submission_cv_ensemble.csv\", index=False\n)\n\n# Simpan model\nlgb_full.booster_.save_model(str(OUT_DIR / \"lgbm_full.txt\"))\njoblib.dump(lgb_full, OUT_DIR / \"lgbm_full.pkl\")\n\n# Fitur\npd.Series(train_X.columns).to_csv(OUT_DIR / \"features.txt\", index=False, header=False)\n\n# OOF\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred_raw[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# Feature importance\nfi = pd.DataFrame({\n    \"feature\": train_X.columns,\n    \"gain\":   lgb_full.booster_.feature_importance(importance_type=\"gain\"),\n    \"split\":  lgb_full.booster_.feature_importance(importance_type=\"split\"),\n}).sort_values(\"gain\", ascending=False)\nfi.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\nprint(\"Top 20 FI:\")\nprint(fi.head(20))\n\n# Metadata + caps bounds\nmeta = {\n    \"cv_mse_raw\": float(cv_mse),\n    \"n_splits\": int(n_splits),\n    \"gap_steps\": int(gap),\n    \"gap_hours\": float(gap / steps_per_hour),\n    \"lgb_params\": {**lgb_params, \"n_estimators\": int(best_iter_final)},\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(train_X.columns)),\n    \"use_log_target\": bool(USE_LOG_TARGET),\n    \"use_target_lags\": bool(USE_TARGET_LAGS),\n    \"caps_quantiles\": [Q_LOW, Q_HIGH],\n    \"caps_bounds\": {k: [float(v[0]), float(v[1])] for k, v in caps.items()},\n    \"impute_policy\": {\n        \"train\": \"past_rolling_median -> ffill<=6h -> fallback median TRAIN-only\",\n        \"test\":  \"seed last-known(train) -> ffill<=6h -> fallback median TRAIN-only\",\n    }\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\n\n# =========================================================\n# Blok 12 â€” Dokumentasi (Kontrol MSE & Catatan Modeling)\n# =========================================================\ncontrols_csv = \"\"\"Area,Teknik Pengendalian,Keterangan\nPenanganan Outlier,\"Hapus, Transformasi Logaritma, atau Capping.\",\"Kunci penting untuk MSE, karena outlier menghasilkan kesalahan besar yang sangat memengaruhi nilai MSE.\"\nPenanganan Missing Values,\"Imputasi (dengan rata-rata/median), Imputasi Deret Waktu (Interpolasi).\",\"Data yang hilang harus ditangani agar tidak mengganggu pola dalam deret waktu.\"\nNormalisasi/Standardisasi,\"Min-Max Scaling atau Z-score Standardization.\",\"Memastikan semua fitur berkontribusi secara proporsional, mencegah fitur dengan skala besar mendominasi proses pelatihan.\"\n\"\"\"\ncontrols_df = pd.read_csv(io.StringIO(controls_csv))\ncontrols_path = OUT_DIR / \"mse_controls.csv\"\ncontrols_df.to_csv(controls_path, index=False)\n\nnotes_md = f\"\"\"# Catatan Modeling â€” LGBM Turbidity (versi maksimal)\n\n## Rekap Penerapan\n- **Outlier capping (train-only)**: quantile [{Q_LOW:.3f}, {Q_HIGH:.3f}] â†’ diterapkan ke train & test.\n- **Imputasi hati-hati**: rolling-median masa lalu â†’ ffillâ‰¤6h â†’ fallback median train-only.\n- **Normalisasi/Standardisasi**: fitur **z-score** untuk 8 sensor (train mean/std).\n- **Fitur Temporal**: hour/dow/month + sin/cos.\n- **Interaksi**: speedÃ—salinity, tempÃ—DO, pHÃ—conductance, conductanceÃ—salinity.\n- **Lag/Roll (sensor)**: 1-step, 1h, 6h, & rolling 1h/6h/12h/24h/48h (mean/min/max).\n- **Log-target**: {USE_LOG_TARGET}.\n- **Target lags** (opsional): {USE_TARGET_LAGS} (aktif â†’ butuh AR inference).\n\n## Kenapa MSE keras pada outlier?\nMSE mengkuadratkan galat â†’ kesalahan besar dihukum jauh lebih berat.\nKita redam dengan: capping, imputasi median masa lalu, & transformasi log-target (opsional).\n\n## Regularisasi di LGBM\n- `num_leaves={lgb_params['num_leaves']}`, `min_data_in_leaf={lgb_params['min_data_in_leaf']}`,\n  `reg_alpha={lgb_params['reg_alpha']}`, `reg_lambda={lgb_params['reg_lambda']}`,\n  `feature_fraction={lgb_params['feature_fraction']}`, `bagging_fraction={lgb_params['bagging_fraction']}`.\n\"\"\"\nnotes_path = OUT_DIR / \"modeling_notes.md\"\nwith open(notes_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(notes_md)\n\n# Update metadata\nmeta_path = OUT_DIR / \"metadata.json\"\ntry:\n    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n        meta_loaded = json.load(f)\nexcept:\n    meta_loaded = {}\nmeta_loaded.setdefault(\"docs\", {})\nmeta_loaded[\"docs\"].update({\n    \"mse_controls_csv\": str(controls_path.name),\n    \"modeling_notes_md\": str(notes_path.name),\n})\nwith open(meta_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(meta_loaded, f, indent=2, ensure_ascii=False)\n\nprint(\"ðŸ“„ Dokumen tersimpan:\", controls_path.name, \"dan\", notes_path.name)\nprint(controls_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:13:35.281556Z","iopub.execute_input":"2025-11-01T03:13:35.281834Z","iopub.status.idle":"2025-11-01T03:17:34.118867Z","shell.execute_reply.started":"2025-11-01T03:13:35.281814Z","shell.execute_reply":"2025-11-01T03:17:34.118068Z"}},"outputs":[{"name":"stdout","text":"n_features: 179\ntrain_X: (47346, 179) test_X: (14610, 179) | USE_LOG_TARGET: True | USE_TARGET_LAGS: False\nFold 1 â€” best_iter=546, MSE(raw)=2.524500\nFold 2 â€” best_iter=4545, MSE(raw)=38.570857\nFold 3 â€” best_iter=7, MSE(raw)=12.862864\nFold 4 â€” best_iter=7, MSE(raw)=9.556301\nFold 5 â€” best_iter=267, MSE(raw)=43.463531\n\nCV MSE (OOF, raw) = 18.690772 | median best_iter = 267\nPred test (full)  mean/std: 6.378930636974528 3.163879498663481\nPred test (cvens) mean/std: 4.669245304047889 0.6604283203701171\nâœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916   3.572625\n1          54917   3.592873\n2          54918   3.655010\n3          54919   3.598167\n4          54920   3.688808\n5          54921   3.619780\n6          54922   3.642764\n7          54923   3.719023\n8          54924   3.682903\n9          54925   4.245930\nTop 20 FI:\n                                        feature          gain  split\n73             Specific Conductance_roll288_max  19207.092807    179\n83             Average Water Speed_roll144_mean   7992.371688    221\n86             Average Water Speed_roll288_mean   7970.686021    366\n71            Specific Conductance_roll288_mean   7051.384853    132\n147                Dissolved Oxygen_roll288_min   6417.946410    234\n174                     Chlorophyll_roll144_min   4535.015104    185\n18                                        month   2962.974887    162\n117  Dissolved Oxygen (%Saturation)_roll288_min   2723.177947    168\n72             Specific Conductance_roll288_min   2030.651860    164\n135                  Dissolved Oxygen_roll6_min   1856.359390    151\n177                     Chlorophyll_roll288_min   1677.510556    189\n39                     Average Water Speed_lag6   1545.194357    426\n146               Dissolved Oxygen_roll288_mean   1406.636021    200\n132                              pH_roll288_min   1406.546881    118\n105    Dissolved Oxygen (%Saturation)_roll6_min   1388.879007    189\n5                              Dissolved Oxygen   1359.519076    239\n162                     Temperature_roll288_min   1301.042397    193\n77              Average Water Speed_roll36_mean   1250.919470    423\n128                             pH_roll144_mean   1173.935415    149\n131                             pH_roll288_mean   1077.783334    244\nðŸ“„ Dokumen tersimpan: mse_controls.csv dan modeling_notes.md\n                        Area  \\\n0         Penanganan Outlier   \n1  Penanganan Missing Values   \n2  Normalisasi/Standardisasi   \n\n                                 Teknik Pengendalian  \\\n0       Hapus, Transformasi Logaritma, atau Capping.   \n1  Imputasi (dengan rata-rata/median), Imputasi D...   \n2      Min-Max Scaling atau Z-score Standardization.   \n\n                                          Keterangan  \n0  Kunci penting untuk MSE, karena outlier mengha...  \n1  Data yang hilang harus ditangani agar tidak me...  \n2  Memastikan semua fitur berkontribusi secara pr...  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\n# =========================================================\n# Blok 0 â€” Setup, Paths, Library\n# =========================================================\nimport os, gc, math, json, warnings, io\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# LightGBM\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"lightgbm\"])\n    import lightgbm as lgb\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\ntry:\n    import joblib\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"joblib\"])\n    import joblib\n\n# ====== Paths (Kaggle) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback lokal\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Fitur sensor top-korelasi (raw)\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n# === Opsi peningkatan ===\nUSE_LOG_TARGET   = True      # latih di log1p(Turbidity), inverse saat evaluasi & submit\nUSE_TARGET_LAGS  = False     # OFF (one-shot inference). ON => butuh AR inference\nUSE_DIR_FEATURES = True      # pakai arah arus â†’ sin/cos + u/v\nADD_DELTAS       = True      # tambah fitur perubahan (d1, d6h)\nADD_EWM          = True      # tambah EWMA (6h, 24h) anti-leakage\nCV_BLEND         = \"median\"  # 'mean' atau 'median' untuk blending prediksi CV\nBOOSTING_TYPE    = \"gbdt\"    # 'gbdt' atau 'dart'\nGAP_MULTIPLIER   = 1         # 1â†’24h, 2â†’48h gap CV\nQ_LOW, Q_HIGH    = 0.005, 0.995  # capping quantiles (train-only)\n\nDIR_COL = \"Average Water Direction\"  # arah arus (derajat)\n\n\n# =========================================================\n# Blok 1 â€” Load, Parse Timestamp, Sort, Sanity Checks\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\nneed_train = TOP_CORR_RAW + ([DIR_COL] if DIR_COL in train.columns else []) + [TARGET, \"Timestamp\", IDCOL]\nneed_test  = TOP_CORR_RAW + ([DIR_COL] if DIR_COL in test.columns  else []) + [\"Timestamp\", IDCOL]\nmt = [c for c in need_train if c not in train.columns]\nms = [c for c in need_test  if c not in test.columns]\nif mt: print(\"âš ï¸ Missing train cols:\", mt)\nif ms: print(\"âš ï¸ Missing test cols :\", ms)\n\n\n# =========================================================\n# Blok 1a â€” Drop baris dengan TARGET NaN (train-only)\n# =========================================================\n# Pastikan target numerik; string seperti \"NaN\" akan jadi NaN\ntrain[TARGET] = pd.to_numeric(train[TARGET], errors=\"coerce\")\n\nn_before = len(train)\ntrain = train[train[TARGET].notna()].reset_index(drop=True)\nn_dropped = n_before - len(train)\nprint(f\"ðŸ§¹ Dropped {n_dropped} rows with NaN in '{TARGET}' from train.\")\n\n\n\n\n# =========================================================\n# Blok 2 â€” Estimasi Resolusi Waktu (steps/hour)\n# =========================================================\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt)==0: return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med<=0: return 1\n    return int(max(1, round(3600.0/med)))\n\nsteps_per_hour = estimate_steps_per_hour(train)\nsteps_1h  = max(1, steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_12h = max(1, 12*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\nsteps_48h = max(1, 48*steps_per_hour)\n\nprint(f\"steps/hour={steps_per_hour} | 1h={steps_1h} 6h={steps_6h} 12h={steps_12h} 24h={steps_24h} 48h={steps_48h}\")\n\n\n# =========================================================\n# Blok 2b â€” Outlier Capping (train-only bounds untuk 8 sensor)\n# =========================================================\ndef compute_caps(ref_df, cols, q_low, q_high):\n    bounds = {}\n    for c in cols:\n        lo, hi = ref_df[c].quantile([q_low, q_high])\n        if not np.isfinite(lo): lo = ref_df[c].min()\n        if not np.isfinite(hi): hi = ref_df[c].max()\n        bounds[c] = (lo, hi)\n    return bounds\n\ndef apply_caps(df, bounds):\n    out = df.copy()\n    for c, (lo, hi) in bounds.items():\n        if c in out.columns:\n            out[c] = out[c].clip(lo, hi)\n    return out\n\ncaps = compute_caps(train, TOP_CORR_RAW, Q_LOW, Q_HIGH)\ntrain = apply_caps(train, caps)\ntest  = apply_caps(test,  caps)\n\n\n\n# =========================================================\n# Blok 3 â€” Flag Missingness (FULL RAW, sebelum imputasi)\n# =========================================================\n# Daftar kolom yang diimputasi (sensor + arah jika ada)\nIMP_COLS = TOP_CORR_RAW + ([DIR_COL] if (USE_DIR_FEATURES and DIR_COL in train.columns) else [])\n\ntrain[\"_seg\"] = \"train\"\ntest[\"_seg\"]  = \"test\"\nfull_raw = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\nfor c in IMP_COLS:\n    full_raw[c+\"_was_na\"] = full_raw[c].isna().astype(int)\n\ntrain_flags = full_raw.loc[full_raw[\"_seg\"]==\"train\", [c+\"_was_na\" for c in IMP_COLS]].reset_index(drop=True)\ntest_flags  = full_raw.loc[full_raw[\"_seg\"]==\"test\",  [c+\"_was_na\" for c in IMP_COLS]].reset_index(drop=True)\n\n\n# =========================================================\n# Blok 4 â€” Imputasi TRAIN (TRAIN-only)\n# =========================================================\ntrain[\"pH\"] = train[\"pH\"].clip(0, 14)\nif USE_DIR_FEATURES and DIR_COL in train.columns:\n    train[DIR_COL] = (train[DIR_COL] % 360).clip(0, 360)\n\ntrain_medians = {c: train[c].median(skipna=True) for c in IMP_COLS}\n\ndef impute_train_timeaware(df, cols, roll_win, ffill_limit, med_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        past_med = s.shift(1).rolling(window=roll_win, min_periods=1).median()  # masa lalu saja\n        s = s.where(~s.isna(), past_med)\n        s = s.ffill(limit=ffill_limit)  # â‰¤ 6 jam\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))  # fallback train-only\n        out[c] = s\n    return out\n\ntrain_imp = impute_train_timeaware(\n    df=train, cols=IMP_COLS, roll_win=steps_6h, ffill_limit=steps_6h, med_map=train_medians\n)\n\nlast_known_from_train = {}\nfor c in IMP_COLS:\n    last_val = train_imp[c].iloc[-1]\n    if pd.isna(last_val):\n        last_val = train_medians[c]\n    last_known_from_train[c] = last_val\n\n\n\n# =========================================================\n# Blok 5 â€” Imputasi TEST (seed dari TRAIN, tanpa statistik test)\n# =========================================================\ntest[\"pH\"] = test[\"pH\"].clip(0, 14)\nif USE_DIR_FEATURES and DIR_COL in test.columns:\n    test[DIR_COL] = (test[DIR_COL] % 360).clip(0, 360)\n\ndef impute_test_seeded(df, cols, ffill_limit, med_map, seed_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        if len(s)>0 and pd.isna(s.iloc[0]) and pd.notna(seed_map.get(c, np.nan)):\n            s.iloc[0] = seed_map[c]\n        s = s.ffill(limit=ffill_limit)  # â‰¤ 6 jam\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))  # fallback median TRAIN-only\n        out[c] = s\n    return out\n\ntest_imp = impute_test_seeded(\n    df=test, cols=IMP_COLS, ffill_limit=steps_6h, med_map=train_medians, seed_map=last_known_from_train\n)\n\n\n\n# =========================================================\n# Blok 6 â€” Gabungkan, Tambahkan Flags, Fitur Temporal & Interaksi\n# =========================================================\ntrain_imp[\"_seg\"] = \"train\"\ntest_imp[\"_seg\"]  = \"test\"\nfull_imp = pd.concat([train_imp, test_imp], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\n# flags\nflag_cols = [c+\"_was_na\" for c in IMP_COLS]\nfull_flags = pd.concat([train_flags, test_flags], axis=0, ignore_index=True)\nfor col in flag_cols:\n    full_imp[col] = full_flags[col].values\n\n# Direction â†’ sin/cos + u/v\nDIR_FEATS = []\nif USE_DIR_FEATURES and DIR_COL in full_imp.columns:\n    full_imp[\"dir_rad\"] = np.deg2rad((full_imp[DIR_COL] % 360).fillna(0.0))\n    full_imp[\"dir_sin\"] = np.sin(full_imp[\"dir_rad\"])\n    full_imp[\"dir_cos\"] = np.cos(full_imp[\"dir_rad\"])\n    full_imp[\"u\"] = full_imp[\"Average Water Speed\"] * full_imp[\"dir_cos\"]\n    full_imp[\"v\"] = full_imp[\"Average Water Speed\"] * full_imp[\"dir_sin\"]\n    DIR_FEATS = [\"dir_sin\",\"dir_cos\",\"u\",\"v\"]\n\n# Fitur temporal\ndef add_time_features(df):\n    out = df.copy()\n    out[\"hour\"]  = out[\"Timestamp\"].dt.hour\n    out[\"dow\"]   = out[\"Timestamp\"].dt.dayofweek\n    out[\"month\"] = out[\"Timestamp\"].dt.month\n    # sin/cos siklik\n    out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24.0)\n    out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24.0)\n    out[\"dow_sin\"]  = np.sin(2*np.pi*out[\"dow\"]/7.0)\n    out[\"dow_cos\"]  = np.cos(2*np.pi*out[\"dow\"]/7.0)\n    return out\n\nfull_imp = add_time_features(full_imp)\nTIME_FEATS = [\"hour\",\"dow\",\"month\",\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\"]\n\n# Fitur interaksi (domain)\ndef add_interactions(df):\n    out = df.copy()\n    out[\"speed_x_sal\"] = out[\"Average Water Speed\"] * out[\"Salinity\"]\n    out[\"temp_x_do\"]   = out[\"Temperature\"] * out[\"Dissolved Oxygen\"]\n    out[\"ph_x_cond\"]   = out[\"pH\"] * out[\"Specific Conductance\"]\n    out[\"cond_x_sal\"]  = out[\"Specific Conductance\"] * out[\"Salinity\"]\n    return out\n\nfull_imp = add_interactions(full_imp)\nINTER_FEATS = [\"speed_x_sal\",\"temp_x_do\",\"ph_x_cond\",\"cond_x_sal\"]\n\n\n\n# =========================================================\n# Blok 6b â€” Lag & Rolling (sensor + u/v), Z-Score, Delta & EWMA\n# =========================================================\ndef add_lag_features(df, cols, lags):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n    return out\n\ndef add_roll_features(df, cols, windows):\n    out = df.copy()\n    for c in cols:\n        for w in windows:\n            past = out[c].shift(1).rolling(window=w, min_periods=1)  # masa lalu\n            out[f\"{c}_roll{w}_mean\"] = past.mean()\n            out[f\"{c}_roll{w}_min\"]  = past.min()\n            out[f\"{c}_roll{w}_max\"]  = past.max()\n    return out\n\n# Sensor utama untuk lag/roll/z\nSENSOR_COLS = TOP_CORR_RAW + ([\"u\",\"v\"] if (\"u\" in full_imp.columns and \"v\" in full_imp.columns) else [])\n\n# Lag & rolling\nfull_imp = add_lag_features(full_imp, SENSOR_COLS, [1, steps_1h, steps_6h])\nfull_imp = add_roll_features(full_imp, SENSOR_COLS, [steps_1h, steps_6h, steps_12h, steps_24h, steps_48h])\n\nlag_feats   = [c for c in full_imp.columns if \"_lag\"  in c]\nroll_feats  = [c for c in full_imp.columns if \"_roll\" in c and c.endswith((\"mean\",\"min\",\"max\"))]\nmiss_flags  = [c+\"_was_na\" for c in IMP_COLS]\n\n# Z-score (train-only mean/std)\ntrain_mask = full_imp[\"_seg\"]==\"train\"\nZ_FEATS = []\nfor c in SENSOR_COLS:\n    m = full_imp.loc[train_mask, c].mean()\n    s = full_imp.loc[train_mask, c].std(ddof=0)\n    full_imp[c+\"_z\"] = (full_imp[c]-m) / (s if (s and s!=0) else 1.0)\n    Z_FEATS.append(c+\"_z\")\n\n# Delta features\nDELTA_FEATS = []\nif ADD_DELTAS:\n    for c in SENSOR_COLS:\n        full_imp[f\"{c}_d1\"]   = full_imp[c] - full_imp[c].shift(1)\n        full_imp[f\"{c}_d6h\"]  = full_imp[c] - full_imp[c].shift(steps_6h)\n        DELTA_FEATS += [f\"{c}_d1\", f\"{c}_d6h\"]\n\n# EWMA (halflife ~ window; anti-leakage via shift(1))\nEWM_FEATS = []\nif ADD_EWM:\n    for c in SENSOR_COLS:\n        s = full_imp[c].shift(1)\n        full_imp[f\"{c}_ewm6_mean\"]  = s.ewm(halflife=steps_6h,  min_periods=1).mean()\n        full_imp[f\"{c}_ewm24_mean\"] = s.ewm(halflife=steps_24h, min_periods=1).mean()\n        EWM_FEATS += [f\"{c}_ewm6_mean\", f\"{c}_ewm24_mean\"]\n\n\n\n\n# =========================================================\n# Blok 6c â€” (Opsional) Lag Target (OFF by default, AR inference needed)\n# =========================================================\nTARGET_LAGS = []\nif USE_TARGET_LAGS:\n    for L in [1, steps_1h, steps_6h]:\n        full_imp[f\"{TARGET}_lag{L}\"] = np.nan\n    train_idx = full_imp[\"_seg\"]==\"train\"\n    for L in [1, steps_1h, steps_6h]:\n        full_imp.loc[train_idx, f\"{TARGET}_lag{L}\"] = train[TARGET].shift(L).values\n    TARGET_LAGS = [f\"{TARGET}_lag{L}\" for L in [1, steps_1h, steps_6h]]\n\n\n\n# =========================================================\n# Blok 7 â€” Split Train/Test Matrices (drop NaN awal lag/roll)\n# =========================================================\ntrain_fe = full_imp[full_imp[\"_seg\"]==\"train\"].copy()\ntest_fe  = full_imp[full_imp[\"_seg\"]==\"test\"].copy()\n\n# Align target\ntrain_fe[TARGET] = train[TARGET].values\n\n# Kumpulkan fitur\nTIME_FEATS = [\"hour\",\"dow\",\"month\",\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\"]\nFEATURES = (\n    TOP_CORR_RAW +              # sensor mentah\n    DIR_FEATS +                 # sin/cos, u/v (jika ada)\n    miss_flags +                # flags NA\n    TIME_FEATS +                # temporal\n    INTER_FEATS +              # interaksi\n    Z_FEATS +                   # z-score\n    lag_feats + roll_feats +    # lag & rolling\n    DELTA_FEATS + EWM_FEATS +   # delta & ewma\n    TARGET_LAGS                 # target lag (opsional)\n)\n\n# Drop baris awal train yang masih NaN (efek lag/roll/ewm)\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\n# Log target (opsional)\ny_raw = train_fe[TARGET].values\ny = np.log1p(y_raw) if USE_LOG_TARGET else y_raw\n\ntrain_X = train_fe[FEATURES].copy()\ntest_X  = test_fe[FEATURES].copy()\n\n# Isi NaN sisa test dengan median TRAIN (aman)\ntest_X = test_X.fillna(train_X.median())\n\nprint(\"n_features:\", len(FEATURES))\nprint(\"train_X:\", train_X.shape, \"test_X:\", test_X.shape, \"| USE_LOG_TARGET:\", USE_LOG_TARGET, \"| USE_TARGET_LAGS:\", USE_TARGET_LAGS)\n\n\n# =========================================================\n# Blok 8 â€” TimeSeriesSplit CV (gap = 24h) & LGBM Training (regularized + extra_trees)\n# =========================================================\ngap = steps_24h * GAP_MULTIPLIER\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\nlgb_params = dict(\n    objective=\"l2\",\n    boosting_type=BOOSTING_TYPE,   # 'gbdt' atau 'dart'\n    learning_rate=0.035,\n    num_leaves=72,\n    max_depth=-1,\n    min_data_in_leaf=120,\n    feature_fraction=0.9,\n    feature_fraction_bynode=0.8,\n    bagging_fraction=0.75,\n    bagging_freq=1,\n    extra_trees=True,\n    reg_alpha=0.3,\n    reg_lambda=6.0,\n    n_estimators=12000,\n    random_state=SEED,\n    verbose=-1\n)\n\noof_pred_log = np.zeros(len(train_fe))\ntest_fold_preds_log = []\nbest_iters = []\nfold = 0\n\nfor tr_idx, va_idx in tscv.split(train_X):\n    fold += 1\n    X_tr, y_tr = train_X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = train_X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=800, verbose=False)]\n    )\n\n    best_it = model.best_iteration_\n    best_iters.append(int(best_it) if best_it is not None else int(lgb_params[\"n_estimators\"]))\n\n    pred_va_log = model.predict(X_va, num_iteration=best_it)\n    oof_pred_log[va_idx] = pred_va_log\n\n    pred_te_log = model.predict(test_X, num_iteration=best_it)\n    test_fold_preds_log.append(pred_te_log)\n\n    # MSE OOF ruang asli\n    y_va_raw = np.expm1(y_va) if USE_LOG_TARGET else y_va\n    pred_va_raw = np.expm1(pred_va_log) if USE_LOG_TARGET else pred_va_log\n    pred_va_raw = np.clip(pred_va_raw, 0, None)\n    mse_fold = mean_squared_error(y_va_raw, pred_va_raw)\n    print(f\"Fold {fold} â€” best_iter={best_it}, MSE(raw)={mse_fold:.6f}\")\n\nbest_iter_final = int(np.median(best_iters)) if len(best_iters)>0 else int(lgb_params[\"n_estimators\"])\n\n# CV MSE (OOF) ruang asli\noof_pred_raw = np.expm1(oof_pred_log) if USE_LOG_TARGET else oof_pred_log\noof_pred_raw = np.clip(oof_pred_raw, 0, None)\ncv_mse = mean_squared_error(train_fe[TARGET].values, oof_pred_raw)\nprint(f\"\\nCV MSE (OOF, raw) = {cv_mse:.6f} | median best_iter = {best_iter_final}\")\n\n\n# =========================================================\n# Blok 9 â€” Train Full Model & Prediksi Test (Full + CV-Blending median)\n# =========================================================\nlgb_full = lgb.LGBMRegressor(**{**lgb_params, \"n_estimators\": best_iter_final})\nlgb_full.fit(train_X, y)\n\npred_test_full_log  = lgb_full.predict(test_X)\nstack_te = np.vstack(test_fold_preds_log) if len(test_fold_preds_log)>0 else np.vstack([pred_test_full_log])\npred_test_cvens_log_mean   = stack_te.mean(axis=0)\npred_test_cvens_log_median = np.median(stack_te, axis=0)\n\ndef inv_and_clip(x):\n    return np.clip(np.expm1(x) if USE_LOG_TARGET else x, 0, None)\n\n# Blend pilihan\nif CV_BLEND == \"mean\":\n    pred_test_cvens = inv_and_clip(pred_test_cvens_log_mean)\nelse:\n    pred_test_cvens = inv_and_clip(pred_test_cvens_log_median)\n\npred_test_full = inv_and_clip(pred_test_full_log)\n\nprint(\"Blend:\", CV_BLEND)\nprint(\"Pred test (full)  mean/std:\", float(pred_test_full.mean()),  float(pred_test_full.std()))\nprint(\"Pred test (cvens) mean/std:\", float(pred_test_cvens.mean()), float(pred_test_cvens.std()))\n\n\n\n# =========================================================\n# Blok 10 â€” Submission (default: CV-blended)\n# =========================================================\nFINAL_PRED = pred_test_cvens  # ganti ke pred_test_full bila ingin full-model\n\nsubmission = pd.DataFrame({\n    \"Record number\": test_fe[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n\n# Jaga urutan sesuai sample\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n\n\n# =========================================================\n# Blok 11 â€” Artefak (model, fitur, OOF, FI, metadata)\n# =========================================================\nOUT_DIR = Path(\"./outputs_lgbm_maximized\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Submission alternatif\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": pred_test_full}).to_csv(\n    OUT_DIR / \"submission_full_model.csv\", index=False\n)\npd.DataFrame({\"Record number\": test_fe[IDCOL].values, \"Turbidity\": FINAL_PRED}).to_csv(\n    OUT_DIR / \"submission_cv_ensemble.csv\", index=False\n)\n\n# Simpan model\nlgb_full.booster_.save_model(str(OUT_DIR / \"lgbm_full.txt\"))\njoblib.dump(lgb_full, OUT_DIR / \"lgbm_full.pkl\")\n\n# Fitur\npd.Series(train_X.columns).to_csv(OUT_DIR / \"features.txt\", index=False, header=False)\n\n# OOF\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_pred_raw[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# Feature importance\nfi = pd.DataFrame({\n    \"feature\": train_X.columns,\n    \"gain\":   lgb_full.booster_.feature_importance(importance_type=\"gain\"),\n    \"split\":  lgb_full.booster_.feature_importance(importance_type=\"split\"),\n}).sort_values(\"gain\", ascending=False)\nfi.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\nprint(\"Top 20 FI:\")\nprint(fi.head(20))\n\n# Metadata + caps bounds\nmeta = {\n    \"cv_mse_raw\": float(cv_mse),\n    \"n_splits\": int(n_splits),\n    \"gap_steps\": int(gap),\n    \"gap_hours\": float(gap / steps_per_hour),\n    \"lgb_params\": {**lgb_params, \"n_estimators\": int(best_iter_final)},\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(train_X.columns)),\n    \"use_log_target\": bool(USE_LOG_TARGET),\n    \"use_target_lags\": bool(USE_TARGET_LAGS),\n    \"caps_quantiles\": [Q_LOW, Q_HIGH],\n    \"caps_bounds\": {k: [float(v[0]), float(v[1])] for k, v in caps.items()},\n    \"blend\": CV_BLEND,\n    \"use_dir_features\": bool(USE_DIR_FEATURES),\n    \"add_deltas\": bool(ADD_DELTAS),\n    \"add_ewm\": bool(ADD_EWM),\n    \"boosting_type\": BOOSTING_TYPE,\n    \"impute_policy\": {\n        \"train\": \"past_rolling_median -> ffill<=6h -> fallback median TRAIN-only\",\n        \"test\":  \"seed last-known(train) -> ffill<=6h -> fallback median TRAIN-only\",\n    }\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:50.839207Z","iopub.execute_input":"2025-11-01T03:51:50.839521Z","iopub.status.idle":"2025-11-01T03:53:20.339369Z","shell.execute_reply.started":"2025-11-01T03:51:50.839499Z","shell.execute_reply":"2025-11-01T03:53:20.338459Z"}},"outputs":[{"name":"stdout","text":"ðŸ§¹ Dropped 6066 rows with NaN in 'Turbidity' from train.\nsteps/hour=6 | 1h=6 6h=36 12h=72 24h=144 48h=288\nn_features: 262\ntrain_X: (47346, 262) test_X: (14610, 262) | USE_LOG_TARGET: True | USE_TARGET_LAGS: False\nFold 1 â€” best_iter=259, MSE(raw)=5.854045\nFold 2 â€” best_iter=108, MSE(raw)=41.684473\nFold 3 â€” best_iter=9, MSE(raw)=13.134942\nFold 4 â€” best_iter=6, MSE(raw)=9.766971\nFold 5 â€” best_iter=904, MSE(raw)=42.985433\n\nCV MSE (OOF, raw) = 19.765407 | median best_iter = 108\nBlend: median\nPred test (full)  mean/std: 5.908182841917406 2.2359612857142936\nPred test (cvens) mean/std: 3.4844024459642173 0.2549995364472989\nâœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916   3.109019\n1          54917   3.128946\n2          54918   3.128946\n3          54919   3.128946\n4          54920   3.128946\n5          54921   3.110548\n6          54922   3.128946\n7          54923   3.128946\n8          54924   3.128946\n9          54925   3.128946\nTop 20 FI:\n                                         feature         gain  split\n99              Average Water Speed_roll288_mean  4851.111907     58\n245               Average Water Speed_ewm24_mean  3350.398804     61\n244                Average Water Speed_ewm6_mean  3172.748307     56\n249    Dissolved Oxygen (%Saturation)_ewm24_mean  3133.724269     69\n243              Specific Conductance_ewm24_mean  3086.792290     26\n111                        Salinity_roll144_mean  2731.368503     26\n129  Dissolved Oxygen (%Saturation)_roll288_mean  2439.596544     44\n242               Specific Conductance_ewm6_mean  2294.989114     16\n96              Average Water Speed_roll144_mean  2224.147464     42\n74                Specific Conductance_roll6_max  2017.053873     18\n30                                     ph_x_cond  1949.426879     24\n144                              pH_roll288_mean  1761.425190     53\n75              Specific Conductance_roll36_mean  1740.948243     17\n0                           Specific Conductance  1597.805585     20\n23                                         month  1570.724180     70\n160                 Dissolved Oxygen_roll288_min  1515.609532     54\n78              Specific Conductance_roll72_mean  1421.608625     12\n204                               u_roll288_mean  1361.410413     68\n259                                 u_ewm24_mean  1314.501654     95\n251                                pH_ewm24_mean  1296.959872     65\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# =========================================================\n# Blok 0 â€” Setup, Paths, Library\n# =========================================================\nimport os, gc, math, json, warnings, io\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.utils.validation import check_is_fitted\nimport joblib\n\n# ====== Paths (Kaggle) ======\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback lokal\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nrng = np.random.RandomState(SEED)\nnp.random.seed(SEED)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\n\n# Sensor top-korelasi (raw)\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n# === Opsi modeling ===\nUSE_LOG_TARGET   = True      # latih di log1p(Turbidity), inverse saat evaluasi & submit\nUSE_TARGET_LAGS  = False     # ON => akan buat fitur lag target + (opsional) AR inference\nUSE_DIR_FEATURES = True      # arah arus â†’ sin/cos + u/v\nADD_DELTAS       = True      # delta d1 & d6h\nADD_EWM          = True      # EWMA 6h & 24h (anti-leakage)\nGAP_MULTIPLIER   = 1         # 1â†’24h gap antar fold\nQ_LOW, Q_HIGH    = 0.005, 0.995  # outlier capping quantiles (train-only)\n\n# Linear model toggle\nTRY_RIDGE        = True      # uji RidgeCV selain LinearRegression, pilih yang terbaik\nRIDGE_ALPHAS     = np.logspace(-3, 3, 13)  # kandidat alpha Ridge\n\nDIR_COL = \"Average Water Direction\"  # derajat\n\n# =========================================================\n# Blok 1 â€” Load, Parse Timestamp, Sort, Sanity Checks\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\n# sort\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\n# pastikan kolom\nneed_train = TOP_CORR_RAW + ([DIR_COL] if DIR_COL in train.columns else []) + [TARGET, \"Timestamp\", IDCOL]\nneed_test  = TOP_CORR_RAW + ([DIR_COL] if DIR_COL in test.columns  else []) + [\"Timestamp\", IDCOL]\nmt = [c for c in need_train if c not in train.columns]\nms = [c for c in need_test  if c not in test.columns]\nif mt: print(\"âš ï¸ Missing train cols:\", mt)\nif ms: print(\"âš ï¸ Missing test cols :\", ms)\n\n# =========================================================\n# Blok 1a â€” Drop baris dengan TARGET NaN (train-only)\n# =========================================================\ntrain[TARGET] = pd.to_numeric(train[TARGET], errors=\"coerce\")\nn_before = len(train)\ntrain = train[train[TARGET].notna()].reset_index(drop=True)\nprint(f\"ðŸ§¹ Dropped {n_before - len(train)} rows with NaN in '{TARGET}' from train.\")\n\n# =========================================================\n# Blok 2 â€” Estimasi Resolusi Waktu (steps/hour)\n# =========================================================\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt)==0: return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med<=0: return 1\n    return int(max(1, round(3600.0/med)))\n\nsteps_per_hour = estimate_steps_per_hour(train)\nsteps_1h  = max(1, steps_per_hour)\nsteps_6h  = max(1, 6*steps_per_hour)\nsteps_12h = max(1, 12*steps_per_hour)\nsteps_24h = max(1, 24*steps_per_hour)\nsteps_48h = max(1, 48*steps_per_hour)\nprint(f\"steps/hour={steps_per_hour} | 1h={steps_1h} 6h={steps_6h} 12h={steps_12h} 24h={steps_24h} 48h={steps_48h}\")\n\n# =========================================================\n# Blok 2b â€” Outlier Capping (train-only bounds untuk 8 sensor)\n# =========================================================\ndef compute_caps(ref_df, cols, q_low, q_high):\n    bounds = {}\n    for c in cols:\n        lo, hi = ref_df[c].quantile([q_low, q_high])\n        if not np.isfinite(lo): lo = ref_df[c].min()\n        if not np.isfinite(hi): hi = ref_df[c].max()\n        bounds[c] = (lo, hi)\n    return bounds\n\ndef apply_caps(df, bounds):\n    out = df.copy()\n    for c, (lo, hi) in bounds.items():\n        if c in out.columns:\n            out[c] = out[c].clip(lo, hi)\n    return out\n\ncaps = compute_caps(train, TOP_CORR_RAW, Q_LOW, Q_HIGH)\ntrain = apply_caps(train, caps)\ntest  = apply_caps(test,  caps)\n\n# =========================================================\n# Blok 3 â€” Flag Missingness (FULL RAW, sebelum imputasi)\n# =========================================================\nIMP_COLS = TOP_CORR_RAW + ([DIR_COL] if (USE_DIR_FEATURES and DIR_COL in train.columns) else [])\ntrain[\"_seg\"] = \"train\"\ntest[\"_seg\"]  = \"test\"\nfull_raw = pd.concat([train, test], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\nfor c in IMP_COLS:\n    full_raw[c+\"_was_na\"] = full_raw[c].isna().astype(int)\n\ntrain_flags = full_raw.loc[full_raw[\"_seg\"]==\"train\", [c+\"_was_na\" for c in IMP_COLS]].reset_index(drop=True)\ntest_flags  = full_raw.loc[full_raw[\"_seg\"]==\"test\",  [c+\"_was_na\" for c in IMP_COLS]].reset_index(drop=True)\n\n# =========================================================\n# Blok 4 â€” Imputasi TRAIN (TRAIN-only)\n# =========================================================\ntrain[\"pH\"] = train[\"pH\"].clip(0, 14)\nif USE_DIR_FEATURES and DIR_COL in train.columns:\n    train[DIR_COL] = (train[DIR_COL] % 360).clip(0, 360)\n\ntrain_medians = {c: train[c].median(skipna=True) for c in IMP_COLS}\n\ndef impute_train_timeaware(df, cols, roll_win, ffill_limit, med_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        past_med = s.shift(1).rolling(window=roll_win, min_periods=1).median()\n        s = s.where(~s.isna(), past_med)\n        s = s.ffill(limit=ffill_limit)  # â‰¤ 6 jam\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))\n        out[c] = s\n    return out\n\ntrain_imp = impute_train_timeaware(\n    df=train, cols=IMP_COLS, roll_win=steps_6h, ffill_limit=steps_6h, med_map=train_medians\n)\n\nlast_known_from_train = {}\nfor c in IMP_COLS:\n    last_val = train_imp[c].iloc[-1]\n    if pd.isna(last_val): last_val = train_medians[c]\n    last_known_from_train[c] = last_val\n\n# =========================================================\n# Blok 5 â€” Imputasi TEST (seed dari TRAIN, tanpa statistik test)\n# =========================================================\ntest[\"pH\"] = test[\"pH\"].clip(0, 14)\nif USE_DIR_FEATURES and DIR_COL in test.columns:\n    test[DIR_COL] = (test[DIR_COL] % 360).clip(0, 360)\n\ndef impute_test_seeded(df, cols, ffill_limit, med_map, seed_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        if len(s)>0 and pd.isna(s.iloc[0]) and pd.notna(seed_map.get(c, np.nan)):\n            s.iloc[0] = seed_map[c]\n        s = s.ffill(limit=ffill_limit)\n        s = s.fillna(med_map.get(c, np.nanmedian(out[c].values)))\n        out[c] = s\n    return out\n\ntest_imp = impute_test_seeded(\n    df=test, cols=IMP_COLS, ffill_limit=steps_6h, med_map=train_medians, seed_map=last_known_from_train\n)\n\n# =========================================================\n# Blok 6 â€” Gabungkan, Flags, Fitur Temporal & Interaksi\n# =========================================================\ntrain_imp[\"_seg\"] = \"train\"\ntest_imp[\"_seg\"]  = \"test\"\nfull_imp = pd.concat([train_imp, test_imp], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n\n# flags\nflag_cols = [c+\"_was_na\" for c in IMP_COLS]\nfull_flags = pd.concat([train_flags, test_flags], axis=0, ignore_index=True)\nfor col in flag_cols:\n    full_imp[col] = full_flags[col].values\n\n# Direction â†’ sin/cos + u/v\nDIR_FEATS = []\nif USE_DIR_FEATURES and DIR_COL in full_imp.columns:\n    full_imp[\"dir_rad\"] = np.deg2rad((full_imp[DIR_COL] % 360).fillna(0.0))\n    full_imp[\"dir_sin\"] = np.sin(full_imp[\"dir_rad\"])\n    full_imp[\"dir_cos\"] = np.cos(full_imp[\"dir_rad\"])\n    full_imp[\"u\"] = full_imp[\"Average Water Speed\"] * full_imp[\"dir_cos\"]\n    full_imp[\"v\"] = full_imp[\"Average Water Speed\"] * full_imp[\"dir_sin\"]\n    DIR_FEATS = [\"dir_sin\",\"dir_cos\",\"u\",\"v\"]\n\n# Fitur temporal\ndef add_time_features(df):\n    out = df.copy()\n    out[\"hour\"]  = out[\"Timestamp\"].dt.hour\n    out[\"dow\"]   = out[\"Timestamp\"].dt.dayofweek\n    out[\"month\"] = out[\"Timestamp\"].dt.month\n    out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24.0)\n    out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24.0)\n    out[\"dow_sin\"]  = np.sin(2*np.pi*out[\"dow\"]/7.0)\n    out[\"dow_cos\"]  = np.cos(2*np.pi*out[\"dow\"]/7.0)\n    return out\n\nfull_imp = add_time_features(full_imp)\nTIME_FEATS = [\"hour\",\"dow\",\"month\",\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\"]\n\n# Interaksi\ndef add_interactions(df):\n    out = df.copy()\n    out[\"speed_x_sal\"] = out[\"Average Water Speed\"] * out[\"Salinity\"]\n    out[\"temp_x_do\"]   = out[\"Temperature\"] * out[\"Dissolved Oxygen\"]\n    out[\"ph_x_cond\"]   = out[\"pH\"] * out[\"Specific Conductance\"]\n    out[\"cond_x_sal\"]  = out[\"Specific Conductance\"] * out[\"Salinity\"]\n    return out\n\nfull_imp = add_interactions(full_imp)\nINTER_FEATS = [\"speed_x_sal\",\"temp_x_do\",\"ph_x_cond\",\"cond_x_sal\"]\n\n# =========================================================\n# Blok 6b â€” Lag & Rolling (sensor + u/v), Z-Score, Delta & EWMA\n# =========================================================\ndef add_lag_features(df, cols, lags):\n    out = df.copy()\n    for c in cols:\n        for L in lags:\n            out[f\"{c}_lag{L}\"] = out[c].shift(L)\n    return out\n\ndef add_roll_features(df, cols, windows):\n    out = df.copy()\n    for c in cols:\n        for w in windows:\n            past = out[c].shift(1).rolling(window=w, min_periods=1)\n            out[f\"{c}_roll{w}_mean\"] = past.mean()\n            out[f\"{c}_roll{w}_min\"]  = past.min()\n            out[f\"{c}_roll{w}_max\"]  = past.max()\n    return out\n\nSENSOR_COLS = TOP_CORR_RAW + ([\"u\",\"v\"] if (\"u\" in full_imp.columns and \"v\" in full_imp.columns) else [])\nfull_imp = add_lag_features(full_imp, SENSOR_COLS, [1, steps_1h, steps_6h])\nfull_imp = add_roll_features(full_imp, SENSOR_COLS, [steps_1h, steps_6h, steps_12h, steps_24h, steps_48h])\n\nlag_feats   = [c for c in full_imp.columns if \"_lag\"  in c]\nroll_feats  = [c for c in full_imp.columns if \"_roll\" in c and c.endswith((\"mean\",\"min\",\"max\"))]\nmiss_flags  = [c+\"_was_na\" for c in IMP_COLS]\n\n# Z-score (train-only mean/std untuk raw sensor & u/v)\ntrain_mask = full_imp[\"_seg\"]==\"train\"\nZ_FEATS = []\nfor c in SENSOR_COLS:\n    m = full_imp.loc[train_mask, c].mean()\n    s = full_imp.loc[train_mask, c].std(ddof=0)\n    full_imp[c+\"_z\"] = (full_imp[c]-m) / (s if (s and s!=0) else 1.0)\n    Z_FEATS.append(c+\"_z\")\n\n# Delta\nDELTA_FEATS = []\nif ADD_DELTAS:\n    for c in SENSOR_COLS:\n        full_imp[f\"{c}_d1\"]   = full_imp[c] - full_imp[c].shift(1)\n        full_imp[f\"{c}_d6h\"]  = full_imp[c] - full_imp[c].shift(steps_6h)\n        DELTA_FEATS += [f\"{c}_d1\", f\"{c}_d6h\"]\n\n# EWMA\nEWM_FEATS = []\nif ADD_EWM:\n    for c in SENSOR_COLS:\n        s = full_imp[c].shift(1)\n        full_imp[f\"{c}_ewm6_mean\"]  = s.ewm(halflife=steps_6h,  min_periods=1).mean()\n        full_imp[f\"{c}_ewm24_mean\"] = s.ewm(halflife=steps_24h, min_periods=1).mean()\n        EWM_FEATS += [f\"{c}_ewm6_mean\", f\"{c}_ewm24_mean\"]\n\n# =========================================================\n# Blok 6c â€” Lag Target (opsional)\n# =========================================================\nTARGET_LAGS = []\nif USE_TARGET_LAGS:\n    for L in [1, steps_1h, steps_6h]:\n        full_imp[f\"{TARGET}_lag{L}\"] = np.nan\n    train_idx = full_imp[\"_seg\"]==\"train\"\n    for L in [1, steps_1h, steps_6h]:\n        full_imp.loc[train_idx, f\"{TARGET}_lag{L}\"] = train[TARGET].shift(L).values\n    TARGET_LAGS = [f\"{TARGET}_lag{L}\" for L in [1, steps_1h, steps_6h]]\n\n# =========================================================\n# Blok 7 â€” Split, Standarisasi & Matrices\n# =========================================================\ntrain_fe = full_imp[full_imp[\"_seg\"]==\"train\"].copy()\ntest_fe  = full_imp[full_imp[\"_seg\"]==\"test\"].copy()\ntrain_fe[TARGET] = train[TARGET].values\n\nTIME_FEATS = [\"hour\",\"dow\",\"month\",\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\"]\nFEATURES = (\n    TOP_CORR_RAW +\n    DIR_FEATS +\n    miss_flags +\n    TIME_FEATS +\n    INTER_FEATS +\n    Z_FEATS +\n    lag_feats + roll_feats +\n    DELTA_FEATS + EWM_FEATS +\n    TARGET_LAGS\n)\n\n# Drop baris awal train yang masih NaN (efek lag/roll/ewm)\ntrain_fe = train_fe.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)\n\ny_raw = train_fe[TARGET].values\ny = np.log1p(y_raw) if USE_LOG_TARGET else y_raw\n\nX_all = train_fe[FEATURES].copy()\nX_test = test_fe[FEATURES].copy()\n\n# Isi NaN sisa test dengan median TRAIN (aman)\nX_test = X_test.fillna(X_all.median())\n\nprint(\"n_features:\", len(FEATURES))\nprint(\"train_X:\", X_all.shape, \"test_X:\", X_test.shape, \"| USE_LOG_TARGET:\", USE_LOG_TARGET, \"| USE_TARGET_LAGS:\", USE_TARGET_LAGS)\n\n# =========================================================\n# Util â€” Standarisasi per fold (train-only stats)\n# =========================================================\ndef standardize_fit_transform(X_tr, X_va=None):\n    mu = X_tr.mean(axis=0)\n    sigma = X_tr.std(axis=0, ddof=0).replace(0, 1.0)\n    X_tr_std = (X_tr - mu) / sigma\n    if X_va is None:\n        return X_tr_std, mu, sigma, None\n    X_va_std = (X_va - mu) / sigma\n    return X_tr_std, mu, sigma, X_va_std\n\n# =========================================================\n# Blok 8 â€” TimeSeriesSplit CV dengan Linear vs Ridge (pilih terbaik)\n# =========================================================\ngap = steps_24h * GAP_MULTIPLIER\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n\noof_lin = np.zeros(len(X_all))\noof_ridge = np.zeros(len(X_all)) if TRY_RIDGE else None\nbest_model_name_per_fold = []\nfold = 0\n\ntest_preds_lin = []\ntest_preds_ridge = [] if TRY_RIDGE else None\n\nfor tr_idx, va_idx in tscv.split(X_all):\n    fold += 1\n    X_tr, y_tr = X_all.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = X_all.iloc[va_idx], y[va_idx]\n\n    # Standarisasi (train-only)\n    X_tr_std, mu, sigma, X_va_std = standardize_fit_transform(X_tr, X_va)\n\n    # LinearRegression\n    lin = LinearRegression(n_jobs=None)\n    lin.fit(X_tr_std, y_tr)\n    pred_va_lin = lin.predict(X_va_std)\n    oof_lin[va_idx] = pred_va_lin\n\n    # RidgeCV (opsional)\n    if TRY_RIDGE:\n        ridge = RidgeCV(alphas=RIDGE_ALPHAS, store_cv_values=False)\n        ridge.fit(X_tr_std, y_tr)\n        pred_va_ridge = ridge.predict(X_va_std)\n        oof_ridge[va_idx] = pred_va_ridge\n\n    # Evaluasi di ruang asli\n    def to_raw(z):\n        return np.clip(np.expm1(z) if USE_LOG_TARGET else z, 0, None)\n\n    mse_lin = mean_squared_error(np.expm1(y_va) if USE_LOG_TARGET else y_va, to_raw(pred_va_lin))\n    if TRY_RIDGE:\n        mse_ridge = mean_squared_error(np.expm1(y_va) if USE_LOG_TARGET else y_va, to_raw(pred_va_ridge))\n        if mse_ridge <= mse_lin:\n            best_model_name_per_fold.append((\"ridge\", getattr(ridge, \"alpha_\", None)))\n        else:\n            best_model_name_per_fold.append((\"linear\", None))\n    else:\n        best_model_name_per_fold.append((\"linear\", None))\n\n    # Predict test per model (pakai scaler fold)\n    X_test_std = (X_test - mu) / sigma\n    test_preds_lin.append(lin.predict(X_test_std))\n    if TRY_RIDGE:\n        test_preds_ridge.append(ridge.predict(X_test_std))\n\n    print(f\"Fold {fold} â€” MSE_lin={mse_lin:.6f}\" + (f\" | MSE_ridge={mse_ridge:.6f} (alpha~{getattr(ridge, 'alpha_', None)})\" if TRY_RIDGE else \"\"))\n\n# Pilih model berdasarkan OOF keseluruhan\ndef to_raw_arr(arr):\n    return np.clip(np.expm1(arr) if USE_LOG_TARGET else arr, 0, None)\n\ncv_mse_lin = mean_squared_error(y_raw, to_raw_arr(oof_lin))\nprint(f\"\\nCV MSE LinearRegression (OOF, raw) = {cv_mse_lin:.6f}\")\n\nif TRY_RIDGE:\n    cv_mse_ridge = mean_squared_error(y_raw, to_raw_arr(oof_ridge))\n    print(f\"CV MSE RidgeCV (OOF, raw)         = {cv_mse_ridge:.6f}\")\n    use_ridge = (cv_mse_ridge <= cv_mse_lin)\nelse:\n    use_ridge = False\n\nprint(\">> Model terpilih:\", \"RidgeCV\" if use_ridge else \"LinearRegression\")\n\n# Blend test preds sesuai model terpilih\nstack_te = np.vstack(test_preds_ridge if use_ridge else test_preds_lin)\npred_test_cvens_log = np.median(stack_te, axis=0)  # median fold â†’ robust\npred_test_cvens = to_raw_arr(pred_test_cvens_log)\n\n# =========================================================\n# Blok 9 â€” Train Full Model di seluruh TRAIN (untuk submit_full)\n# =========================================================\n# Fit scaler full\nX_all_std, mu_full, sigma_full, _ = standardize_fit_transform(X_all)\n\nif use_ridge:\n    final_model = RidgeCV(alphas=RIDGE_ALPHAS, store_cv_values=False)\nelse:\n    final_model = LinearRegression()\n\nfinal_model.fit(X_all_std, y)\nX_test_std_full = (X_test - mu_full) / sigma_full\npred_test_full_log = final_model.predict(X_test_std_full)\npred_test_full = to_raw_arr(pred_test_full_log)\n\nprint(\"Pred test (full)  mean/std:\", float(pred_test_full.mean()),  float(pred_test_full.std()))\nprint(\"Pred test (cvens) mean/std:\", float(pred_test_cvens.mean()), float(pred_test_cvens.std()))\n\n# =========================================================\n# Blok 10 â€” Submission (pakai CV-median by default)\n# =========================================================\nFINAL_PRED = pred_test_cvens  # bisa ganti ke pred_test_full jika ingin\n\nsubmission = pd.DataFrame({\n    \"Record number\": test[IDCOL].values,\n    \"Turbidity\": FINAL_PRED\n})\n# jaga urutan sample\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n# =========================================================\n# Blok 11 â€” Artefak (model, scaler, OOF, koefisien)\n# =========================================================\nOUT_DIR = Path(\"./outputs_linear\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# simpan pilihan model & scaler stats\njoblib.dump({\"model\": final_model, \"mu\": mu_full, \"sigma\": sigma_full, \"features\": FEATURES, \"use_log_target\": USE_LOG_TARGET}, OUT_DIR / \"linear_full.pkl\")\n\n# submissions\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\npd.DataFrame({\"Record number\": test[IDCOL].values, \"Turbidity\": pred_test_full}).to_csv(\n    OUT_DIR / \"submission_full_model.csv\", index=False\n)\npd.DataFrame({\"Record number\": test[IDCOL].values, \"Turbidity\": pred_test_cvens}).to_csv(\n    OUT_DIR / \"submission_cv_ensemble.csv\", index=False\n)\n\n# OOF simpan\noof_best = oof_ridge if use_ridge else oof_lin\noof_best_raw = to_raw_arr(oof_best)\noof_df = train_fe[[IDCOL, \"Timestamp\", TARGET]].copy()\noof_df[\"oof_pred\"] = oof_best_raw[:len(oof_df)]\noof_df.to_csv(OUT_DIR / \"oof_predictions.csv\", index=False)\n\n# Koefisien (jika tersedia)\ncoef_path = OUT_DIR / \"coefficients.csv\"\ntry:\n    coefs = final_model.coef_.ravel() if hasattr(final_model, \"coef_\") else np.zeros(len(FEATURES))\n    pd.DataFrame({\"feature\": FEATURES, \"coefficient\": coefs}).to_csv(coef_path, index=False)\n    print(\"Top 20 |coef|:\")\n    print(pd.DataFrame({\"feature\": FEATURES, \"coef_abs\": np.abs(coefs)}).sort_values(\"coef_abs\", ascending=False).head(20))\nexcept Exception as e:\n    print(\"Skip saving coefficients:\", e)\n\n# Metadata ringkas\nmeta = {\n    \"cv_mse_linear\": float(cv_mse_lin),\n    \"cv_mse_ridge\": float(cv_mse_ridge) if TRY_RIDGE else None,\n    \"model_chosen\": \"RidgeCV\" if use_ridge else \"LinearRegression\",\n    \"n_splits\": int(n_splits),\n    \"gap_steps\": int(steps_24h * GAP_MULTIPLIER),\n    \"gap_hours\": float((steps_24h * GAP_MULTIPLIER) / steps_per_hour),\n    \"steps_per_hour\": int(steps_per_hour),\n    \"n_features\": int(len(FEATURES)),\n    \"use_log_target\": bool(USE_LOG_TARGET),\n    \"use_target_lags\": bool(USE_TARGET_LAGS)\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\nprint(\"âœ… Artefak disimpan di:\", OUT_DIR.resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:20:49.398345Z","iopub.execute_input":"2025-11-01T04:20:49.398910Z","iopub.status.idle":"2025-11-01T04:21:02.178619Z","shell.execute_reply.started":"2025-11-01T04:20:49.398889Z","shell.execute_reply":"2025-11-01T04:21:02.177858Z"}},"outputs":[{"name":"stdout","text":"ðŸ§¹ Dropped 6066 rows with NaN in 'Turbidity' from train.\nsteps/hour=6 | 1h=6 6h=36 12h=72 24h=144 48h=288\nn_features: 262\ntrain_X: (47346, 262) test_X: (14610, 262) | USE_LOG_TARGET: True | USE_TARGET_LAGS: False\nFold 1 â€” MSE_lin=1492.127336 | MSE_ridge=1136.228456 (alpha~0.09999999999999999)\nFold 2 â€” MSE_lin=52.318842 | MSE_ridge=52.304153 (alpha~0.0031622776601683794)\nFold 3 â€” MSE_lin=12.451476 | MSE_ridge=12.442850 (alpha~0.01)\nFold 4 â€” MSE_lin=11.219129 | MSE_ridge=11.210151 (alpha~0.09999999999999999)\nFold 5 â€” MSE_lin=497.478164 | MSE_ridge=501.076824 (alpha~0.09999999999999999)\n\nCV MSE LinearRegression (OOF, raw) = 345.126921\nCV MSE RidgeCV (OOF, raw)         = 286.404835\n>> Model terpilih: RidgeCV\nPred test (full)  mean/std: 4.926139963461877 3.8387585812347904\nPred test (cvens) mean/std: 29.8006544378608 19.912590105090516\nâœ… submission.csv dibuat!\n   Record number  Turbidity\n0          54916  19.768608\n1          54917  16.483177\n2          54918  17.864685\n3          54919  17.372715\n4          54920  18.698157\n5          54921  23.606258\n6          54922  19.734915\n7          54923  20.066576\n8          54924  21.814589\n9          54925  21.589713\nTop 20 |coef|:\n                                       feature  coef_abs\n30                                   ph_x_cond  4.929002\n253                Dissolved Oxygen_ewm24_mean  2.438856\n250                               pH_ewm6_mean  1.292791\n32                      Specific Conductance_z  1.280486\n0                         Specific Conductance  1.280486\n42                   Specific Conductance_lag1  1.257920\n44                  Specific Conductance_lag36  1.033967\n244              Average Water Speed_ewm6_mean  0.947107\n252                 Dissolved Oxygen_ewm6_mean  0.917810\n159              Dissolved Oxygen_roll288_mean  0.857227\n249  Dissolved Oxygen (%Saturation)_ewm24_mean  0.845000\n243            Specific Conductance_ewm24_mean  0.795986\n251                              pH_ewm24_mean  0.641463\n255                     Temperature_ewm24_mean  0.636602\n245             Average Water Speed_ewm24_mean  0.622973\n99            Average Water Speed_roll288_mean  0.619919\n254                      Temperature_ewm6_mean  0.580875\n84           Specific Conductance_roll288_mean  0.540355\n223                   Specific Conductance_d6h  0.509585\n141                            pH_roll144_mean  0.483331\nâœ… Artefak disimpan di: /kaggle/working/outputs_linear\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# =========================================================\n# SARIMAX Turbidity â€” End-to-End with Exogenous\n# =========================================================\nimport os, gc, math, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\n# Statsmodels (SARIMAX)\ntry:\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"statsmodels\"])\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nfrom sklearn.metrics import mean_squared_error\n\n# ===== Paths (Kaggle-style) =====\nKAGGLE_DIR = Path(\"/kaggle/input/dataset-axion\")\nTRAIN_PATH = KAGGLE_DIR / \"train.csv\"\nTEST_PATH  = KAGGLE_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = KAGGLE_DIR / \"sample_submission.csv\"\n\n# Fallback lokal\nif not TRAIN_PATH.exists():\n    TRAIN_PATH = Path(\"./train.csv\")\n    TEST_PATH  = Path(\"./test.csv\")\n    SAMPLE_SUB_PATH = Path(\"./sample_submission.csv\")\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"Turbidity\"\nIDCOL  = \"Record number\"\nDIR_COL = \"Average Water Direction\"  # degrees [0,360)\n\n# Top-8 fitur sensor (raw)\nTOP_CORR_RAW = [\n    \"Specific Conductance\",\n    \"Average Water Speed\",\n    \"Salinity\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Dissolved Oxygen\",\n    \"Temperature\",\n    \"Chlorophyll\",\n]\n\n# ---------- Opsi ----------\nUSE_LOG_TARGET      = True     # latih di log1p(y)\nUSE_DIR_FEATURES    = True     # tambah sin/cos & u,v dari arah arus\nQ_LOW, Q_HIGH       = 0.005, 0.995   # capping quantiles (train-only)\nVAL_FRAC            = 0.2      # validasi holdout 20% terakhir dari train\nMAX_SEASON_S        = 240      # batasi s agar training tidak terlalu berat\nPRINT_TOP_K_AIC     = 5        # tampilkan kandidat AIC terbaik\n\n# =========================================================\n# Blok 1 â€” Load & basic checks\n# =========================================================\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SAMPLE_SUB_PATH)\n\nfor df in (train, test):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n\ntrain = train.sort_values(\"Timestamp\").reset_index(drop=True)\ntest  = test.sort_values(\"Timestamp\").reset_index(drop=True)\n\n# Pastikan kolom minimal tersedia\nneed_train = TOP_CORR_RAW + [TARGET, \"Timestamp\", IDCOL]\nneed_test  = TOP_CORR_RAW + [\"Timestamp\", IDCOL]\nfor c in need_train:\n    if c not in train.columns:\n        print(\"âš ï¸ Missing train col:\", c)\nfor c in need_test:\n    if c not in test.columns:\n        print(\"âš ï¸ Missing test col :\", c)\n\n# =========================================================\n# Blok 1a â€” Drop target NaN\n# =========================================================\ntrain[TARGET] = pd.to_numeric(train[TARGET], errors=\"coerce\")\nn0 = len(train)\ntrain = train[train[TARGET].notna()].reset_index(drop=True)\nprint(f\"ðŸ§¹ Dropped {n0-len(train)} rows with NaN target.\")\n\n# =========================================================\n# Blok 2 â€” Estimasi resolusi waktu â†’ steps/hour & s (24 jam)\n# =========================================================\ndef estimate_steps_per_hour(df, time_col=\"Timestamp\"):\n    dt = df[time_col].diff().dt.total_seconds().dropna()\n    if len(dt) == 0:\n        return 1\n    med = np.median(dt)\n    if not np.isfinite(med) or med <= 0:\n        return 1\n    return int(max(1, round(3600.0/med)))\n\nsteps_per_hour = estimate_steps_per_hour(train)\nsteps_24h = max(1, 24 * steps_per_hour)\ns_period  = steps_24h if steps_24h <= MAX_SEASON_S else None  # clamp seasonality if huge\n\nprint(f\"steps/hour={steps_per_hour} | seasonal_s={s_period if s_period else 'None'}\")\n\n# =========================================================\n# Blok 3 â€” Outlier capping (train-only bounds) untuk exog\n# =========================================================\ndef compute_caps(ref_df, cols, q_low, q_high):\n    bounds = {}\n    for c in cols:\n        lo, hi = ref_df[c].quantile([q_low, q_high])\n        if not np.isfinite(lo): lo = ref_df[c].min()\n        if not np.isfinite(hi): hi = ref_df[c].max()\n        bounds[c] = (lo, hi)\n    return bounds\n\ndef apply_caps(df, bounds):\n    out = df.copy()\n    for c, (lo, hi) in bounds.items():\n        if c in out.columns:\n            out[c] = out[c].clip(lo, hi)\n    return out\n\ncaps = compute_caps(train, TOP_CORR_RAW, Q_LOW, Q_HIGH)\ntrain = apply_caps(train, caps)\ntest  = apply_caps(test,  caps)\n\n# =========================================================\n# Blok 4 â€” Flag missingness & Imputasi time-aware (TRAIN & TEST terpisah)\n# =========================================================\nIMP_COLS = TOP_CORR_RAW + ([DIR_COL] if (USE_DIR_FEATURES and DIR_COL in train.columns) else [])\n\n# pH & Direction sanitasi\ntrain[\"pH\"] = train[\"pH\"].clip(0, 14)\ntest[\"pH\"]  = test[\"pH\"].clip(0, 14)\nif USE_DIR_FEATURES and DIR_COL in train.columns:\n    train[DIR_COL] = (train[DIR_COL] % 360).clip(0, 360)\nif USE_DIR_FEATURES and DIR_COL in test.columns:\n    test[DIR_COL]  = (test[DIR_COL]  % 360).clip(0, 360)\n\n# median TRAIN-only\ntrain_medians = {c: train[c].median(skipna=True) for c in IMP_COLS}\n\ndef impute_train_timeaware(df, cols, roll_win, ffill_limit, med_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        # rolling median masa lalu (shift 1)\n        past_med = s.shift(1).rolling(window=roll_win, min_periods=1).median()\n        s = s.where(~s.isna(), past_med)\n        s = s.ffill(limit=ffill_limit)            # â‰¤ 6 jam\n        s = s.fillna(med_map.get(c, s.median()))  # fallback median TRAIN\n        out[c] = s\n    return out\n\ndef impute_test_seeded(df, cols, ffill_limit, med_map, seed_map):\n    out = df.copy()\n    for c in cols:\n        s = out[c].copy()\n        if len(s)>0 and pd.isna(s.iloc[0]) and pd.notna(seed_map.get(c, np.nan)):\n            s.iloc[0] = seed_map[c]\n        s = s.ffill(limit=ffill_limit)\n        s = s.fillna(med_map.get(c, s.median()))\n        out[c] = s\n    return out\n\n# imputasi train\nsteps_6h = max(1, 6*steps_per_hour)\ntrain_imp = impute_train_timeaware(train, IMP_COLS, roll_win=steps_6h, ffill_limit=steps_6h, med_map=train_medians)\n\n# seed untuk test: last-known dari train imputasi\nlast_known_from_train = {c: (train_imp[c].iloc[-1] if pd.notna(train_imp[c].iloc[-1]) else train_medians[c]) for c in IMP_COLS}\n\n# imputasi test\ntest_imp = impute_test_seeded(test, IMP_COLS, ffill_limit=steps_6h, med_map=train_medians, seed_map=last_known_from_train)\n\n# =========================================================\n# Blok 5 â€” Exogenous assembly (+ opsi fitur arah)\n# =========================================================\ndef add_dir_features(df):\n    if (DIR_COL in df.columns):\n        rad = np.deg2rad((df[DIR_COL] % 360).fillna(0.0))\n        dir_sin = np.sin(rad)\n        dir_cos = np.cos(rad)\n        u = df[\"Average Water Speed\"] * dir_cos\n        v = df[\"Average Water Speed\"] * dir_sin\n        return pd.concat([df, \n                          pd.DataFrame({\"dir_sin\":dir_sin, \"dir_cos\":dir_cos, \"u\":u, \"v\":v}, index=df.index)\n                         ], axis=1)\n    return df\n\ntrain_imp = add_dir_features(train_imp.copy())\ntest_imp  = add_dir_features(test_imp.copy())\n\nEXOG_COLS = TOP_CORR_RAW + ([\"dir_sin\",\"dir_cos\",\"u\",\"v\"] if (USE_DIR_FEATURES and \"u\" in train_imp.columns) else [])\n\n# Standarisasi exog pakai TRAIN-only\nmu_exog = train_imp[EXOG_COLS].mean(axis=0)\nsd_exog = train_imp[EXOG_COLS].std(axis=0).replace(0, 1.0)\n\nX_train = (train_imp[EXOG_COLS] - mu_exog) / sd_exog\nX_test  = (test_imp[EXOG_COLS]  - mu_exog) / sd_exog\n\n# =========================================================\n# Blok 6 â€” Target transform (opsional)\n# =========================================================\ny_train_raw = train[TARGET].values\ny_train = np.log1p(y_train_raw) if USE_LOG_TARGET else y_train_raw\n\n# Index waktu agar stabil\ny_index = pd.Series(y_train, index=train_imp[\"Timestamp\"])\nX_train.index = train_imp[\"Timestamp\"]\nX_test.index  = test_imp[\"Timestamp\"]\n\nprint(\"Exog cols:\", len(EXOG_COLS), \"| Train:\", X_train.shape, \"| Test:\", X_test.shape, \"| LogTarget:\", USE_LOG_TARGET)\n\n# =========================================================\n# Blok 7 â€” Kandidat SARIMAX & validasi holdout\n# =========================================================\n# Seasonal period s\nuse_season = (s_period is not None and s_period >= 2)\n\n# Kandidat non-seasonal (p,d,q)\npdq_list = [(1,1,0), (0,1,1), (1,1,1), (2,1,1)]\n\n# Kandidat seasonal (P,D,Q,s) â€” jika dipakai\nseasonal_list = [(0,0,0,0)]\nif use_season:\n    seasonal_list += [(1,0,1,s_period), (0,1,1,s_period)]\n\n# Split holdout\nn = len(y_index)\nsplit = int(max(1, round((1.0 - VAL_FRAC) * n)))\ny_tr, y_va = y_index.iloc[:split], y_index.iloc[split:]\nX_tr, X_va = X_train.iloc[:split], X_train.iloc[split:]\n\ndef safe_fit(pdq, sdq):\n    try:\n        if sdq[3] == 0:  # no seasonality\n            seasonal_order = (0,0,0,0)\n        else:\n            seasonal_order = sdq\n        model = SARIMAX(\n            y_tr,\n            exog=X_tr,\n            order=pdq,\n            seasonal_order=seasonal_order,\n            trend=\"c\",\n            enforce_stationarity=False,\n            enforce_invertibility=False\n        )\n        res = model.fit(disp=False, maxiter=200)\n        # forecast pada validasi\n        fc = res.get_forecast(steps=len(y_va), exog=X_va).predicted_mean\n        # inverse & clip\n        pred_va = np.expm1(fc.values) if USE_LOG_TARGET else fc.values\n        pred_va = np.clip(pred_va, 0, None)\n        y_va_raw = np.expm1(y_va.values) if USE_LOG_TARGET else y_va.values\n        mse = mean_squared_error(y_va_raw, pred_va)\n        aic = res.aic\n        return {\"pdq\": pdq, \"sdq\": seasonal_order, \"mse\": mse, \"aic\": aic, \"res\": res}\n    except Exception as e:\n        return None\n\nresults = []\nfor pdq in pdq_list:\n    for sdq in seasonal_list:\n        out = safe_fit(pdq, sdq)\n        if out is not None:\n            results.append(out)\n        print(f\"try pdq={pdq}, sdq={sdq} =>\", \"OK\" if out else \"fail\")\n\nif len(results) == 0:\n    raise RuntimeError(\"Semua kandidat SARIMAX gagal fit. Coba kecilkan dimensi exog/ matikan seasonality.\")\n\n# rangkum & pilih terbaik (berdasarkan MSE validasi)\nresults_sorted = sorted(results, key=lambda d: (d[\"mse\"], d[\"aic\"]))\nprint(\"\\nTop candidates (by MSE):\")\nfor r in results_sorted[:PRINT_TOP_K_AIC]:\n    print(f\"  pdq={r['pdq']}, sdq={r['sdq']}, MSE={r['mse']:.4f}, AIC={r['aic']:.1f}\")\n\nbest = results_sorted[0]\nprint(f\"\\nâœ… Best (holdout): pdq={best['pdq']}, sdq={best['sdq']}, MSE_val={best['mse']:.6f}, AIC={best['aic']:.1f}\")\n\n# =========================================================\n# Blok 8 â€” Refit di FULL TRAIN & Forecast TEST\n# =========================================================\ndef fit_full_and_forecast(pdq, sdq):\n    model = SARIMAX(\n        y_index,             # full train (log/linear sesuai USE_LOG_TARGET)\n        exog=X_train,\n        order=pdq,\n        seasonal_order=sdq,\n        trend=\"c\",\n        enforce_stationarity=False,\n        enforce_invertibility=False\n    )\n    res = model.fit(disp=False, maxiter=300)\n    fc_test = res.get_forecast(steps=len(X_test), exog=X_test).predicted_mean\n    pred_test = np.expm1(fc_test.values) if USE_LOG_TARGET else fc_test.values\n    pred_test = np.clip(pred_test, 0, None)\n    return res, pred_test\n\nres_full, pred_test = fit_full_and_forecast(best[\"pdq\"], best[\"sdq\"])\n\nprint(\"Pred test mean/std:\", float(np.mean(pred_test)), float(np.std(pred_test)))\n\n# =========================================================\n# Blok 9 â€” Submission\n# =========================================================\nsubmission = pd.DataFrame({\n    \"Record number\": test[IDCOL].values,\n    \"Turbidity\": pred_test\n})\n# jaga urutan sample\nif \"Record number\" in sub.columns:\n    submission = sub[[\"Record number\"]].merge(submission, on=\"Record number\", how=\"left\")\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv dibuat!\")\nprint(submission.head(10))\n\n# =========================================================\n# Blok 10 â€” Artefak & Metadata\n# =========================================================\nOUT_DIR = Path(\"./outputs_sarimax\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nsubmission.to_csv(OUT_DIR / \"submission.csv\", index=False)\n\n# Simpan ringkas parameter & statistik\nmeta = {\n    \"use_log_target\": bool(USE_LOG_TARGET),\n    \"exog_cols\": EXOG_COLS,\n    \"n_exog\": int(len(EXOG_COLS)),\n    \"steps_per_hour\": int(steps_per_hour),\n    \"seasonal_period\": int(s_period) if s_period else None,\n    \"best_pdq\": list(best[\"pdq\"]),\n    \"best_sdq\": list(best[\"sdq\"]),\n    \"val_mse\": float(best[\"mse\"]),\n    \"aic\": float(best[\"aic\"]),\n    \"caps_quantiles\": [Q_LOW, Q_HIGH],\n    \"caps_bounds\": {k: [float(v[0]), float(v[1])] for k, v in caps.items()}\n}\nwith open(OUT_DIR / \"metadata.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)\n\n# simpan exog scaler\npd.Series(EXOG_COLS).to_csv(OUT_DIR / \"exog_features.txt\", index=False, header=False)\npd.concat([mu_exog.rename(\"mu\"), sd_exog.rename(\"sigma\")], axis=1).to_csv(OUT_DIR / \"exog_scaler.csv\")\n\nprint(\"ðŸ“¦ Artefak disimpan di:\", OUT_DIR.resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T04:36:53.948887Z","iopub.execute_input":"2025-11-01T04:36:53.949476Z"}},"outputs":[{"name":"stdout","text":"ðŸ§¹ Dropped 6066 rows with NaN target.\nsteps/hour=6 | seasonal_s=144\nExog cols: 12 | Train: (47382, 12) | Test: (14610, 12) | LogTarget: True\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n","output_type":"stream"},{"name":"stdout","text":"try pdq=(1, 1, 0), sdq=(0, 0, 0, 0) => OK\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ===== Tambahan: set frekuensi waktu agar SARIMAX tidak warning =====\nimport pandas as pd\n\ndef freq_from_steps_per_hour(sph: int) -> str:\n    step_sec = int(round(3600 / max(1, sph)))\n    if step_sec % 60 == 0:\n        return f\"{step_sec // 60}T\"   # menit\n    return f\"{step_sec}S\"             # detik\n\n# Coba infer dari data dulu; jika gagal, fallback dari steps/hour\nfreq_infer = pd.infer_freq(train_imp[\"Timestamp\"])\nfreq = freq_infer if freq_infer is not None else freq_from_steps_per_hour(steps_per_hour)\nprint(\"Using freq:\", freq)\n\n# Seri target dengan index waktu\ny_series = pd.Series(y_train, index=train_imp[\"Timestamp\"]).asfreq(freq)\n\n# Exog: pastikan index waktu & freq sama, lalu reindex ke target\nX_train = X_train.copy()\nX_test  = X_test.copy()\nX_train.index = train_imp[\"Timestamp\"]\nX_test.index  = test_imp[\"Timestamp\"]\n\nX_train = X_train.asfreq(freq)\nX_test  = X_test.asfreq(freq)\n\n# Selaraskan exog ke index target; isi potensi celah exog dengan ffill/bfill lokal\nX_train = X_train.reindex(y_series.index).ffill().bfill()\n\n# Buang baris target yang NaN (jika asfreq menambah grid kosong di tengah)\nvalid_idx = y_series.dropna().index\ny_index = y_series.loc[valid_idx]\nX_train = X_train.loc[valid_idx]\n\n# (opsional) pastikan test exog juga rapih\nX_test = X_test.ffill().bfill()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}