{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13572673,"sourceType":"datasetVersion","datasetId":8622209}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv(\"/kaggle/input/axion-2025-dataset/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/axion-2025-dataset/test.csv\")\ndf.info()\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.170332Z","iopub.execute_input":"2025-11-01T03:51:23.171026Z","iopub.status.idle":"2025-11-01T03:51:23.305103Z","shell.execute_reply.started":"2025-11-01T03:51:23.171001Z","shell.execute_reply":"2025-11-01T03:51:23.304433Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 53448 entries, 0 to 53447\nData columns (total 12 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   Timestamp                       53448 non-null  object \n 1   Record number                   53448 non-null  int64  \n 2   Average Water Speed             53229 non-null  float64\n 3   Average Water Direction         53447 non-null  float64\n 4   Chlorophyll                     51744 non-null  float64\n 5   Temperature                     44234 non-null  float64\n 6   Dissolved Oxygen                49139 non-null  float64\n 7   Dissolved Oxygen (%Saturation)  47690 non-null  float64\n 8   pH                              52355 non-null  float64\n 9   Salinity                        49481 non-null  float64\n 10  Specific Conductance            52072 non-null  float64\n 11  Turbidity                       47382 non-null  float64\ndtypes: float64(10), int64(1), object(1)\nmemory usage: 4.9+ MB\n","output_type":"stream"},{"execution_count":309,"output_type":"execute_result","data":{"text/plain":"             Timestamp  Record number  Average Water Speed  \\\n0  2023-08-04 23:00:00           1468                4.834   \n1  2023-08-04 23:30:00           1469                2.544   \n2  2023-08-04 23:00:00           1470                1.260   \n3  2023-08-04 23:30:00           1471                0.760   \n4  2023-08-04 23:00:00           1472                3.397   \n\n   Average Water Direction  Chlorophyll  Temperature  Dissolved Oxygen  \\\n0                   73.484        1.621       20.018             7.472   \n1                  106.424        1.959       19.986             7.455   \n2                  156.755        1.620       20.001             7.430   \n3                  281.754        1.761       19.983             7.419   \n4                  244.637        1.635       19.986             7.429   \n\n   Dissolved Oxygen (%Saturation)     pH  Salinity  Specific Conductance  \\\n0                         101.175  8.176    35.215                53.262   \n1                         100.884  8.175    35.209                53.254   \n2                         100.571  8.171    35.207                53.252   \n3                         100.398  8.171    35.211                53.257   \n4                         100.538  8.171    35.208                53.253   \n\n   Turbidity  \n0      2.068  \n1      1.994  \n2      2.030  \n3      1.973  \n4      1.944  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Timestamp</th>\n      <th>Record number</th>\n      <th>Average Water Speed</th>\n      <th>Average Water Direction</th>\n      <th>Chlorophyll</th>\n      <th>Temperature</th>\n      <th>Dissolved Oxygen</th>\n      <th>Dissolved Oxygen (%Saturation)</th>\n      <th>pH</th>\n      <th>Salinity</th>\n      <th>Specific Conductance</th>\n      <th>Turbidity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-08-04 23:00:00</td>\n      <td>1468</td>\n      <td>4.834</td>\n      <td>73.484</td>\n      <td>1.621</td>\n      <td>20.018</td>\n      <td>7.472</td>\n      <td>101.175</td>\n      <td>8.176</td>\n      <td>35.215</td>\n      <td>53.262</td>\n      <td>2.068</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-08-04 23:30:00</td>\n      <td>1469</td>\n      <td>2.544</td>\n      <td>106.424</td>\n      <td>1.959</td>\n      <td>19.986</td>\n      <td>7.455</td>\n      <td>100.884</td>\n      <td>8.175</td>\n      <td>35.209</td>\n      <td>53.254</td>\n      <td>1.994</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-08-04 23:00:00</td>\n      <td>1470</td>\n      <td>1.260</td>\n      <td>156.755</td>\n      <td>1.620</td>\n      <td>20.001</td>\n      <td>7.430</td>\n      <td>100.571</td>\n      <td>8.171</td>\n      <td>35.207</td>\n      <td>53.252</td>\n      <td>2.030</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-08-04 23:30:00</td>\n      <td>1471</td>\n      <td>0.760</td>\n      <td>281.754</td>\n      <td>1.761</td>\n      <td>19.983</td>\n      <td>7.419</td>\n      <td>100.398</td>\n      <td>8.171</td>\n      <td>35.211</td>\n      <td>53.257</td>\n      <td>1.973</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-08-04 23:00:00</td>\n      <td>1472</td>\n      <td>3.397</td>\n      <td>244.637</td>\n      <td>1.635</td>\n      <td>19.986</td>\n      <td>7.429</td>\n      <td>100.538</td>\n      <td>8.171</td>\n      <td>35.208</td>\n      <td>53.253</td>\n      <td>1.944</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":309},{"cell_type":"code","source":"# Cek Top Corr\nimport numpy as np\nimport pandas as pd\n\n# opsional (p-value); fallback ke Pandas-only jika SciPy tak tersedia\ntry:\n    from scipy.stats import pearsonr\n    SCIPY_OK = True\nexcept Exception:\n    SCIPY_OK = False\n\n# Path sesuai instruksi\ndf_train = pd.read_csv(\"/kaggle/input/axion-2025-dataset/train.csv\")\ndf_test  = pd.read_csv(\"/kaggle/input/axion-2025-dataset/test.csv\")  # tidak dipakai untuk korelasi\n\n# Blok 2 — Fungsi korelasi target-wise (pairwise NaN-safe)\ndef pearson_target_corr(df, target=\"Turbidity\", exclude_cols=(\"Timestamp\", \"Record number\")):\n    # pilih kolom numerik\n    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    # drop kolom yang ingin dikecualikan + target sendiri\n    features = [c for c in num_cols if c != target and c not in exclude_cols]\n\n    rows = []\n    for col in features:\n        pair = df[[target, col]].dropna()\n        n = len(pair)\n        # hindari error pada kolom konstan atau sampel terlalu kecil\n        if n < 3 or pair[col].nunique(dropna=True) < 2 or pair[target].nunique(dropna=True) < 2:\n            r, p = np.nan, np.nan\n        else:\n            if SCIPY_OK:\n                r, p = pearsonr(pair[target].values, pair[col].values)\n            else:\n                # fallback tanpa p-value\n                r = pair[target].corr(pair[col], method=\"pearson\")\n                p = np.nan\n        rows.append({\n            \"feature\": col,\n            \"n_pair\": n,\n            \"pearson_r\": r,\n            \"abs_r\": abs(r) if pd.notnull(r) else np.nan,\n            \"p_value\": p\n        })\n\n    out = pd.DataFrame(rows).sort_values(\"abs_r\", ascending=False, na_position=\"last\").reset_index(drop=True)\n    return out\n\n# Blok 3 — Jalankan & tampilkan hasil\ncorr_tbl = pearson_target_corr(df_train, target=\"Turbidity\", exclude_cols=(\"Timestamp\",\"Record number\"))\ndisplay_cols = [\"feature\", \"n_pair\", \"pearson_r\", \"p_value\"]\nprint(corr_tbl[display_cols].to_string(index=False))\n\n# opsional: simpan ke CSV\ncorr_tbl.to_csv(\"turbidity_pearson_correlations.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.306318Z","iopub.execute_input":"2025-11-01T03:51:23.306529Z","iopub.status.idle":"2025-11-01T03:51:23.484958Z","shell.execute_reply.started":"2025-11-01T03:51:23.306512Z","shell.execute_reply":"2025-11-01T03:51:23.484120Z"}},"outputs":[{"name":"stdout","text":"                       feature  n_pair  pearson_r       p_value\n          Specific Conductance   46317  -0.179238  0.000000e+00\n           Average Water Speed   47349   0.173904 3.912140e-318\n                      Salinity   44413  -0.168092 8.846373e-279\nDissolved Oxygen (%Saturation)   42444  -0.158935 3.912737e-238\n                            pH   46577   0.113211 1.124025e-132\n              Dissolved Oxygen   43550  -0.105243 1.721306e-107\n                   Temperature   38260  -0.066157  2.224704e-38\n                   Chlorophyll   45738   0.031591  1.401403e-11\n       Average Water Direction   47381  -0.008140  7.641617e-02\n","output_type":"stream"}],"execution_count":310},{"cell_type":"code","source":"# Blok 1 — Setup + Paths + Load Data (gabung)\nimport os, sys, gc, warnings, math, json, random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nSEED = 42\ndef set_seed(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\nset_seed(SEED)\n\n# Paths\nDATA_DIR = Path(\"/kaggle/input/axion-2025-dataset\")\nWORK_DIR = Path(\"/kaggle/working\")\nTRAIN_PATH = DATA_DIR / \"train.csv\"\nTEST_PATH  = DATA_DIR / \"test.csv\"\nSAMPLE_SUB_PATH = DATA_DIR / \"sample_submission.csv\"\n\n# Kolom penting\nTARGET_COL = \"Turbidity\"\nID_COL     = \"Record number\"\nTIME_COL   = \"Timestamp\"\nRAW_DIR_COL= \"Average Water Direction\"   # circular (0-360)\n\n# Seleksi fitur (di Blok 6 semua ranking dicetak penuh)\n# 1 = Pearson, 2 = Spearman, [1,2] = gabungan (porsi adil)\nFEATURE_METHODS = [1, 2]\nTOP_K = 25\nALWAYS_KEEP = []  # misal: [\"hour_sin\",\"hour_cos\"]\n\n# Imputasi & winsor\nKNN_K = 5\nWINSOR_Q_LOW, WINSOR_Q_HIGH = 0.005, 0.995\n\n# Load\ndf_train = pd.read_csv(TRAIN_PATH)\ndf_test  = pd.read_csv(TEST_PATH)\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\n# pastikan kolom unik\ndf_train = df_train.loc[:, ~df_train.columns.duplicated()].copy()\ndf_test  = df_test.loc[:,  ~df_test.columns.duplicated()].copy()\n\nprint(\"Loaded:\", df_train.shape, df_test.shape, sample_sub.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.485729Z","iopub.execute_input":"2025-11-01T03:51:23.485968Z","iopub.status.idle":"2025-11-01T03:51:23.617329Z","shell.execute_reply.started":"2025-11-01T03:51:23.485952Z","shell.execute_reply":"2025-11-01T03:51:23.616704Z"}},"outputs":[{"name":"stdout","text":"Loaded: (53448, 12) (14610, 11) (14610, 2)\n","output_type":"stream"}],"execution_count":311},{"cell_type":"code","source":"# Blok 2 — Utilities (metrik, normalisasi angka koma→titik, FE waktu, winsor, korelasi)\nfrom scipy.stats import pearsonr, spearmanr\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)\n\ndef mape(y_true, y_pred, eps=1e-6):\n    y_true = np.array(y_true, dtype=float)\n    y_pred = np.array(y_pred, dtype=float)\n    denom = np.clip(np.abs(y_true), eps, None)\n    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n\ndef smape(y_true, y_pred, eps=1e-6):\n    y_true = np.array(y_true, dtype=float)\n    y_pred = np.array(y_pred, dtype=float)\n    denom = np.clip((np.abs(y_true) + np.abs(y_pred)), eps, None) / 2.0\n    return np.mean(np.abs(y_pred - y_true) / denom) * 100.0\n\ndef normalize_comma_decimals(df: pd.DataFrame, cols):\n    d = df.copy()\n    for c in cols:\n        if c in d.columns:\n            if d[c].dtype == object:\n                d[c] = d[c].str.replace(\" \", \"\", regex=False)\n                d[c] = d[c].str.replace(\",\", \".\", regex=False)\n            d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n    return d\n\ndef add_time_features(df: pd.DataFrame, time_col: str = \"Timestamp\"):\n    d = df.copy()\n    d[time_col] = pd.to_datetime(d[time_col], errors=\"coerce\", utc=False)\n    d[\"year\"] = d[time_col].dt.year\n    d[\"month\"] = d[time_col].dt.month\n    d[\"dayofweek\"] = d[time_col].dt.dayofweek\n    d[\"hour\"] = d[time_col].dt.hour\n    d[\"dayofyear\"] = d[time_col].dt.dayofyear\n    # cyclical dari waktu (aman karena tidak bergantung sensor)\n    d[\"hour_sin\"]  = np.sin(2*np.pi*(d[\"hour\"].fillna(0)/24))\n    d[\"hour_cos\"]  = np.cos(2*np.pi*(d[\"hour\"].fillna(0)/24))\n    d[\"month_sin\"] = np.sin(2*np.pi*((d[\"month\"].fillna(1)-1)/12))\n    d[\"month_cos\"] = np.cos(2*np.pi*((d[\"month\"].fillna(1)-1)/12))\n    d[\"doy_sin\"]   = np.sin(2*np.pi*((d[\"dayofyear\"].fillna(1)-1)/365))\n    d[\"doy_cos\"]   = np.cos(2*np.pi*((d[\"dayofyear\"].fillna(1)-1)/365))\n    return d\n\ndef make_winsor_bounds(df: pd.DataFrame, cols, q_low=0.005, q_high=0.995):\n    bounds = {}\n    for c in cols:\n        lo = df[c].quantile(q_low)\n        hi = df[c].quantile(q_high)\n        bounds[c] = (lo, hi)\n    return bounds\n\ndef corr_with_target(df, features, target, method=\"pearson\"):\n    rows = []\n    for c in features:\n        x = df[c].values\n        y = df[target].values\n        mask = np.isfinite(x) & np.isfinite(y)\n        n_pair = int(mask.sum())\n        if n_pair < 3 or np.unique(x[mask]).size < 2:\n            r, p = np.nan, np.nan\n        else:\n            if method == \"pearson\":\n                r, p = pearsonr(x[mask], y[mask])\n            elif method == \"spearman\":\n                r, p = spearmanr(x[mask], y[mask])\n            else:\n                raise ValueError(\"Unknown method\")\n        rows.append({\"feature\": c, \"n_pair\": n_pair, \"r\": r, \"abs_r\": abs(r) if pd.notnull(r) else np.nan, \"p_value\": p})\n    out = pd.DataFrame(rows).sort_values(\"abs_r\", ascending=False).reset_index(drop=True)\n    return out\n\nprint(\"Utilities ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.618234Z","iopub.execute_input":"2025-11-01T03:51:23.619118Z","iopub.status.idle":"2025-11-01T03:51:23.632840Z","shell.execute_reply.started":"2025-11-01T03:51:23.619091Z","shell.execute_reply":"2025-11-01T03:51:23.632146Z"}},"outputs":[{"name":"stdout","text":"Utilities ready\n","output_type":"stream"}],"execution_count":312},{"cell_type":"code","source":"# Blok 3 — Normalisasi angka (\",\" → \".\") untuk semua kolom numerik (train & test)\nnumeric_cols_all = [\n    \"Average Water Speed\",\n    \"Average Water Direction\",\n    \"Chlorophyll\",\n    \"Temperature\",\n    \"Dissolved Oxygen\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Salinity\",\n    \"Specific Conductance\",\n    \"Turbidity\"  # train only; di test akan diabaikan jika tak ada\n]\n\ndf_train = normalize_comma_decimals(df_train, numeric_cols_all)\ndf_test  = normalize_comma_decimals(df_test,  numeric_cols_all)\n\nprint(\"Dtypes (train numerics):\")\nprint(df_train[[c for c in numeric_cols_all if c in df_train.columns]].dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.634557Z","iopub.execute_input":"2025-11-01T03:51:23.634841Z","iopub.status.idle":"2025-11-01T03:51:23.662918Z","shell.execute_reply.started":"2025-11-01T03:51:23.634825Z","shell.execute_reply":"2025-11-01T03:51:23.662191Z"}},"outputs":[{"name":"stdout","text":"Dtypes (train numerics):\nAverage Water Speed               float64\nAverage Water Direction           float64\nChlorophyll                       float64\nTemperature                       float64\nDissolved Oxygen                  float64\nDissolved Oxygen (%Saturation)    float64\npH                                float64\nSalinity                          float64\nSpecific Conductance              float64\nTurbidity                         float64\ndtype: object\n","output_type":"stream"}],"execution_count":313},{"cell_type":"code","source":"# Blok 4 — FE waktu (aman) + kunci grup untuk imputasi slice waktu\ntrain_time = add_time_features(df_train, TIME_COL)\ntest_time  = add_time_features(df_test,  TIME_COL)\n\nfor d in (train_time, test_time):\n    d[\"_month\"] = d[\"month\"].astype(\"Int64\")\n    d[\"_hour\"]  = d[\"hour\"].astype(\"Int64\")\n\nprint(\"Time features ready. Keys for grouping added (_month, _hour).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.663753Z","iopub.execute_input":"2025-11-01T03:51:23.664091Z","iopub.status.idle":"2025-11-01T03:51:23.715737Z","shell.execute_reply.started":"2025-11-01T03:51:23.664070Z","shell.execute_reply":"2025-11-01T03:51:23.715054Z"}},"outputs":[{"name":"stdout","text":"Time features ready. Keys for grouping added (_month, _hour).\n","output_type":"stream"}],"execution_count":314},{"cell_type":"code","source":"# # Blok 5 — Imputasi BASE numerik → KNN → clip pH → winsor + diagnostik\n# base_numeric_cols = [\n#     \"Average Water Speed\",\n#     \"Average Water Direction\",\n#     \"Chlorophyll\",\n#     \"Temperature\",\n#     \"Dissolved Oxygen\",\n#     \"Dissolved Oxygen (%Saturation)\",\n#     \"pH\",\n#     \"Salinity\",\n#     \"Specific Conductance\",\n# ]\n\n# missing_before_base = df_train[base_numeric_cols].isna().sum().sort_values(ascending=False)\n# print(\"\\n[Diagnostik] Missing BASE sebelum imputasi (TRAIN):\")\n# print(missing_before_base[missing_before_base>0].to_string())\n\n# # rangkai data dengan kunci waktu\n# train_b = pd.concat([train_time[[\"_month\",\"_hour\"]], df_train[base_numeric_cols]], axis=1)\n# test_b  = pd.concat([test_time[[\"_month\",\"_hour\"]],  df_test[base_numeric_cols]],  axis=1)\n\n# # Tahap-1: median per slice waktu\n# group_keys = [\"_month\",\"_hour\"]\n# group_medians = {c: train_b.groupby(group_keys, observed=True)[c].median() for c in base_numeric_cols}\n# global_medians = {c: train_b[c].median() for c in base_numeric_cols}\n\n# def apply_group_median_fill_base(df_slice: pd.DataFrame):\n#     d = df_slice.copy()\n#     keys = list(zip(d[\"_month\"].fillna(0).astype(int), d[\"_hour\"].fillna(0).astype(int)))\n#     for c in base_numeric_cols:\n#         gm = group_medians[c]\n#         fill_vals = [gm.get((m, h), np.nan) for (m, h) in keys]\n#         fill_vals = pd.Series(fill_vals, index=d.index)\n#         d[c] = d[c].fillna(fill_vals)\n#         d[c] = d[c].fillna(global_medians[c])\n#     return d\n\n# train_b1 = apply_group_median_fill_base(train_b)\n# test_b1  = apply_group_median_fill_base(test_b)\n# missing_after_stage1_base = train_b1[base_numeric_cols].isna().sum().sort_values(ascending=False)\n\n# # Tahap-2: KNNImputer (fit di TRAIN)\n# scaler_base = StandardScaler()\n# imputer_base = KNNImputer(n_neighbors=KNN_K, weights=\"uniform\")\n\n# scaler_base.fit(train_b1[base_numeric_cols])\n# train_scaled = scaler_base.transform(train_b1[base_numeric_cols])\n# test_scaled  = scaler_base.transform(test_b1[base_numeric_cols])\n\n# imputer_base.fit(train_scaled)\n# train_imputed_scaled = imputer_base.transform(train_scaled)\n# test_imputed_scaled  = imputer_base.transform(test_scaled)\n\n# train_b2 = train_b1.copy()\n# test_b2  = test_b1.copy()\n# train_b2[base_numeric_cols] = scaler_base.inverse_transform(train_imputed_scaled)\n# test_b2[base_numeric_cols]  = scaler_base.inverse_transform(test_imputed_scaled)\n\n# # Domain-clip pH\n# for d in (train_b2, test_b2):\n#     d[\"pH\"] = d[\"pH\"].clip(0, 14)\n\n# # Winsorize ringan (bounds dari TRAIN)\n# winsor_cols = [\n#     \"Average Water Speed\",\"Chlorophyll\",\"Temperature\",\n#     \"Dissolved Oxygen\",\"Dissolved Oxygen (%Saturation)\",\"pH\",\n#     \"Salinity\",\"Specific Conductance\",\"Average Water Direction\"\n# ]\n# def apply_winsor_bounds_inplace(df_slice, bounds, cols):\n#     for c in cols:\n#         lo, hi = bounds[c]\n#         df_slice[c] = df_slice[c].clip(lo, hi)\n\n# winsor_bounds = make_winsor_bounds(train_b2, winsor_cols, WINSOR_Q_LOW, WINSOR_Q_HIGH)\n# apply_winsor_bounds_inplace(train_b2, winsor_bounds, winsor_cols)\n# apply_winsor_bounds_inplace(test_b2,  winsor_bounds, winsor_cols)\n\n# # Fallback median bila masih ada residual NaN\n# for c in base_numeric_cols:\n#     med = train_b2[c].median()\n#     train_b2[c] = train_b2[c].fillna(med)\n#     test_b2[c]  = test_b2[c].fillna(med)\n\n# missing_after_final_base = train_b2[base_numeric_cols].isna().sum().sort_values(ascending=False)\n\n# # Diagnostik lengkap fase imputasi base\n# filled_stage1 = (missing_before_base - missing_after_stage1_base).clip(lower=0)\n# filled_stage2 = (missing_after_stage1_base - missing_after_final_base).clip(lower=0)\n# filled_total  = (missing_before_base - missing_after_final_base).clip(lower=0)\n\n# print(\"\\n[Diagnostik] Imputasi BASE per kolom (TRAIN):\")\n# print(pd.DataFrame({\n#     \"before\": missing_before_base,\n#     \"after_stage1\": missing_after_stage1_base,\n#     \"after_final\": missing_after_final_base,\n#     \"filled_stage1\": filled_stage1,\n#     \"filled_stage2\": filled_stage2,\n#     \"filled_total\":  filled_total\n# }).loc[missing_before_base.index].to_string())\n\n# if int(missing_after_final_base.sum()) == 0:\n#     print(\"\\n[OK] Tidak ada missing tersisa pada BASE numerik (TRAIN).\")\n# else:\n#     print(\"\\n[Warning] Masih ada missing di BASE numerik (TRAIN):\")\n#     print(missing_after_final_base[missing_after_final_base>0].to_string())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.716614Z","iopub.execute_input":"2025-11-01T03:51:23.716809Z","iopub.status.idle":"2025-11-01T03:51:23.721857Z","shell.execute_reply.started":"2025-11-01T03:51:23.716794Z","shell.execute_reply":"2025-11-01T03:51:23.721064Z"}},"outputs":[],"execution_count":315},{"cell_type":"code","source":"# # Blok 5 — Imputasi BASE numerik (per kolom; sesuai saran) + clip pH + winsor + diagnostik\n# from sklearn.ensemble import RandomForestRegressor\n\n# base_numeric_cols = [\n#     \"Average Water Speed\",\n#     \"Average Water Direction\",\n#     \"Chlorophyll\",\n#     \"Temperature\",\n#     \"Dissolved Oxygen\",\n#     \"Dissolved Oxygen (%Saturation)\",\n#     \"pH\",\n#     \"Salinity\",\n#     \"Specific Conductance\",\n# ]\n\n# # ====== 0) Diagnostik awal ======\n# missing_before_base = df_train[base_numeric_cols].isna().sum().sort_values(ascending=False)\n# print(\"\\n[Diagnostik] Missing BASE sebelum imputasi (TRAIN):\")\n# print(missing_before_base[missing_before_base>0].to_string())\n\n# # rangkai data dengan kunci waktu & waktu-bantu (_month, _hour) yg sudah dibuat di blok sebelumnya\n# train_b = pd.concat([df_train[[TIME_COL]], train_time[[\"_month\",\"_hour\"]], df_train[base_numeric_cols]], axis=1)\n# test_b  = pd.concat([df_test[[TIME_COL]],  test_time[[\"_month\",\"_hour\"]],  df_test[base_numeric_cols]],  axis=1)\n\n# # ====== 1) Helper per-metode ======\n# def _time_interp_linear_train(s: pd.Series, t: pd.Series) -> pd.Series:\n#     d = pd.DataFrame({TIME_COL: t, \"val\": s}).sort_values(TIME_COL).set_index(TIME_COL)\n#     d[\"val\"] = d[\"val\"].interpolate(method=\"time\", limit_direction=\"both\")\n#     d[\"val\"] = d[\"val\"].ffill().bfill()\n#     return d[\"val\"].reset_index(drop=True)\n\n# def _time_interp_linear_test(train_s: pd.Series, train_t: pd.Series,\n#                              test_s: pd.Series,  test_t: pd.Series) -> pd.Series:\n#     tr = pd.DataFrame({TIME_COL: train_t, \"val\": train_s, \"_is_test\": 0})\n#     te = pd.DataFrame({TIME_COL: test_t,  \"val\": test_s,  \"_is_test\": 1})\n#     both = pd.concat([tr, te], axis=0, ignore_index=True).sort_values(TIME_COL).set_index(TIME_COL)\n#     mask_te = both[\"_is_test\"]==1\n#     both[\"val\"] = both[\"val\"].interpolate(method=\"time\", limit_direction=\"forward\")\n#     both[\"val\"] = both[\"val\"].ffill()\n#     out = both.loc[mask_te, \"val\"].reset_index(drop=True)\n#     return out\n\n# def _median_by_month(train_col: pd.Series, train_month: pd.Series,\n#                      test_col: pd.Series,  test_month: pd.Series) -> tuple[pd.Series, pd.Series]:\n#     m_map = train_col.groupby(train_month).median()\n#     g_med = float(train_col.median())\n#     tr = train_col.copy()\n#     te = test_col.copy()\n#     tr = tr.fillna(train_month.map(m_map))\n#     tr = tr.fillna(g_med)\n#     te = te.fillna(test_month.map(m_map))\n#     te = te.fillna(g_med)\n#     return tr, te\n\n# def _rolling_impute(train_s: pd.Series, train_t: pd.Series,\n#                     test_s: pd.Series,  test_t: pd.Series,\n#                     win: int = 5, how: str = \"median\") -> tuple[pd.Series, pd.Series]:\n#     # gabungkan, urut waktu → rolling past-only\n#     tr = pd.DataFrame({TIME_COL: train_t, \"val\": train_s, \"_is_test\":0})\n#     te = pd.DataFrame({TIME_COL: test_t,  \"val\": test_s,  \"_is_test\":1})\n#     both = pd.concat([tr, te], axis=0, ignore_index=True).sort_values(TIME_COL)\n#     if how == \"median\":\n#         roll = both[\"val\"].rolling(win, min_periods=1).median()\n#     else:\n#         roll = both[\"val\"].rolling(win, min_periods=1).mean()\n#     both[\"val\"] = both[\"val\"].fillna(roll)\n#     # fallback sedikit agar aman\n#     g_med = float(pd.to_numeric(train_s, errors=\"coerce\").median())\n#     both[\"val\"] = both[\"val\"].fillna(g_med)\n#     # pisah kembali\n#     tr_out = both.loc[both[\"_is_test\"]==0, \"val\"].reset_index(drop=True)\n#     te_out = both.loc[both[\"_is_test\"]==1, \"val\"].reset_index(drop=True)\n#     return tr_out, te_out\n\n# # mapping strategi sesuai \"BEST METHOD PER COLUMN\"\n# strategy = {\n#     \"Average Water Direction\": \"rf_per_column\",\n#     \"Average Water Speed\": \"median_by_month\",\n#     \"Chlorophyll\": \"rolling_median_w5\",\n#     \"Dissolved Oxygen\": \"rf_per_column\",\n#     \"Dissolved Oxygen (%Saturation)\": \"rf_per_column\",\n#     \"pH\": \"rolling_mean_w5\",\n#     \"Salinity\": \"rf_per_column\",\n#     \"Specific Conductance\": \"time_interpolate_linear\",\n#     \"Temperature\": \"time_interpolate_linear\",\n# }\n\n# # ====== 2) Tahap NON-RF — isi kolom non-RF sesuai metode ======\n# train_step = train_b[base_numeric_cols].copy()\n# test_step  = test_b[base_numeric_cols].copy()\n\n# orig_missing_train = {c: train_step[c].isna().values for c in base_numeric_cols}\n# orig_missing_test  = {c: test_step[c].isna().values  for c in base_numeric_cols}\n\n# for c in base_numeric_cols:\n#     meth = strategy.get(c, \"median_by_month\")\n#     if meth == \"time_interpolate_linear\":\n#         train_step[c] = _time_interp_linear_train(train_b[c], train_b[TIME_COL])\n#         test_step[c]  = _time_interp_linear_test(train_b[c], train_b[TIME_COL], test_b[c], test_b[TIME_COL])\n#     elif meth == \"median_by_month\":\n#         tr, te = _median_by_month(train_b[c], train_b[\"_month\"], test_b[c], test_b[\"_month\"])\n#         train_step[c], test_step[c] = tr, te\n#     elif meth == \"rolling_median_w5\":\n#         tr, te = _rolling_impute(train_b[c], train_b[TIME_COL], test_b[c], test_b[TIME_COL], win=5, how=\"median\")\n#         train_step[c], test_step[c] = tr, te\n#     elif meth == \"rolling_mean_w5\":\n#         tr, te = _rolling_impute(train_b[c], train_b[TIME_COL], test_b[c], test_b[TIME_COL], win=5, how=\"mean\")\n#         train_step[c], test_step[c] = tr, te\n#     elif meth == \"rf_per_column\":\n#         # akan diisi pada Tahap RF; sementara biarkan (akan ada fallback median sementara untuk fitur lain)\n#         pass\n#     else:\n#         # fallback aman\n#         tr, te = _median_by_month(train_b[c], train_b[\"_month\"], test_b[c], test_b[\"_month\"])\n#         train_step[c], test_step[c] = tr, te\n\n# missing_after_nonrf = train_step[base_numeric_cols].isna().sum().sort_values(ascending=False)\n\n# # ====== 3) Tahap RF per kolom ======\n# rf_cols = [c for c,m in strategy.items() if m==\"rf_per_column\" and c in base_numeric_cols]\n# # siapkan fitur RF: semua kolom base (hasil non-RF) + _month, _hour\n# Xtr_base = pd.concat([train_time[[\"_month\",\"_hour\"]].reset_index(drop=True), train_step.reset_index(drop=True)], axis=1)\n# Xte_base = pd.concat([test_time[[\"_month\",\"_hour\"]].reset_index(drop=True),  test_step.reset_index(drop=True)],  axis=1)\n\n# # Isi sementara NA prediktor dengan median TRAIN (agar model bisa fit)\n# med_map = {c: pd.to_numeric(Xtr_base[c], errors=\"coerce\").median() for c in Xtr_base.columns if c not in [\"_month\",\"_hour\"]}\n# for D in (Xtr_base, Xte_base):\n#     for c in D.columns:\n#         if c in [\"_month\",\"_hour\"]: \n#             D[c] = pd.to_numeric(D[c], errors=\"coerce\").fillna(0).astype(int)\n#         else:\n#             D[c] = pd.to_numeric(D[c], errors=\"coerce\").fillna(med_map.get(c, 0.0))\n\n# for c in rf_cols:\n#     # target y\n#     y_tr = pd.to_numeric(df_train[c], errors=\"coerce\")\n#     mask_train_known = y_tr.notna()\n#     if mask_train_known.sum() == 0:\n#         # bila seluruhnya NaN, skip dan pakai median\n#         fill_val = float(y_tr.median() if pd.notna(y_tr.median()) else 0.0)\n#         train_step[c] = train_step[c].fillna(fill_val)\n#         test_step[c]  = test_step[c].fillna(fill_val)\n#         continue\n\n#     # fitur = semua prediktor kecuali target kolom c\n#     feat_cols_rf = [col for col in Xtr_base.columns if col not in [c]]\n#     X_tr_rf = Xtr_base.loc[mask_train_known, feat_cols_rf]\n#     y_tr_rf = y_tr.loc[mask_train_known].astype(float)\n\n#     rf = RandomForestRegressor(\n#         n_estimators=300, max_depth=12, min_samples_leaf=3,\n#         random_state=SEED, n_jobs=-1\n#     )\n#     rf.fit(X_tr_rf, y_tr_rf)\n\n#     # prediksi untuk baris yang tadinya NA saja\n#     if orig_missing_train[c].any():\n#         idx_na_tr = np.where(orig_missing_train[c])[0]\n#         if len(idx_na_tr):\n#             pred_tr = rf.predict(Xtr_base.iloc[idx_na_tr][feat_cols_rf])\n#             train_step.loc[idx_na_tr, c] = pred_tr.astype(float)\n#     if orig_missing_test[c].any():\n#         idx_na_te = np.where(orig_missing_test[c])[0]\n#         if len(idx_na_te):\n#             pred_te = rf.predict(Xte_base.iloc[idx_na_te][feat_cols_rf])\n#             test_step.loc[idx_na_te, c] = pred_te.astype(float)\n\n# # ====== 4) Domain-clip pH & Winsorize ringan ======\n# for d in (train_step, test_step):\n#     d[\"pH\"] = pd.to_numeric(d[\"pH\"], errors=\"coerce\").clip(0, 14)\n\n# winsor_cols = [\n#     \"Average Water Speed\",\"Chlorophyll\",\"Temperature\",\n#     \"Dissolved Oxygen\",\"Dissolved Oxygen (%Saturation)\",\"pH\",\n#     \"Salinity\",\"Specific Conductance\",\"Average Water Direction\"\n# ]\n# winsor_bounds = make_winsor_bounds(train_step, winsor_cols, WINSOR_Q_LOW, WINSOR_Q_HIGH)\n# def _apply_winsor(df_slice, bounds, cols):\n#     for cc in cols:\n#         lo, hi = bounds.get(cc, (np.nan, np.nan))\n#         if pd.notna(lo) and pd.notna(hi) and cc in df_slice.columns:\n#             df_slice[cc] = pd.to_numeric(df_slice[cc], errors=\"coerce\").clip(lo, hi)\n# _apply_winsor(train_step, winsor_bounds, winsor_cols)\n# _apply_winsor(test_step,  winsor_bounds, winsor_cols)\n\n# # ====== 5) Fallback median jika masih ada NA ======\n# for c in base_numeric_cols:\n#     med = pd.to_numeric(train_step[c], errors=\"coerce\").median()\n#     train_step[c] = pd.to_numeric(train_step[c], errors=\"coerce\").fillna(med)\n#     test_step[c]  = pd.to_numeric(test_step[c],  errors=\"coerce\").fillna(med)\n\n# # ====== 6) Output akhir + diagnostik ======\n# train_b2 = train_step.copy()\n# test_b2  = test_step.copy()\n\n# missing_after_final_base = train_b2[base_numeric_cols].isna().sum().sort_values(ascending=False)\n\n# filled_stage1 = (missing_before_base - missing_after_nonrf).clip(lower=0)\n# filled_stage2 = (missing_after_nonrf - missing_after_final_base).clip(lower=0)\n# filled_total  = (missing_before_base - missing_after_final_base).clip(lower=0)\n\n# print(\"\\n[Diagnostik] Imputasi BASE per kolom (TRAIN):\")\n# print(pd.DataFrame({\n#     \"before\": missing_before_base,\n#     \"after_stage1(nonRF)\": missing_after_nonrf,\n#     \"after_final\": missing_after_final_base,\n#     \"filled_stage1\": filled_stage1,\n#     \"filled_stage2(RF+post)\": filled_stage2,\n#     \"filled_total\":  filled_total\n# }).loc[missing_before_base.index].to_string())\n\n# if int(missing_after_final_base.sum()) == 0:\n#     print(\"\\n[OK] Tidak ada missing tersisa pada BASE numerik (TRAIN).\")\n# else:\n#     print(\"\\n[Warning] Masih ada missing di BASE numerik (TRAIN):\")\n#     print(missing_after_final_base[missing_after_final_base>0].to_string())\n\n# # (Compat) Buat scaler_base dummy agar blok penyimpanan artefak tidak error\n# class _DummyScaler: pass\n# scaler_base = _DummyScaler()\n# scaler_base.mean_  = train_b2[base_numeric_cols].mean().values\n# _s = train_b2[base_numeric_cols].std(ddof=0).replace(0, 1.0)\n# scaler_base.scale_ = _s.values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.722890Z","iopub.execute_input":"2025-11-01T03:51:23.723353Z","iopub.status.idle":"2025-11-01T03:51:23.743689Z","shell.execute_reply.started":"2025-11-01T03:51:23.723334Z","shell.execute_reply":"2025-11-01T03:51:23.743011Z"}},"outputs":[],"execution_count":316},{"cell_type":"code","source":"# Blok 5 — Imputasi BASE numerik (per kolom) + clip pH + winsor + imputasi TARGET orde-2 + diagnostik\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\n\n# ---- Konfigurasi umum ----\nbase_numeric_cols = [\n    \"Average Water Speed\",\n    \"Average Water Direction\",\n    \"Chlorophyll\",\n    \"Temperature\",\n    \"Dissolved Oxygen\",\n    \"Dissolved Oxygen (%Saturation)\",\n    \"pH\",\n    \"Salinity\",\n    \"Specific Conductance\",\n]\n\n# Mapping strategi sesuai “BEST METHOD PER COLUMN”\nstrategy = {\n    \"Average Water Direction\": \"rf_per_column\",\n    \"Average Water Speed\": \"median_by_month\",\n    \"Chlorophyll\": \"rolling_median_w5\",\n    \"Dissolved Oxygen\": \"rf_per_column\",\n    \"Dissolved Oxygen (%Saturation)\": \"rf_per_column\",\n    \"pH\": \"rolling_mean_w5\",\n    \"Salinity\": \"rf_per_column\",\n    \"Specific Conductance\": \"time_interpolate_linear\",\n    \"Temperature\": \"time_interpolate_linear\",\n}\n\n# Toggle target imputasi sebagai label (default False = label asli saja)\nUSE_IMPUTED_TARGET_AS_LABEL = False\n\n# ===== A) Diagnostik awal =====\nmissing_before_base = df_train[base_numeric_cols].isna().sum().sort_values(ascending=False)\nprint(\"\\n[Diagnostik] Missing BASE sebelum imputasi (TRAIN):\")\nprint(missing_before_base[missing_before_base>0].to_string())\n\n# rangkai data dengan waktu bantu (_month, _hour) dari blok waktu sebelumnya\ntrain_b = pd.concat([df_train[[TIME_COL]], train_time[[\"_month\",\"_hour\"]], df_train[base_numeric_cols]], axis=1)\ntest_b  = pd.concat([df_test[[TIME_COL]],  test_time[[\"_month\",\"_hour\"]],  df_test[base_numeric_cols]],  axis=1)\n\n# ===== B) Helper per-metode =====\ndef _time_interp_linear_train(s: pd.Series, t: pd.Series) -> pd.Series:\n    d = pd.DataFrame({TIME_COL: t, \"val\": s}).sort_values(TIME_COL).set_index(TIME_COL)\n    d[\"val\"] = d[\"val\"].interpolate(method=\"time\", limit_direction=\"both\")\n    d[\"val\"] = d[\"val\"].ffill().bfill()\n    return d[\"val\"].reset_index(drop=True)\n\ndef _time_interp_linear_test(train_s: pd.Series, train_t: pd.Series,\n                             test_s: pd.Series,  test_t: pd.Series) -> pd.Series:\n    tr = pd.DataFrame({TIME_COL: train_t, \"val\": train_s, \"_is_test\": 0})\n    te = pd.DataFrame({TIME_COL: test_t,  \"val\": test_s,  \"_is_test\": 1})\n    both = pd.concat([tr, te], axis=0, ignore_index=True).sort_values(TIME_COL).set_index(TIME_COL)\n    mask_te = both[\"_is_test\"]==1\n    both[\"val\"] = both[\"val\"].interpolate(method=\"time\", limit_direction=\"forward\")\n    both[\"val\"] = both[\"val\"].ffill()\n    out = both.loc[mask_te, \"val\"].reset_index(drop=True)\n    return out\n\ndef _median_by_month(train_col: pd.Series, train_month: pd.Series,\n                     test_col: pd.Series,  test_month: pd.Series) -> tuple[pd.Series, pd.Series]:\n    m_map = train_col.groupby(train_month).median()\n    g_med = float(train_col.median())\n    tr = train_col.copy()\n    te = test_col.copy()\n    tr = tr.fillna(train_month.map(m_map))\n    tr = tr.fillna(g_med)\n    te = te.fillna(test_month.map(m_map))\n    te = te.fillna(g_med)\n    return tr, te\n\ndef _rolling_impute(train_s: pd.Series, train_t: pd.Series,\n                    test_s: pd.Series,  test_t: pd.Series,\n                    win: int = 5, how: str = \"median\") -> tuple[pd.Series, pd.Series]:\n    # gabungkan, urut waktu → rolling past-only\n    tr = pd.DataFrame({TIME_COL: train_t, \"val\": train_s, \"_is_test\":0})\n    te = pd.DataFrame({TIME_COL: test_t,  \"val\": test_s,  \"_is_test\":1})\n    both = pd.concat([tr, te], axis=0, ignore_index=True).sort_values(TIME_COL)\n    if how == \"median\":\n        roll = both[\"val\"].rolling(win, min_periods=1).median()\n    else:\n        roll = both[\"val\"].rolling(win, min_periods=1).mean()\n    both[\"val\"] = both[\"val\"].fillna(roll)\n    # fallback sedikit agar aman\n    g_med = float(pd.to_numeric(train_s, errors=\"coerce\").median())\n    both[\"val\"] = both[\"val\"].fillna(g_med)\n    # pisah kembali\n    tr_out = both.loc[both[\"_is_test\"]==0, \"val\"].reset_index(drop=True)\n    te_out = both.loc[both[\"_is_test\"]==1, \"val\"].reset_index(drop=True)\n    return tr_out, te_out\n\n# ===== C) Tahap NON-RF — isi kolom non-RF sesuai metode =====\ntrain_step = train_b[base_numeric_cols].copy()\ntest_step  = test_b[base_numeric_cols].copy()\n\norig_missing_train = {c: train_step[c].isna().values for c in base_numeric_cols}\norig_missing_test  = {c: test_step[c].isna().values  for c in base_numeric_cols}\n\nfor c in base_numeric_cols:\n    meth = strategy.get(c, \"median_by_month\")\n    if meth == \"time_interpolate_linear\":\n        train_step[c] = _time_interp_linear_train(train_b[c], train_b[TIME_COL])\n        test_step[c]  = _time_interp_linear_test(train_b[c], train_b[TIME_COL], test_b[c], test_b[TIME_COL])\n    elif meth == \"median_by_month\":\n        tr, te = _median_by_month(train_b[c], train_b[\"_month\"], test_b[c], test_b[\"_month\"])\n        train_step[c], test_step[c] = tr, te\n    elif meth == \"rolling_median_w5\":\n        tr, te = _rolling_impute(train_b[c], train_b[TIME_COL], test_b[c], test_b[TIME_COL], win=5, how=\"median\")\n        train_step[c], test_step[c] = tr, te\n    elif meth == \"rolling_mean_w5\":\n        tr, te = _rolling_impute(train_b[c], train_b[TIME_COL], test_b[c], test_b[TIME_COL], win=5, how=\"mean\")\n        train_step[c], test_step[c] = tr, te\n    elif meth == \"rf_per_column\":\n        # akan diisi pada Tahap RF; biarkan dulu\n        pass\n    else:\n        # fallback aman\n        tr, te = _median_by_month(train_b[c], train_b[\"_month\"], test_b[c], test_b[\"_month\"])\n        train_step[c], test_step[c] = tr, te\n\nmissing_after_nonrf = train_step[base_numeric_cols].isna().sum().sort_values(ascending=False)\n\n# ===== D) Tahap RF per kolom =====\nrf_cols = [c for c,m in strategy.items() if m==\"rf_per_column\" and c in base_numeric_cols]\n\n# fitur RF: semua kolom base (hasil non-RF) + _month, _hour\nXtr_base = pd.concat([train_time[[\"_month\",\"_hour\"]].reset_index(drop=True), train_step.reset_index(drop=True)], axis=1)\nXte_base = pd.concat([test_time[[\"_month\",\"_hour\"]].reset_index(drop=True),  test_step.reset_index(drop=True)],  axis=1)\n\n# Isi sementara NA prediktor dengan median TRAIN (agar model bisa fit)\nmed_map = {c: pd.to_numeric(Xtr_base[c], errors=\"coerce\").median() for c in Xtr_base.columns if c not in [\"_month\",\"_hour\"]}\nfor D in (Xtr_base, Xte_base):\n    for c in D.columns:\n        if c in [\"_month\",\"_hour\"]:\n            D[c] = pd.to_numeric(D[c], errors=\"coerce\").fillna(0).astype(int)\n        else:\n            D[c] = pd.to_numeric(D[c], errors=\"coerce\").fillna(med_map.get(c, 0.0))\n\nfor c in rf_cols:\n    # target y = nilai asli kolom c (TRAIN)\n    y_tr = pd.to_numeric(df_train[c], errors=\"coerce\")\n    mask_train_known = y_tr.notna()\n    if mask_train_known.sum() == 0:\n        fill_val = float(y_tr.median() if pd.notna(y_tr.median()) else 0.0)\n        train_step[c] = train_step[c].fillna(fill_val)\n        test_step[c]  = test_step[c].fillna(fill_val)\n        continue\n\n    feat_cols_rf = [col for col in Xtr_base.columns if col != c]\n    X_tr_rf = Xtr_base.loc[mask_train_known, feat_cols_rf]\n    y_tr_rf = y_tr.loc[mask_train_known].astype(float)\n\n    rf = RandomForestRegressor(\n        n_estimators=300, max_depth=12, min_samples_leaf=3,\n        random_state=SEED, n_jobs=-1\n    )\n    rf.fit(X_tr_rf, y_tr_rf)\n\n    # prediksi hanya untuk posisi yang tadinya NA\n    if orig_missing_train[c].any():\n        idx_na_tr = np.where(orig_missing_train[c])[0]\n        if len(idx_na_tr):\n            pred_tr = rf.predict(Xtr_base.iloc[idx_na_tr][feat_cols_rf])\n            train_step.loc[idx_na_tr, c] = pred_tr.astype(float)\n    if orig_missing_test[c].any():\n        idx_na_te = np.where(orig_missing_test[c])[0]\n        if len(idx_na_te):\n            pred_te = rf.predict(Xte_base.iloc[idx_na_te][feat_cols_rf])\n            test_step.loc[idx_na_te, c] = pred_te.astype(float)\n\n# ===== E) Domain-clip pH & Winsorize ringan =====\nfor d in (train_step, test_step):\n    d[\"pH\"] = pd.to_numeric(d[\"pH\"], errors=\"coerce\").clip(0, 14)\n\nwinsor_cols = [\n    \"Average Water Speed\",\"Chlorophyll\",\"Temperature\",\n    \"Dissolved Oxygen\",\"Dissolved Oxygen (%Saturation)\",\"pH\",\n    \"Salinity\",\"Specific Conductance\",\"Average Water Direction\"\n]\nwinsor_bounds = make_winsor_bounds(train_step, winsor_cols, WINSOR_Q_LOW, WINSOR_Q_HIGH)\n\ndef _apply_winsor(df_slice, bounds, cols):\n    for cc in cols:\n        lo, hi = bounds.get(cc, (np.nan, np.nan))\n        if pd.notna(lo) and pd.notna(hi) and cc in df_slice.columns:\n            df_slice[cc] = pd.to_numeric(df_slice[cc], errors=\"coerce\").clip(lo, hi)\n\n_apply_winsor(train_step, winsor_bounds, winsor_cols)\n_apply_winsor(test_step,  winsor_bounds, winsor_cols)\n\n# ===== F) Fallback median jika masih ada NA =====\nfor c in base_numeric_cols:\n    med = pd.to_numeric(train_step[c], errors=\"coerce\").median()\n    train_step[c] = pd.to_numeric(train_step[c], errors=\"coerce\").fillna(med)\n    test_step[c]  = pd.to_numeric(test_step[c],  errors=\"coerce\").fillna(med)\n\n# Output akhir fitur imputasi\ntrain_b2 = train_step.copy()\ntest_b2  = test_step.copy()\n\nmissing_after_final_base = train_b2[base_numeric_cols].isna().sum().sort_values(ascending=False)\n\n# ===== G) Imputasi TARGET orde-2 (context) + toggle label =====\ntgt_df = df_train[[TIME_COL, TARGET_COL]].copy().sort_values(TIME_COL).set_index(TIME_COL)\nmissing_before_tgt = int(tgt_df[TARGET_COL].isna().sum())\ntry:\n    tgt_df[TARGET_COL] = tgt_df[TARGET_COL].interpolate(method=\"spline\", order=2, limit_direction=\"both\")\nexcept Exception:\n    tgt_df[TARGET_COL] = tgt_df[TARGET_COL].interpolate(method=\"polynomial\", order=2, limit_direction=\"both\")\ntgt_df[TARGET_COL] = tgt_df[TARGET_COL].ffill().bfill()\ndf_train[\"Turbidity_ctx\"] = tgt_df.reset_index()[TARGET_COL].astype(float).values\nmissing_after_tgt = int(df_train[\"Turbidity_ctx\"].isna().sum())\n\nif USE_IMPUTED_TARGET_AS_LABEL:\n    df_train[\"Turbidity_filled\"] = (\n        pd.to_numeric(df_train[TARGET_COL], errors=\"coerce\")\n        .fillna(df_train[\"Turbidity_ctx\"])\n        .astype(float)\n    )\n    label_note = \"Label latihan = Turbidity_filled (pakai imputasi).\"\nelse:\n    df_train[\"Turbidity_filled\"] = pd.to_numeric(df_train[TARGET_COL], errors=\"coerce\").astype(float)\n    label_note = \"Label latihan = nilai asli (NaN tidak dilatih).\"\n\n# ===== H) Diagnostik lengkap =====\nfilled_stage1 = (missing_before_base - missing_after_nonrf).clip(lower=0)\nfilled_stage2 = (missing_after_nonrf - missing_after_final_base).clip(lower=0)\nfilled_total  = (missing_before_base - missing_after_final_base).clip(lower=0)\n\nprint(\"\\n[Diagnostik] Imputasi BASE per kolom (TRAIN):\")\nprint(pd.DataFrame({\n    \"before\": missing_before_base,\n    \"after_stage1(nonRF)\": missing_after_nonrf,\n    \"after_final\": missing_after_final_base,\n    \"filled_stage1\": filled_stage1,\n    \"filled_stage2(RF+post)\": filled_stage2,\n    \"filled_total\":  filled_total\n}).loc[missing_before_base.index].to_string())\n\nif int(missing_after_final_base.sum()) == 0:\n    print(\"\\n[OK] Tidak ada missing tersisa pada BASE numerik (TRAIN).\")\nelse:\n    print(\"\\n[Warning] Masih ada missing di BASE numerik (TRAIN):\")\n    print(missing_after_final_base[missing_after_final_base>0].to_string())\n\nprint(f\"\\n[TARGET_CTX] Missing sebelum={missing_before_tgt} | sesudah={missing_after_tgt} (ctx encoder).\")\nprint(f\"[TARGET_LABEL] {label_note}\")\n\n# (Compat) Buat scaler_base dummy agar blok penyimpanan artefak tidak error\nclass _DummyScaler: pass\nscaler_base = _DummyScaler()\nscaler_base.mean_  = train_b2[base_numeric_cols].mean().values\n_s = train_b2[base_numeric_cols].std(ddof=0).replace(0, 1.0)\nscaler_base.scale_ = _s.values\n\n# (Opsional) Cetak ringkas strategi yang dipakai\nprint(\"\\n[STRATEGY] Imputasi per kolom:\")\nfor k in base_numeric_cols:\n    print(f\"  - {k}: {strategy.get(k, 'median_by_month')}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.744492Z","iopub.execute_input":"2025-11-01T03:51:23.744762Z","iopub.status.idle":"2025-11-01T03:51:23.901050Z","shell.execute_reply.started":"2025-11-01T03:51:23.744746Z","shell.execute_reply":"2025-11-01T03:51:23.899864Z"}},"outputs":[{"name":"stdout","text":"\n[Diagnostik] Missing BASE sebelum imputasi (TRAIN):\nTemperature                       9214\nDissolved Oxygen (%Saturation)    5758\nDissolved Oxygen                  4309\nSalinity                          3967\nChlorophyll                       1704\nSpecific Conductance              1376\npH                                1093\nAverage Water Speed                219\nAverage Water Direction              1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1946690872.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"median_by_month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"time_interpolate_linear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time_interp_linear_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTIME_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mtest_step\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0m_time_interp_linear_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTIME_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTIME_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"median_by_month\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/1946690872.py\u001b[0m in \u001b[0;36m_time_interp_linear_train\u001b[0;34m(s, t)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_time_interp_linear_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mTIME_COL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIME_COL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIME_COL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_direction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"both\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[1;32m   8497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8498\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_interp_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8499\u001b[0;31m             new_data = obj._mgr.interpolate(\n\u001b[0m\u001b[1;32m   8500\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8501\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, inplace, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         return self.apply_with_block(\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;34m\"interpolate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, index, inplace, limit, limit_direction, limit_area, downcast, using_cow, already_warned, **kwargs)\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0;31m# Dispatch to the EA method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m         new_values = self.array_values.interpolate(\n\u001b[0m\u001b[1;32m   1798\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, axis, index, limit, limit_direction, limit_area, copy, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# TODO: assert we have floating dtype?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         missing.interpolate_2d_inplace(\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0mout_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/missing.py\u001b[0m in \u001b[0;36minterpolate_2d_inplace\u001b[0;34m(data, index, axis, method, limit, limit_direction, limit_area, fill_value, mask, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"time\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0;34m\"time-weighted interpolation only works \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;34m\"on Series or DataFrames with a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: time-weighted interpolation only works on Series or DataFrames with a DatetimeIndex"],"ename":"ValueError","evalue":"time-weighted interpolation only works on Series or DataFrames with a DatetimeIndex","output_type":"error"}],"execution_count":317},{"cell_type":"code","source":"# Blok 6 — FE turunan (SETELAH imputasi) + rakit train_ready/test_ready + candidate_features\n# (PATCH: dedup kolom + akses kolom arah yang robust)\n\n# Satukan waktu + base terimput\ntrain_base = pd.concat(\n    [train_time.reset_index(drop=True), train_b2[[\n        \"Average Water Speed\",\"Average Water Direction\",\"Chlorophyll\",\"Temperature\",\n        \"Dissolved Oxygen\",\"Dissolved Oxygen (%Saturation)\",\"pH\",\"Salinity\",\"Specific Conductance\"\n    ]].reset_index(drop=True)],\n    axis=1\n)\ntest_base  = pd.concat(\n    [test_time.reset_index(drop=True),  test_b2[[\n        \"Average Water Speed\",\"Average Water Direction\",\"Chlorophyll\",\"Temperature\",\n        \"Dissolved Oxygen\",\"Dissolved Oxygen (%Saturation)\",\"pH\",\"Salinity\",\"Specific Conductance\"\n    ]].reset_index(drop=True)],\n    axis=1\n)\n\n# >>>> NEW: pastikan tidak ada kolom duplikat (keep first) <<<<\ntrain_base = train_base.loc[:, ~train_base.columns.duplicated()].copy()\ntest_base  = test_base.loc[:,  ~test_base.columns.duplicated()].copy()\n\n# >>>> NEW: helper agar selalu dapat Series 1D biarpun ada duplikat tersisa <<<<\ndef _get_series(df: pd.DataFrame, col: str) -> pd.Series:\n    obj = df[col]\n    # Jika karena suatu alasan masih DataFrame (duplikat), ambil kolom pertama\n    if isinstance(obj, pd.DataFrame):\n        return obj.iloc[:, 0]\n    return obj\n\n# FE sirkular & turunan penting (aman karena semua base sudah diimput)\ndef add_sensor_fe(df: pd.DataFrame) -> pd.DataFrame:\n    d = df.copy()\n    # arah (pakai accessor robust)\n    dir_deg = pd.to_numeric(_get_series(d, \"Average Water Direction\"), errors=\"coerce\")\n    rad = np.deg2rad(dir_deg)\n    d[\"dir_sin\"] = np.sin(rad)\n    d[\"dir_cos\"] = np.cos(rad)\n\n    # interaksi dan nonlin\n    spd = pd.to_numeric(_get_series(d, \"Average Water Speed\"), errors=\"coerce\")\n    tmp = pd.to_numeric(_get_series(d, \"Temperature\"), errors=\"coerce\")\n    sal = pd.to_numeric(_get_series(d, \"Salinity\"), errors=\"coerce\")\n    sc  = pd.to_numeric(_get_series(d, \"Specific Conductance\"), errors=\"coerce\")\n    do_sat = pd.to_numeric(_get_series(d, \"Dissolved Oxygen (%Saturation)\"), errors=\"coerce\")\n\n    d[\"speed_x_dir_sin\"] = spd * d[\"dir_sin\"]\n    d[\"speed_x_dir_cos\"] = spd * d[\"dir_cos\"]\n    d[\"temp2\"] = tmp ** 2\n    d[\"cond_per_sal\"] = sc / sal.replace(0, np.nan)\n    d[\"do_sat_diff\"] = do_sat - 100.0\n    return d\n\ntrain_base = add_sensor_fe(train_base)\ntest_base  = add_sensor_fe(test_base)\n\n# Kandidat fitur (tanpa Average Water Direction mentah — sudah diwakili sin/cos)\nbase_sensor_cols = [\n    \"Average Water Speed\",\"Chlorophyll\",\"Temperature\",\"Dissolved Oxygen\",\n    \"Dissolved Oxygen (%Saturation)\",\"pH\",\"Salinity\",\"Specific Conductance\"\n]\neng_time = [\n    \"hour\",\"month\",\"dayofweek\",\"dayofyear\",\n    \"hour_sin\",\"hour_cos\",\"month_sin\",\"month_cos\",\"doy_sin\",\"doy_cos\",\n]\neng_circ = [\"dir_sin\",\"dir_cos\"]\neng_extra = [\"speed_x_dir_sin\",\"speed_x_dir_cos\",\"temp2\",\"cond_per_sal\",\"do_sat_diff\"]\n\ncandidate_features = base_sensor_cols + eng_time + eng_circ + eng_extra\n\n# Rakit frame siap modeling (BELUM seleksi fitur)\ntrain_ready = train_base.copy()\ntrain_ready[TARGET_COL] = df_train[TARGET_COL].values\ntest_ready  = test_base.copy()\n\n# Safety FE: ubah inf→NaN lalu isi median berdasar TRAIN\nfor D in (train_ready, test_ready):\n    D.replace([np.inf, -np.inf], np.nan, inplace=True)\n\nmed_map_full = train_ready[candidate_features].median(numeric_only=True).to_dict()\nfor D in (train_ready, test_ready):\n    for c in candidate_features:\n        if D[c].isna().any():\n            D[c] = D[c].fillna(med_map_full.get(c, 0.0))\n\nprint(f\"[FE] Candidate features: {len(candidate_features)} -> {candidate_features}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.901742Z","iopub.status.idle":"2025-11-01T03:51:23.902047Z","shell.execute_reply.started":"2025-11-01T03:51:23.901914Z","shell.execute_reply":"2025-11-01T03:51:23.901929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ready.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.902763Z","iopub.status.idle":"2025-11-01T03:51:23.902976Z","shell.execute_reply.started":"2025-11-01T03:51:23.902867Z","shell.execute_reply":"2025-11-01T03:51:23.902875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Blok 7 — Ranking korelasi (FULL) + seleksi fitur + build X/y/X_test\nmethods_map = {1: \"pearson\", 2: \"spearman\"}\nchosen_methods = [methods_map[m] for m in FEATURE_METHODS]\n\nfeatures_available = [c for c in candidate_features if c in train_ready.columns]\nif len(features_available) < len(candidate_features):\n    missing_feat = [c for c in candidate_features if c not in train_ready.columns]\n    print(\"[Warn] Fitur tidak ditemukan (skip):\", missing_feat)\n\ntop_cap = min(TOP_K, len(features_available))\n\n# Basis korelasi: drop baris target NaN\ncorr_base = train_ready[features_available + [TARGET_COL]].copy()\ncorr_base = corr_base[corr_base[TARGET_COL].notna()].reset_index(drop=True)\n\n# Hitung & CETAK ranking penuh + simpan CSV\ncorr_outputs = {}\nfor m in chosen_methods:\n    corr_tbl = corr_with_target(corr_base, features_available, TARGET_COL, method=m)\n    corr_outputs[m] = corr_tbl\n    print(f\"\\n[Ranking Korelasi — {m.upper()}] (abs_r desc)\")\n    print(corr_tbl[[\"feature\",\"n_pair\",\"r\",\"p_value\",\"abs_r\"]].to_string(index=True))\n    corr_tbl.to_csv(WORK_DIR / f\"corr_ranking_{m}.csv\", index=False)\n\n# Seleksi fitur: porsi adil per metode, sisanya isi rata-rata ranking\nselected = set()\nk_per_method = max(1, top_cap // max(1, len(chosen_methods)))\nfor m in chosen_methods:\n    selected.update(corr_outputs[m].head(k_per_method)[\"feature\"].tolist())\n\nif len(selected) < top_cap:\n    feature_scores = {}\n    for m, cdf in corr_outputs.items():\n        cdf = cdf.reset_index(drop=False).rename(columns={\"index\": \"rank0\"})\n        cdf[\"rank\"] = cdf[\"rank0\"] + 1\n        for _, row in cdf.iterrows():\n            feature_scores.setdefault(row[\"feature\"], []).append(row[\"rank\"])\n    avg_rank = {f: np.mean(rks) for f, rks in feature_scores.items()}\n    fill_pool = [f for f in features_available if f not in selected]\n    fill_order = sorted(fill_pool, key=lambda x: avg_rank.get(x, 1e9))\n    for f in fill_order:\n        if len(selected) >= top_cap:\n            break\n        selected.add(f)\n\nselected = list(sorted(set(list(selected) + list(ALWAYS_KEEP))))\nselected = [f for f in selected if f in features_available][:top_cap]\nprint(f\"\\n[SELECTED] features ({len(selected)}): {selected}\")\n\n# Build X, y, X_test (dipakai blok modeling)\nX_all = train_ready[selected].copy()\ny_all = train_ready[TARGET_COL].copy()\nmask_train = y_all.notna().values\n\nX = X_all.loc[mask_train].astype(np.float32).copy()\ny = y_all.loc[mask_train].astype(np.float32).copy()\nX_test = test_ready[selected].astype(np.float32).copy()\n\n# Safety terakhir\nfor D in (X, X_test):\n    if D.isna().any().any():\n        med_map = X.median(numeric_only=True).to_dict()\n        for c in D.columns:\n            if D[c].isna().any():\n                D[c] = D[c].fillna(med_map.get(c, 0.0))\n\nprint(f\"\\n[Check Modeling] Rows (train, after drop NaN target): {len(X)}\")\nprint(f\"[Check Modeling] Missing in X: {int(X.isna().sum().sum())} | Missing in y: {int(y.isna().sum())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.903831Z","iopub.status.idle":"2025-11-01T03:51:23.904072Z","shell.execute_reply.started":"2025-11-01T03:51:23.903958Z","shell.execute_reply":"2025-11-01T03:51:23.903970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Blok 8 — PEMODELAN (konfig di sini; mudah diganti)\nimport lightgbm as lgb\n\nMODEL_NAME = \"lgbm\"\nMODEL_PARAMS = {\n    \"objective\": \"regression\",\n    \"boosting_type\": \"gbdt\",\n    \"metric\": \"rmse\",\n    \"n_estimators\": 5000,   # dipangkas oleh early_stopping saat validasi\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 64,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.9,\n    \"bagging_freq\": 1,\n    \"min_data_in_leaf\": 40,\n    \"reg_alpha\": 0.0,\n    \"reg_lambda\": 0.0,\n    \"random_state\": SEED,\n    \"n_jobs\": -1,\n    \"verbose\": -1\n}\n\ndef make_model(name: str, params: dict):\n    if name == \"lgbm\":\n        return lgb.LGBMRegressor(**params)\n    else:\n        raise NotImplementedError(f\"Model '{name}' belum diimplementasikan.\")\n\nprint(f\"Model ready: {MODEL_NAME}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.905725Z","iopub.status.idle":"2025-11-01T03:51:23.906062Z","shell.execute_reply.started":"2025-11-01T03:51:23.905945Z","shell.execute_reply":"2025-11-01T03:51:23.905961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Blok 9 — VALIDASI (CV 5-fold) + OOF metrics + FI\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\noof_pred = np.zeros(len(X), dtype=float)\nmodels = []\nbest_iters = []\nfold_metrics = []\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):\n    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n    model = make_model(MODEL_NAME, MODEL_PARAMS)\n    if MODEL_NAME == \"lgbm\":\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(0)]\n        )\n        pred_va = model.predict(X_va, num_iteration=model.best_iteration_)\n        best_iters.append(model.best_iteration_)\n    else:\n        model.fit(X_tr, y_tr)\n        pred_va = model.predict(X_va)\n\n    oof_pred[va_idx] = pred_va\n\n    mae_   = mean_absolute_error(y_va, pred_va)\n    mse_   = mean_squared_error(y_va, pred_va)  # MSE per fold\n    rmse_  = rmse(y_va, pred_va)\n    mape_  = mape(y_va, pred_va)\n    smape_ = smape(y_va, pred_va)\n\n    fold_metrics.append({\n        \"fold\": fold,\n        \"MAE\": mae_,\n        \"MSE\": mse_,\n        \"RMSE\": rmse_,\n        \"MAPE\": mape_,\n        \"SMAPE\": smape_\n    })\n    models.append(model)\n\n    # === Print MSE per fold ===\n    print(f\"Fold {fold}: MSE = {mse_:.6f}\")\n\n# === Rata-rata per metrik (mean of folds) ===\nfm_df = pd.DataFrame(fold_metrics).sort_values(\"fold\")\navg_metrics = fm_df[[\"MAE\",\"MSE\",\"RMSE\",\"MAPE\",\"SMAPE\"]].mean().to_dict()\nprint(\"\\nRata-rata per metrik (mean of 5 folds)\")\nprint(json.dumps({k: round(v, 6) for k, v in avg_metrics.items()}, indent=2))\n\n# (Opsional) OOF metrics—tetap kita hitung kalau kamu mau bandingkan dengan mean-of-folds\noof_mae  = mean_absolute_error(y, oof_pred)\noof_mse  = mean_squared_error(y, oof_pred)\noof_rmse = rmse(y, oof_pred)\noof_mape = mape(y, oof_pred)\noof_smape= smape(y, oof_pred)\nprint(\"\\nOOF Metrics (gabungan seluruh fold)\")\nprint(json.dumps({\n    \"MAE\": round(oof_mae, 6),\n    \"MSE\": round(oof_mse, 6),\n    \"RMSE\": round(oof_rmse, 6),\n    \"MAPE\": round(oof_mape, 6),\n    \"SMAPE\": round(oof_smape, 6)\n}, indent=2))\n\n# Simpan output evaluasi\nfm_df.to_csv(WORK_DIR / \"cv_fold_metrics.csv\", index=False)\n\n# Feature importance\ntry:\n    if MODEL_NAME == \"lgbm\":\n        fi_mat = [m.booster_.feature_importance(importance_type=\"gain\") for m in models]\n        fi = pd.DataFrame({\"feature\": X.columns, \"importance\": np.mean(fi_mat, axis=0)})\n    elif hasattr(models[0], \"feature_importances_\"):\n        fi_mat = [m.feature_importances_ for m in models]\n        fi = pd.DataFrame({\"feature\": X.columns, \"importance\": np.mean(fi_mat, axis=0)})\n    else:\n        fi = pd.DataFrame({\"feature\": X.columns, \"importance\": np.nan})\n    fi = fi.sort_values(\"importance\", ascending=False)\n    fi.to_csv(WORK_DIR / \"cv_feature_importance.csv\", index=False)\nexcept Exception:\n    pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.907078Z","iopub.status.idle":"2025-11-01T03:51:23.907319Z","shell.execute_reply.started":"2025-11-01T03:51:23.907200Z","shell.execute_reply":"2025-11-01T03:51:23.907213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"k = 5: \"MSE\": 13.889316,\nk = 10, \"MSE\": 6.153683,\nk = 15, \"MSE\": 5.952698,\nk = 20, \"MSE\": 5.884146,\nk = 25, \"MSE\": ","metadata":{}},{"cell_type":"code","source":"# Blok 10 — RETRAIN 100% & simpan model\nimport joblib\n\nif MODEL_NAME == \"lgbm\" and len(best_iters):\n    avg_best_iter = int(np.mean(best_iters))\n    final_n_estimators = max(200, int(avg_best_iter * 1.05))\n    final_params = {**MODEL_PARAMS, \"n_estimators\": final_n_estimators}\nelse:\n    final_params = {**MODEL_PARAMS}\n    final_n_estimators = final_params.get(\"n_estimators\", 1000)\n\nfinal_model = make_model(MODEL_NAME, final_params)\nfinal_model.fit(X, y)\n\nif MODEL_NAME == \"lgbm\":\n    MODEL_TXT_PATH = WORK_DIR / f\"{MODEL_NAME}_turbidity_model.txt\"\n    final_model.booster_.save_model(str(MODEL_TXT_PATH))\n    print(\"Model saved to:\", MODEL_TXT_PATH)\nelse:\n    MODEL_PKL_PATH = WORK_DIR / f\"{MODEL_NAME}_turbidity_model.pkl\"\n    joblib.dump(final_model, MODEL_PKL_PATH)\n    print(\"Model saved to:\", MODEL_PKL_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.908356Z","iopub.status.idle":"2025-11-01T03:51:23.908600Z","shell.execute_reply.started":"2025-11-01T03:51:23.908490Z","shell.execute_reply":"2025-11-01T03:51:23.908501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Blok 11 — Prediksi TEST & Submission (Record number,Turbidity)\ntest_pred = final_model.predict(X_test)\n\nsub = sample_sub[[ID_COL]].copy()\npred_df = pd.DataFrame({ID_COL: df_test[ID_COL], TARGET_COL: test_pred})\nsub = sub.merge(pred_df, on=ID_COL, how=\"left\")\n\nSUB_PATH = WORK_DIR / \"submission.csv\"\nsub.to_csv(SUB_PATH, index=False)\nprint(\"Submission saved to:\", SUB_PATH)\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.909454Z","iopub.status.idle":"2025-11-01T03:51:23.909659Z","shell.execute_reply.started":"2025-11-01T03:51:23.909562Z","shell.execute_reply":"2025-11-01T03:51:23.909571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Blok 12 — Artefak preprocessing (repro-friendly)\nartifacts = {\n    \"selected_features\": list(X.columns),\n    \"feature_methods\": [int(m) for m in FEATURE_METHODS],\n    \"top_k\": TOP_K,\n    \"always_keep\": ALWAYS_KEEP,\n    \"knn_k\": KNN_K,\n    \"winsor_q_low\": WINSOR_Q_LOW,\n    \"winsor_q_high\": WINSOR_Q_HIGH,\n    \"model_name\": MODEL_NAME,\n    \"model_params\": MODEL_PARAMS\n}\nwith open(WORK_DIR / \"preprocessing_artifacts.json\", \"w\") as f:\n    json.dump(artifacts, f, indent=2)\n\npd.DataFrame({\"mean\": scaler_base.mean_, \"scale\": scaler_base.scale_},\n             index=[\"Average Water Speed\",\"Average Water Direction\",\"Chlorophyll\",\"Temperature\",\n                    \"Dissolved Oxygen\",\"Dissolved Oxygen (%Saturation)\",\"pH\",\"Salinity\",\"Specific Conductance\"])\\\n  .to_csv(WORK_DIR / \"scaler_base_stats.csv\")\n\nprint(\"Artifacts saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T03:51:23.911219Z","iopub.status.idle":"2025-11-01T03:51:23.911438Z","shell.execute_reply.started":"2025-11-01T03:51:23.911326Z","shell.execute_reply":"2025-11-01T03:51:23.911335Z"}},"outputs":[],"execution_count":null}]}